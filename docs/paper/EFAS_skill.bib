@article{Alfieri2014,
   abstract = {In operational hydrological forecasting systems, improvements are directly related to the continuous monitoring of the forecast performance. An efficient evaluation framework must be able to spot issues and limitations and provide feedback to the system developers. In regional systems, the expertise of analysts on duty is a major component of the daily evaluation. On the other hand, large scale systems need to be complemented with semi-automated tools to evaluate the quality of forecasts equitably in every part of their domain.This article presents the current status of the monitoring and evaluation framework of the European Flood Awareness System (EFAS). For each grid point of the European river network, 10-day ensemble streamflow predictions are evaluated against a reference simulation which uses observed meteorological fields as input to a calibrated hydrological model. Performance scores are displayed over different regions, forecast lead times, basin sizes, as well as in time, considering average scores for moving 12-month windows of forecasts. Skilful predictions are found in medium to large rivers over the whole 10-day range. On average, performance drops significantly in river basins with upstream area smaller than 300km2, partly due to underestimation of the runoff in mountain areas. Model limitations and recommendations to improve the evaluation framework are discussed in the final section.},
   author = {Lorenzo Alfieri and Florian Pappenberger and Fredrik Wetterhall and Thomas Haiden and David Richardson and Peter Salamon},
   doi = {10.1016/j.jhydrol.2014.06.035},
   issn = {00221694},
   journal = {Journal of Hydrology},
   keywords = {CRPS,Distributed hydrological modelling,Ensemble streamflow predictions,Flood early warning,Skill scores},
   month = {6},
   pages = {913-922},
   publisher = {Elsevier B.V.},
   title = {Evaluation of ensemble streamflow predictions in Europe},
   volume = {517},
   year = {2014},
}
@article{Bartholmes2009,
   abstract = {Since 2005 the European Flood Alert System (EFAS) has been producing probabilistic hydrological forecasts in pre-operational mode at the Joint Research Centre (JRC) of the European Commission. EFAS aims at increasing preparedness for floods in trans-national European river basins by providing medium-range deterministic and proba-bilistic flood forecasting information, from 3 to 10 days in advance, to national hydro-meteorological services. This paper is Part 2 of a study presenting the development and skill assessment of EFAS. In Part 1, the scientific approach adopted in the development of the system has been presented, as well as its basic principles and forecast products. In the present article, two years of existing operational EFAS forecasts are statistically assessed and the skill of EFAS forecasts is analysed with several skill scores. The analysis is based on the comparison of threshold exceedances between proxy-observed and forecasted discharges. Skill is assessed both with and without taking into account the persistence of the forecasted signal during consecutive forecasts. Skill assessment approaches are mostly adopted from meteorology and the analysis also compares probabilistic and deterministic aspects of EFAS. Furthermore, the utility of different skill scores is discussed and their strengths and shortcomings illustrated. The analysis shows the benefit of incorporating past forecasts in the probability analysis, for medium-range forecasts, which effectively increases the skill of the forecasts.},
   author = {J C Bartholmes and J Thielen and M H Ramos and S Gentilini},
   doi = {10.5194/hess-13-141-2009},
   journal = {Hydrol. Earth Syst. Sci},
   pages = {141-153},
   title = {The european flood alert system EFAS – Part 2: Statistical skill assessment of probabilistic and deterministic operational forecasts},
   volume = {13},
   url = {www.hydrol-earth-syst-sci.net/13/141/2009/},
   year = {2009},
}
@article{Brier1950,
   abstract = {Verification of weather forecasts has been a controversial subject for more than a half century. There are a number of reasons why this problem has been so perplexing to meteorologists and others but one of the most important difficulties seems to be in reaching an agreement on the specification of a scale of goodness for weather forecasts. Numerous systems have been proposed but one of the greatest arguments raised against forecast verification is that forecasts which may be the “best” according to the accepted system of arbitrary scores may not be the most useful forecasts. In attempting to resolve this difficulty the forecaster may often find himself in the position of choosing to ignore the verification system or to let it, do the forecasting for him by “hedging” or “playing the system.” This may lead the forecaster to forecast something other than what he thinks will occur, for it is often easier to analyze the effect of different possible forecasts on the verification score than it is to analyze the weather situation. It is generally agreed that this state of affairs is unsatisfactory, as one essential criterion for satisfactory verification is that the verification scheme should influence the forecaster in no undesirable way. Unfortunately, the criterion is difficult, if not impossible to satisfy, although some schemes will be much worse than others in this respect.
It is the purpose of this paper to discuss one situation where it appears to be possible to devise a verification scheme that cannot influence the forecaster in any undesirable way. This is the case when forecasts are expressed in terms of probability statements. The advantages of expressing the degree of assumed reliability of a forecast numerically have been discussed previously [1, 2, 3, 4] so that the purpose here will not be to emphasize the enhanced usefulness of such forecasts but rather to point out how some aspects of the verification problem are simplified or solved.},
   author = {Glenn W. Brier},
   issue = {1},
   journal = {Monthly Weather Review},
   title = {Verification of forecasts expressed in terms of probability},
   volume = {78},
   year = {1950},
}
@misc{Burek2013a,
   author = {Peter Burek and Johan van der Knijff and Ad de Roo},
   city = {Luxembourg},
   doi = {10.2788/24719},
   isbn = {879-92-79-33190-9},
   issn = {1831-9424},
   institution = {European Commission - Joint Research Centre},
   title = {LISFLOOD. Distributed Water Balance and Flood Simulation Model},
   year = {2013},
}
@article{Cloke2009,
   abstract = {Operational medium range flood forecasting systems are increasingly moving towards the adoption of ensembles of numerical weather predictions (NWP), known as ensemble prediction systems (EPS), to drive their predictions. We review the scientific drivers of this shift towards such 'ensemble flood forecasting' and discuss several of the questions surrounding best practice in using EPS in flood forecasting systems. We also review the literature evidence of the 'added value' of flood forecasts based on EPS and point to remaining key challenges in using EPS successfully. © 2009 Elsevier B.V. All rights reserved.},
   author = {H. L. Cloke and F. Pappenberger},
   doi = {10.1016/j.jhydrol.2009.06.005},
   issn = {00221694},
   issue = {3-4},
   journal = {Journal of Hydrology},
   keywords = {Ensemble prediction system,Flood prediction,Numerical weather prediction,Streamflow,Uncertainty},
   month = {9},
   pages = {613-626},
   title = {Ensemble flood forecasting: A review},
   volume = {375},
   year = {2009},
}
@article{Cloke2017,
   abstract = {Flood early warning systems mitigate damages and loss of life and are an economically efficient way of enhancing disaster resilience. The use of continental scale flood early warning systems is rapidly growing. The European Flood Awareness System (EFAS) is a pan-European flood early warning system forced by a multi-model ensemble of numerical weather predictions. Responses to scientific and technical changes can be complex in these computationally expensive continental scale systems, and improvements need to be tested by evaluating runs of the whole system. It is demonstrated here that forecast skill is not correlated with the value of warnings. In order to tell if the system has been improved an evaluation strategy is required that considers both forecast skill and warning value. The combination of a multi-forcing ensemble of EFAS flood forecasts is evaluated with a new skill-value strategy. The full multi-forcing ensemble is recommended for operational forecasting, but, there are spatial variations in the optimal forecast combination. Results indicate that optimizing forecasts based on value rather than skill alters the optimal forcing combination and the forecast performance. Also indicated is that model diversity and ensemble size are both important in achieving best overall performance. The use of several evaluation measures that consider both skill and value is strongly recommended when considering improvements to early warning systems.},
   author = {Hannah L. Cloke and Florian Pappenberger and Paul J. Smith and Fredrik Wetterhall},
   doi = {10.1088/1748-9326/aa625a},
   issn = {17489326},
   issue = {4},
   journal = {Environmental Research Letters},
   keywords = {Copernicus,european flood awareness system,flood early warning systems,flood resilience,forecast skill,monetary value,multi-forcing ensemble},
   month = {3},
   note = {<b>Objective</b><br/><br/>* To prove whether there is a  correlation between forecast skill and value.<br/>* To analyze the skill and value of several combinations of forecasts (both probabilistic and deterministic) in the EFAS framework. The<b> hypothesis</b> is that full multi-model forcing will provide the highest skill and highest warning value.<br/><br/><b>Data</b><br/><br/><u>Observed</u><br/><br/>Discharge proxy obtained by simulation of observed meteorology with the same hydrological model setup.<br/><br/><u>Forecast</u><br/><br/>2 years (January 2012, December 2013) of EFAS reforecast data. It includes 4 NWP models: deterministic DWD, deterministic ECMWF, probabilistic ECMWF-ENS, probabilistic COSMO-LEPS. Forecasts are issued daily with 10 d lead time, but the study focuses only from day 3 onwards.<br/><br/><b>Methods</b><br/><br/><u>Forecast combination and optimization</u><br/><br/>To combine forecasts, weights are given to each individual forecast for each of the 768 river catchments. They use <b>nonhomogeneous Gaussian regression (NGR)</b>; this is a methodology that corrects the raw forecast including the uncertainty derived from a linear regression between past forecasts and observations, and the spread in the current forecast.<br/><br/>Where <i>a</i> and <i>bi</i> are bias correction parameters (related to the forecast mean <i>Mi</i>), <i>c </i>and <i>di</i> are spread correction parameters (related to the forecast standard deviation Si). The parameters are estimated with the las 90 d of forecasts. 5 optimization methods are applied:<br/><br/>* Continuous rank probability      score (CRPS)<br/>* CRPS lagged with forecasts      from 3-10 days before.<br/>* Symmetric extreme dependency      index (SEDI). It is built out of the hit rate and false alarm rate from      the contingency table.<br/>* Hit rate<br/>* Value, which the authors      measure simply as the number of hits. I don't understand why.<br/><br/><u>Sensitivity analysis</u><br/><br/>They apply two approaches:<br/><br/>* Leave one out comparison      (LOOC): comparison between combinations that apply a forcing with those      that do not apply it.<br/>* Add one in comparison (AOIC):      an individual forcing is added to each combinations and compared to the      combination without it.<br/><br/>Skill scores are derived from the previous metrics used in the optimization. It means that they are normalized by dividing it by a reference score.<br/><br/><b>Results</b><br/><br/><u>Is an optimization method looking at both skill and value required?</u><br/><br/>* Spearman rank correlation      coefficients prove a week correlation between skill and value.<br/>* The full multi-model approach      ranks higher both for CRPS and value.<br/><br/><u>Which NWP forcings have the greatest relative contribution to improved forecast performance?</u><br/><br/>* DWD, ECMWF-HRES and ECMWF-ENS      have a positive contribution.<br/>* COSMO-LEPS does not add value      in general, but it is interesting in certain areas like the Alps.<br/>* There are strong spatial      dependencies that suggest that using forcings specifically for each      catchment will improve performance.<br/><br/><u>Which NWP forcing most improves forecast performance when added to an existing configuration?</u><br/><br/>* The value of multi-forcing      ensembles is proved, with increasing skill and value for larger      configurations.<br/>*Model diversity is important      (ECMWF deterministic and probabilistic are too similar) in order to      improve the hit rate, but ensemble size is more important for improving      CRPS},
   publisher = {Institute of Physics Publishing},
   title = {How do i know if I've improved my continental scale flood early warning system?},
   volume = {12},
   year = {2017},
}
@article{Demeritt2013,
   abstract = {Although ensemble prediction systems (EPS) are increasingly promoted as the scientific state-of-the-art for operational flood forecasting, the communication, perception, and use of the resulting alerts have received much less attention. Using a variety of qualitative research methods, including direct user feedback at training workshops, participant observation during site visits to 25 forecasting centres across Europe, and in-depth interviews with 69 forecasters, civil protection officials, and policy makers involved in operational flood risk management in 17 European countries, this article discusses the perception, communication, and use of European Flood Alert System (EFAS) alerts in operational flood management. In particular, this article describes how the design of EFAS alerts has evolved in response to user feedback and desires for a hydrographic-like way of visualizing EFAS outputs. It also documents a variety of forecaster perceptions about the value and skill of EFAS forecasts and the best way of using them to inform operational decision making. EFAS flood alerts were generally welcomed by flood forecasters as a sort of 'pre-alert' to spur greater internal vigilance. In most cases, however, they did not lead, by themselves, to further preparatory action or to earlier warnings to the public or emergency services. Their hesitancy to act in response to medium-term, probabilistic alerts highlights some wider institutional obstacles to the hopes in the research community that EPS will be readily embraced by operational forecasters and lead to immediate improvements in flood incident management. The EFAS experience offers lessons for other hydrological services seeking to implement EPS operationally for flood forecasting and warning. © 2012 John Wiley & Sons, Ltd.},
   author = {David Demeritt and Sebastien Nobert and Hannah L. Cloke and Florian Pappenberger},
   doi = {10.1002/hyp.9419},
   issn = {08856087},
   issue = {1},
   journal = {Hydrological Processes},
   keywords = {EFAS,EPS,Flood warnings,Risk communication},
   pages = {147-157},
   title = {The European Flood Alert System and the communication, perception, and use of ensemble predictions for operational flood risk management},
   volume = {27},
   year = {2013},
}
@article{DeRoo2000,
   abstract = {Although many geographical information systems (GISs) are very advanced in data processing and display, current GIS are not capable of physically based modelling. Especially, simulating transport of water and pollutants through landscapes is a problem in a GIS environment. A number of specific routing methods are needed in a GIS for hydrologic modelling, amongst these are the numerical solutions of the Saint-Venant equations, such as the kinematic wave approximation for transport of surface water in a landscape. The PCRaster Spatial Modelling language is a GIS capable of dynamic modelling. It has been extended recently with a kinematic wave approximation simulation tool to allow for physically based water flow modelling. The LISFLOOD model is an example of a physically based model written using the PCRaster GIS environment. The LISFLOOD model simulates river discharge in a drainage basin as a function of spatial data on topography, soils and land cover. Although hydrological modelling capabilities have largely increased, there is still a need for development of other routing methods, such a diffusion wave. Copyright (C) 2000 John Wiley and Sons, Ltd.},
   author = {A. P.J. De Roo and C. G. Wesseling and W. P.A. Van Deursen},
   doi = {10.1002/1099-1085(20000815/30)14:11/12<1981::aid-hyp49>3.0.co;2-f},
   issn = {08856087},
   issue = {11-12},
   journal = {Hydrological Processes},
   keywords = {Catchment,Floods,GIS,Hydrological modelling,Kinematic wave,LISFLOOD,PCRaster},
   pages = {1981-1992},
   publisher = {John Wiley & Sons Ltd},
   title = {Physically based river basin modelling within a GIS: The LISFLOOD model},
   volume = {14},
   year = {2000},
}
@article{Gupta2009,
   abstract = {The mean squared error (MSE) and the related normalization, the Nash–Sutcliffe efficiency (NSE), are the two criteria most widely used for calibration and evaluation of hydrological models with observed data. Here, we present a diagnostically interesting decomposition of NSE (and hence MSE), which facilitates analysis of the relative importance of its different components in the context of hydrological modelling, and show how model calibration problems can arise due to interactions among these components. The analysis is illustrated by calibrating a simple conceptual precipitation-runoff model to daily data for a number of Austrian basins having a broad range of hydro-meteorological characteristics. Evaluation of the results clearly demonstrates the problems that can be associated with any calibration based on the NSE (or MSE) criterion. While we propose and test an alternative criterion that can help to reduce model calibration problems, the primary purpose of this study is not to present an improved measure of model performance. Instead, we seek to show that there are systematic problems inherent with any optimization based on formulations related to the MSE. The analysis and results have implications to the manner in which we calibrate and evaluate environmental models; we discuss these and suggest possible ways forward that may move us towards an improved and diagnostically meaningful approach to model performance evaluation and identification.},
   author = {Hoshin V. Gupta and Harald Kling and Koray K. Yilmaz and Guillermo F. Martinez},
   doi = {10.1016/J.JHYDROL.2009.08.003},
   issn = {0022-1694},
   issue = {1-2},
   journal = {Journal of Hydrology},
   month = {10},
   note = {<b>Objetivo:</b><br/>Hacer un diagnóstico del NSE como función objetivo en la calibración de modelos hidrológicos a partir de su descomposición. A partir de ese diagnóstico se propone un nuevo criterio de optimización.<br/><br/><b>Metodología:</b><br/>Primeramente se hace un desarrollo teórico sobre la descomposición del NSE, sus limitaciones y se plantea el KGE como nuevo criterio de optimización.<br/><br/>La aplicabilidad de estos dos criterios se estudio por medio de la simulación mediante HBV de 49 cuencas a mesoescala en Austria. El modelo se calibra de dos maneras, frente al NSE y el KGE, obteniéndose dos conjuntos de parámetros para cada subcuenca. Sobre estas simulaciones se analiza el rendimiento global, los componentes del NSE/KGE y los conjuntos de parámetros optimizados.<br/><br/><b>Teorías e hipótesis:<br/></b>Se descompone el NSE en tres componentes: la correlación, el sesgo normalizado (a través de la desv. típica) y la variabilidad relativa. De esta descomposición se extraen las siguientes limitaciones del NSE:<br/>- La normalización del sesgo limita el peso específico de éste sobre el global del NSE, especialmente en cuencas con régimen muy variable.<br/>- La variabilidad aparece dos veces en el NSE y se demuestra que el máximo se obtiene cuando la variabilidad es igual a la raíz del coeficiente de correlación. NSE nunca será superior, por tanto, al coeficiente de correlación.<br/>- Se tiende a subestimar la pendiente de las regresiones observado-simulado y viceversa, por lo que se subestiman los caudales altos y sobreestiman los caudales bajos.<br/><br/>Frente al NSE, se propone un nuevo criterio de optimización, el Kling-Gupta efficiency (KGE). Es la distancia euclídea entre los tres componentes (correlación, sesgo relativo y variabilidad relativa) y sus valores óptimos (1, 0 , 1).<br/><br/><b>Resultados:</b><br/>- Fuenrte correlación entre los resultados de NSE y KGE cuando se calibra a éste último. No es así en sentido contrario.<br/>- La correlación es claramente el componente con más peso de los tres tanto para NSE como para KGE. Sin embargo, en el caso de KGE, no se debe a una menor ponderación (los tres componentes tienen el mismo peso), sino a que se consiguen valores de los otros dos componentes muy próximos al óptimo.<br/>- Mientras que ambos métodos obtienen calibraciones de la correlación similares, KGE es muy superior a NSE en la calibración del sesgo y la variabilidad relativos. La pérdida en la validación es menor con KGE, aunque aumenta mucho la variabilidad de los dos componentes.<br/>- NSE obtiene una recta de regresión casi ideal para la regresión de simulados sobre observados, pero muy mala al contrario. KGE obtiene valores similares para ambas regresiones, pero siempre ligeramente inferiores al óptimo.<br/>- Muy fuerte correlación entre los valores de los parámetros optimizados por ambos métodos (0.8). Sin embargo, pequeños cambios en los parámetros conllevan mayores cambios en el rendimiento.<br/>- Dos valores iguales de NSE (o KGE) no tienen por qué significar un mismo rendimiento. Mirando a los tres componentes del criterio, se puede discernir qué conjunto es mejor/peor que otro incluyo para un mismo NSE (KGE).<br/><br/><b>Limitaciones:</b><br/><br/><b>Preguntas sin respuesta:<br/><br/>Futura investigación:<br/></b>},
   pages = {80-91},
   publisher = {Elsevier},
   title = {Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling},
   volume = {377},
   url = {https://www.sciencedirect.com/science/article/pii/S0022169409004843},
   year = {2009},
}
@article{Hall2021,
   author = {Liam Hall},
   note = {<b>Objectives</b><br/><br/>To analyse whether including model performance or forecast skill metrics in the EFAS' notification criteria would improve its accuracy (in terms of hit rate):<br/><br/>* To find the most interesting      metrics to add value to notifications.<br/>* To determine how to include      those metrics in the notification.<br/>* To study spatial      variations.<br/><br/><b>Data</b><br/><br/>EFAS v4.0 data for its first 5 months operating (oct-2020 to april-2021).<br/><br/>* Lists of formal (72) and      informal (48) flood notifications.<br/>* Feedback received from users      on formal notifications (8).<br/><br/><b>Methods</b><br/><br/>The selected model performance metric is the modified Kling-Gupta Efficiency (KGE). Not all notifications have KGE available, since there are areas where there aren't stations available.<br/>The forecast skill is measured by the Continuous ranked Probabilistic Skill Score (CRPSS). The benchmark is the model's climatology calculated over 30 years. Skill is analysed for lead times from 1 to 10 days.<br/>In both cases, notifications are clustered in 4 groups according to the 3 quartiles.<br/><br/><b>Results</b><br/><br/><u>Model performace:</u><br/><br/>* Formal notifications. KGE and      correlation are useful as discriminatory tool.<br/>* Informal notifications. Bias      produces the most useful results.<br/>* Formal+informal notifications.      None of the metrics (KGE or its constituents) show a clear trend.<br/><br/>The spatial analysis doesn't cast any interesting results, apart from a lower performance in Southern Europe related with lack of stations and highly regulated rivers.<br/><br/><u>Forecast skill:</u><br/><br/>* Increasing CRPSS at issued lead time is not useful for improving the hit rate.<br/><br/>The spatial analysis doesn't provide clear results either.<br/><br/><b>Conclusions</b><br/><br/>* KGE can't be used a notification criteria since it is not available in some areas (Italy and Greece).<br/>* CRPSS avoids this problem, but, since it's only based on modelled data, it doesn't take into      consideration reality.<br/>* It is suggested to add performance (KGE) and skill (CRPSS) labelling, rather than criteria. In the case of CRPSS it should show the skill for the same lead time as the      notification.<br/><br/><b>Further research</b><br/><br/>* Collect more feedback.<br/>* Repeat the analysis with larger amount of data. Now there is 2 years data.<br/>* Missed events haven't been analyzed. To do so, it would be necessary to spot flood events that weren't notified by EFAS (floodlist, etc.). Increasing the heat rate should not come at the expense of a higher miss rate.<br/>* To consider the possibility of removing the deterministic requirement for EFAS formal flood notifications (is it still applicable?).<br/><br/><b>Comments</b><br/><br/>He only analyses <b>precision</b>, i.e., the true positive rate: TP /      (TP + FP). Optimising precision means that when the system sends a flood notification, indeed a flood occurs. This may lead to underpredicting floods, since missed events are not taken into account in the optimization. To avoid missing flood events, other criteria such as <b>recall</b> should be applied: TP / (TP + FN). A compromise between these two criteria would be <b>F1</b> of <b>Fbeta</b> scores, which are a trade-off between them.},
   title = {Integrating Measures of Model Performance and Forecast Skill into European Flood Awareness System (EFAS) Notifications},
   year = {2021},
}
@book{hanssen1965,
  title={On the Relationship Between the Frequency of Rain and Various Meteorological Parameters: (with Reference to the Problem Ob Objective Forecasting)},
  author={Hanssen, A.W. and Kuipers, W.J.A.},
  series={Koninkl. Nederlands Meterologisch Institut. Mededelingen en Verhandelingen},
  year={1965},
  publisher={Staatsdrukkerij- en Uitgeverijbedrijf}
}
@article{Knoben2019,
   abstract = {A traditional metric used in hydrology to summarize model performance is the Nash-Sutcliffe efficiency (NSE). Increasingly an alternative metric, the Kling-Gupta efficiency (KGE), is used instead. When NSE is used, NSE = 0 corresponds to using the mean flow as a benchmark predictor. The same reasoning is applied in various studies that use KGE as a metric: negative KGE values are viewed as bad model performance, and only positive values are seen as good model performance. Here we show that using the mean flow as a predictor does not result in KGE=0, but instead KGE = 1- √ 2 ∼ -0:41. Thus, KGE values greater than -0:41 indicate that a model improves upon the mean flow benchmark - even if the model's KGE value is negative. NSE and KGE values cannot be directly compared, because their relationship is non-unique and depends in part on the coefficient of variation of the observed time series. Therefore, modellers who use the KGE metric should not let their understanding of NSE values guide them in interpreting KGE values and instead develop new understanding based on the constitutive parts of the KGE metric and the explicit use of benchmark values to compare KGE scores against. More generally, a strong case can be made for moving away from ad hoc use of aggregated efficiency metrics and towards a framework based on purpose-dependent evaluation metrics and benchmarks that allows for more robust model adequacy assessment.},
   author = {Wouter J.M. Knoben and Jim E. Freer and Ross A. Woods},
   doi = {10.5194/hess-23-4323-2019},
   issn = {16077938},
   issue = {10},
   journal = {Hydrology and Earth System Sciences},
   month = {10},
   pages = {4323-4331},
   publisher = {Copernicus GmbH},
   title = {Technical note: Inherent benchmark or not? Comparing Nash-Sutcliffe and Kling-Gupta efficiency scores},
   volume = {23},
   year = {2019},
}
@article{Pagano2014,
   abstract = {Skillful and timely streamflow forecasts are critically important to water managers and emergency protection services. To provide these forecasts, hydrologists must predict the behavior of complex coupled human–natural systems using incomplete and uncertain information and imperfect models. Moreover, operational predictions often integrate anecdotal information and unmodeled factors. Forecasting agencies face four key challenges: 1) making the most of available data, 2) making accurate predictions using models, 3) turning hydrometeorological forecasts into effective warnings, and 4) administering an operational service. Each challenge presents a variety of research opportunities, including the development of automated quality-control algorithms for the myriad of data used in operational streamflow forecasts, data assimilation, and ensemble forecasting techniques that allow for forecaster input, methods for using human-generated weather forecasts quantitatively, and quantification of human interference in the hydrologic cycle. Furthermore, much can be done to improve the communication of probabilistic forecasts and to design a forecasting paradigm that effectively combines increasingly sophisticated forecasting technology with subjective forecaster expertise. These areas are described in detail to share a real-world perspective and focus for ongoing research endeavors.},
   author = {Thomas C. Pagano and Andrew W. Wood and Maria-Helena Ramos and Hannah L. Cloke and Florian Pappenberger and Martyn P. Clark and Michael Cranston and Dmitri Kavetski and Thibault Mathevet and Soroosh Sorooshian and Jan S. Verkade},
   doi = {10.1175/jhm-d-13-0188.1},
   issn = {1525-755X},
   issue = {4},
   journal = {Journal of Hydrometeorology},
   month = {8},
   pages = {1692-1707},
   publisher = {American Meteorological Society},
   title = {Challenges of Operational River Forecasting},
   volume = {15},
   year = {2014},
}
@article{Pappenberger2008,
   abstract = {All major weather forecast centres verify meteorological forecasts as a normal part of their operational duties, with verifications usually based on variables and methods of meteorological relevance. These forecasts are then often used to drive hydrological models, and this article demonstrates that a considerable gap exists between current meteorological practice and hydrological needs. This article discusses this gap in terms of the type of variables; the domain and resolution; the importance of choosing appropriate thresholds; and the smoothing and accumulation period. A list of recommendations for a user‐focused evaluation is given in the conclusions. We suggest that the meteorological community, and specifically the forecast centres, should consider making these adjustments and producing additional products suitable for hydrological applications. Copyright © 2008 Royal Meteorological Society},
   author = {F. Pappenberger and K. Scipal and R. Buizza},
   doi = {10.1002/asl.171},
   issn = {1530-261X},
   issue = {2},
   journal = {Atmospheric Science Letters},
   month = {4},
   pages = {43-52},
   publisher = {Wiley},
   title = {Hydrological aspects of meteorological verification},
   volume = {9},
   year = {2008},
}
@article{Pappenberger2015,
   abstract = {The skill of a forecast can be assessed by comparing the relative proximity of both the forecast and a benchmark to the observations. Example benchmarks include climatology or a naïve forecast. Hydrological ensemble prediction systems (HEPS) are currently transforming the hydrological forecasting environment but in this new field there is little information to guide researchers and operational forecasters on how benchmarks can be best used to evaluate their probabilistic forecasts. In this study, it is identified that the forecast skill calculated can vary depending on the benchmark selected and that the selection of a benchmark for determining forecasting system skill is sensitive to a number of hydrological and system factors. A benchmark intercomparison experiment is then undertaken using the continuous ranked probability score (CRPS), a reference forecasting system and a suite of 23 different methods to derive benchmarks. The benchmarks are assessed within the operational set-up of the European Flood Awareness System (EFAS) to determine those that are 'toughest to beat' and so give the most robust discrimination of forecast skill, particularly for the spatial average fields that EFAS relies upon.Evaluating against an observed discharge proxy the benchmark that has most utility for EFAS and avoids the most naïve skill across different hydrological situations is found to be meteorological persistency. This benchmark uses the latest meteorological observations of precipitation and temperature to drive the hydrological model. Hydrological long term average benchmarks, which are currently used in EFAS, are very easily beaten by the forecasting system and the use of these produces much naïve skill. When decomposed into seasons, the advanced meteorological benchmarks, which make use of meteorological observations from the past 20. years at the same calendar date, have the most skill discrimination. They are also good at discriminating skill in low flows and for all catchment sizes. Simpler meteorological benchmarks are particularly useful for high flows. Recommendations for EFAS are to move to routine use of meteorological persistency, an advanced meteorological benchmark and a simple meteorological benchmark in order to provide a robust evaluation of forecast skill. This work provides the first comprehensive evidence on how benchmarks can be used in evaluation of skill in probabilistic hydrological forecasts and which benchmarks are most useful for skill discrimination and avoidance of naïve skill in a large scale HEPS. It is recommended that all HEPS use the evidence and methodology provided here to evaluate which benchmarks to employ; so forecasters can have trust in their skill evaluation and will have confidence that their forecasts are indeed better.},
   author = {F. Pappenberger and M. H. Ramos and H. L. Cloke and F. Wetterhall and L. Alfieri and K. Bogner and A. Mueller and P. Salamon},
   doi = {10.1016/j.jhydrol.2015.01.024},
   issn = {00221694},
   journal = {Journal of Hydrology},
   keywords = {Benchmark,Evaluation,Forecast performance,Hydrological ensemble prediction,Probabilistic forecasts,Verification},
   month = {3},
   note = {<b>Summary:</b><br/><br/>The<b> idea</b> behind this paper is that the model benchmark used to assess the skill of a forecast must be chosen with consideration. Naïve benchmarks do not discriminate correctly the forecast skill, whereas the authors state the idea that the tougher to bit is the benchmark, the better it will discriminate the skill forecast.<br/><br/><b>Objectives:</b><br/><br/>To demonstrate how the      calculated forecast skill vary depending on the benchmark.<br/>To demonstrate that the      discrimination skill of a benchmark depends on some hydrological      features.<br/>To show how a benchmark      intercomparison can be made for a large scale forecasting system.<br/><br/><b>Methods:</b><br/><br/>23 benchmarks (climatological and change-signal):<br/>5 simple meteorological      benchmarks.<br/>5 simple meteorological      benchmarks with zero precitpitation.<br/>2 advanced meteorological      benchmarks.<br/>2 advanced meteorological      benchmarks with zero precipitation.<br/>5 simple hydrological      benchmarks.<br/>4 climatic hydrological      benchmarks.<br/>The skill score used is the continuous ranked probability score (CRPS) or the threshold-weighted continuous rank probability score (CRPSt).<br/>Benchmarks are compared against an observed discharge proxy under several conditions:<br/>Full hydrograph, including      a sesonal analysis<br/>Decomposed hydrograph: rising      and falling limbs<br/>High and low flows<br/>Catchment response time and      size<br/>For some points where there are stations, benchmarks are compared against gauged discharge.<br/><br/><b>Results:</b><br/><br/>In general, the results show that the advanced meteorological benchmarks have lower CRPS, which means that they are tougher to beat and better discriminators.<br/>In an operational system such us EFAS a compromise between the power of a benchmark and its computational costs. Meteorological benchmarks are more expensive than hydrological ones since they require running the hydrological model.<br/>The paper recommends for EFAS to implement an advanced meteorological benchmark: observations of temperature and precipitation from the past 20 years at the same calendar day.},
   pages = {697-713},
   publisher = {Elsevier},
   title = {How do I know if my forecasts are better? Using benchmarks in hydrological ensemble prediction},
   volume = {522},
   year = {2015},
}
@article{Ramos2007,
   abstract = {The study presents the development of flood warning decision support products based on ensemble forecasts in the European Flood Alert System (EFAS). EFAS aims to extend the lead time of flood forecasts to 3-10 days in transnational river basins and complement Member States' activities on flood forecasting. Weather forecasts are used as input to a hydrological model and simulated discharges are evaluated for exceedances of flood thresholds. Products were developed in collaboration with users for a concise and useful visualization of probabilistic results. Forecasts of flood events observed in the Danube river basin in 2005 illustrate the analysis. Copyright © 2007 Royal Meteorological Society.},
   author = {Maria Helena Ramos and Jens Bartholmes and Jutta Thielen-del Pozo},
   doi = {10.1002/asl.161},
   issn = {1530261X},
   issue = {4},
   journal = {Atmospheric Science Letters},
   keywords = {Ensemble prediction,Flood forecasting,Uncertainty,Warning},
   month = {10},
   note = {<b>Objective</b><br/><br/>To develop informative decision support products derived from the ensemble hydrological forecast made by EFAS.<br/>An example case study was conducted for the Danube 2005 floods to specifically analyze the influence of the probability threshold in the hit, miss and false rates.<br/><br/><b>Notes</b><br/>At that time, the persistence      threshold in order to issue an alert was 2 consecutive forecasts.},
   pages = {113-119},
   title = {Development of decision support products based on ensemble forecasts in the European flood alert system},
   volume = {8},
   year = {2007},
}
@article{Raynaud2015,
   abstract = {Flash floods are listed among the deadliest and costliest weather-driven hazard worldwide. Yet, only a few systems to predict flash floods run operationally in Europe. Recently, the European Precipitation Index based on Climatology (EPIC) was developed and then set up for daily flash flood early warning for an area covering most of the continent. EPIC is a purely rainfall-driven indicator based on the prediction of statistical threshold exceedence of the upstream precipitation to provide early warning up to 5days in advance. Its main assumption is that flash floods are directly and solely related to extreme accumulations of upstream precipitation. It does not take into account any geo-factors such as slope and land use or processes such as initial soil moisture, which can have a significant impact on the triggering of such events. This study proposes an enhanced version of EPIC through a dynamic and distributed runoff co-efficient which depends on the initial soil moisture. This co-efficient, namely the European Runoff Index based on Climatology (ERIC), is used to weigh each contribution of the upstream precipitation proportionally to the initial soil moisture. The evaluation based on 1year of daily runs proved that ERIC reaches a threat score of 0.5 if it forecasts a probability >35% of exceeding the 20year return period of upstream runoff. This result is 0.16 higher than for EPIC. A case study of the flash flooding affecting central Europe in June 2013 demonstrated the ability of ERIC to successfully detect and locate the affected areas.},
   author = {D. Raynaud and J. Thielen and P. Salamon and P. Burek and S. Anquetin and L. Alfieri},
   doi = {10.1002/met.1469},
   issn = {14698080},
   issue = {3},
   journal = {Meteorological Applications},
   keywords = {Early warning,Flash flood,Probabilistic forecast,Runoff co-efficient,Ungauged basins},
   month = {7},
   note = {<b>Summary</b><br/>The article presents the European Runoff Index based on Climatoloty (ERIC) methodology to predict flash floods. The methods builds on the previous European Precipitation Index based on Climatology (EPIC), but instead of being based only in precipitation, it is based on runoff (precipitation * runoff coefficient).<br/><br/>The notification criteria are tested on a complete year of data using a data set of observed flash floods as comparison. The selected criteria is the 35% probability of exceeding the 20 year return period in at least two cells within a 30 km radius. Persistence was tested but skill degraded.<br/><br/>The paper also shows the performance of ERIC in a flash flood in June 2013 in Germany, Austria, Cezch Republic.},
   pages = {410-418},
   publisher = {John Wiley and Sons Ltd},
   title = {A dynamic runoff co-efficient to improve flash flood early warning in Europe: Evaluation on the 2013 central European floods in Germany},
   volume = {22},
   year = {2015},
}
@article{Roebber2009,
   abstract = {A method for visually representing multiple measures of dichotomous (yes-no) forecast quality (probability of detection, false alarm ratio, bias, and critical success index) in a single diagram is presented. Illustration of the method is provided using performance statistics from two previously published forecast verification studies (snowfall density and convective initiation) and a verification of several new forecast datasets: Storm Prediction Center forecasts of severe storms (nontornadic and tornadic), Hydrometeorological Prediction Center forecasts of heavy precipitation (greater than 12.5 mm in a 6-h period), National Weather Service Forecast Office terminal aviation forecasts (ceiling and visibility), and medium-range ensemble forecasts of 500-hPa height anomalies. The use of such verification metrics in concert with more detailed investigations to advance forecasting is briefly discussed. © 2009 American Meteorological Society.},
   author = {Paul J. Roebber},
   doi = {10.1175/2008WAF2222159.1},
   issn = {08828156},
   issue = {2},
   journal = {Weather and Forecasting},
   month = {4},
   pages = {601-608},
   title = {Visualizing multiple measures of forecast quality},
   volume = {24},
   year = {2009},
}
@article{Skoien2021,
   abstract = {Different postprocessing techniques are frequently employed to improve the outcome of ensemble forecasting models. The main reason is to compensate for biases caused by errors in model structure or initial conditions, and as a correction for under-or overdispersed ensembles. Here we use the ensemble model output statistics method to postprocess the ensemble output from a continental-scale hydrological model, LISFLOOD, as used in the European Flood Awareness System (EFAS). We develop a method for local calibration and interpolation of the postprocessing parameters and compare it with a more traditional global calibration approach for 678 stations in Europe based on long-term observations of runoff and meteorological variables. For the global calibration we also test a reduced model with only a variance inflation factor. Whereas the postprocessing improved the results for the first 1–2 days lead time, the improvement was less for increasing lead times of the verification period. This was the case both for the local and global calibration methods. As the postprocessing is based on assumptions about the distribution of forecast errors, we also present an analysis of the ensemble output that provides some indications of what to expect from the postprocessing.},
   author = {Jon Olav Skøien and Konrad Bogner and Peter Salamon and Fredrik Wetterhall},
   doi = {10.1175/JHM-D-21-0008.1},
   issn = {15257541},
   issue = {10},
   journal = {Journal of Hydrometeorology},
   keywords = {Ensembles,Forecasting,Interpolation schemes,Operational forecasting,Probability forecasts/models/distribution,Statistical techniques},
   month = {10},
   note = {<b>Objectives</b><br/><br/>*To interpolate locally calibrated postprocessing parameters with the geostatistical method  top-kriging.<br/>* To analyse the  effect of transformation methods such as Box-Cox or GEV in the forecast      skill.<br/><br/><b>Data</b><br/><br/><u>Forecast</u><br/><br/>3 years discharge      (2015-2017) with up to 10 days lead time simulated with LISFLOOD. A total      of 69 simulations: 1 ECMWF deterministic, 51 ECMWF stochastic, 1 DWD, 16      COSMO-LEPS<br/><br/><u>Observations</u><br/><br/>discharge proxy      simulated with LISFLOOD using interpolated observed precipitation and      temperature.<br/><br/><b>Methods</b><br/><br/><u>Transformations</u><br/><br/>To apply postprocessing data must be normally distributed. Several transformation methods are tested (normal score transformation, versions of Box-Cox transformation, versions of GEV...) on the discharge data.<br/><br/><u>Ensemble model output statistics (EMOS) </u>(Gneiting et al., 2005)<br/>Its objective is to minimize the continuous ranked probability score (CRPS). The forecast will be calculated as a weighted mean of the ensembles. Optimizing that linear equation will render an unbiased mean forecast, but variance must also be optimize, for which another linear approximation is used. In that way, the CRPS can be analitically solved supposing that the predictive distribution functions is normal.<br/>Top-kriging to interpolate parameters<br/>It is applied to spatial variables (in this case specific discharge) to include in the interpolation the varying spatial support.<br/><br/><u>EMOS calibration</u><br/><br/>* Global calibration: one set      of EMOS parameters for the entire region<br/>* Local calibration:      individually for each location. To avoid equifinality, a spatial penalty      is tested, to induce the calibration into similar parameter sets in      neighboring catchments.<br/>* SCEUA<br/>* 1 year calibration      period<br/><br/><u>Validation</u><br/><br/>* Case 1:  difference in the calibration results between methods.<br/>* Case 2: cross-validation to identify sources of error.<br/>* Case 3: temporal verification.<br/>* Case 4: spatiotemporal cross-validation.<br/><br/>The score used is the continuous ranked probability skill score (CRPSS). The benchmark is the mean of the forecasts (ensembles are previously averaged) and the variance of a global ensemble. They also use probability integral transformation (PIT) as a visual indicator.<br/><br/><b>Conclusions:</b><br/><br/><u>Objective 1</u><br/><br/>the spatial      interpolation of parameters could be useful if there was stronger need for      postprocessing<br/><br/><u>Objective 2</u><br/><br/>none of the      transformations give substantially better results<br/>Since an operational service requires simplicity, only calibration the variance inflation factor would be the best option.},
   pages = {2731-2749},
   publisher = {American Meteorological Society},
   title = {On the Implementation of Postprocessing of Runoff Forecast Ensembles},
   volume = {22},
   year = {2021},
}
@article{Thielen2009a,
   abstract = {This paper presents the development of the Eu-ropean Flood Alert System (EFAS), which aims at increasing preparedness for floods in trans-national European river basins by providing local water authorities with medium-range and probabilistic flood forecasting information 3 to 10 days in advance. The EFAS research project started in 2003 with the development of a prototype at the European Commission Joint Research Centre (JRC), in close collaboration with the national hydrological and meteorological services. The prototype covers the whole of Europe on a 5 km grid. In parallel, different high-resolution data sets have been collected for the Elbe and Danube river basins, allowing the potential of the system under optimum conditions and on a higher resolution to be assessed. Flood warning lead-times of 3-10 days are achieved through the incorporation of medium-range weather forecasts from the Ger-man Weather Service (DWD) and the European Centre for Medium-Range Weather Forecasts (ECMWF), comprising a full set of 51 probabilistic forecasts from the Ensemble Prediction System (EPS) provided by ECMWF. The ensemble of different hydrographs is analysed and combined to produce early flood warning information, which is disseminated to the hydrological services that have agreed to participate in the development of the system. In Part 1 of this paper, the scientific approach adopted in the development of the system is presented. The rational of the project, the systems setup , its underlying components, basic principles and products are described. In Part 2, results of a detailed statistical analysis of the performance of the system are shown, with regard to both probabilistic and deterministic forecasts.},
   author = {J Thielen and J Bartholmes and M.-H Ramos and A De Roo},
   doi = {10.5194/hess-13-125-2009},
   journal = {Hydrol. Earth Syst. Sci},
   pages = {125-140},
   title = {Hydrology and Earth System Sciences The European Flood Alert System – Part 1: Concept and development},
   volume = {13},
   url = {www.hydrol-earth-syst-sci.net/13/125/2009/},
   year = {2009},
}
@article{Thielen2009b,
   abstract = {This paper describes a case study that explores the limits of the predictability of floods, by combining forecasts with multiple spatial and temporal resolutions. Monthly, medium- and short range numerical weather prediction (NWP) data are input to the European Flood Alert System for a flood event that affected rivers in Romania in October 2007. The NWP data comprise ensembles and deterministic forecasts of different spatial resolutions and lead times from different weather prediction models. Results are explored in terms of the individual NWP components as well as the ensemble. In this case study, ensembles of monthly weather forecasts contribute only marginally to the early warning, although some indication is given as early as 3 weeks before the event. The 15-day medium-range weather forecasts produce early flood warning information 9 to 11 days in advance. As the event draws nearer and is in range to be captured by the higher resolution ensemble forecasts, the spatial extent of the event is forecast with much more precision than with the medium-range. A novel post-processing method for the calculation of river discharge is applied to those stations where observations are available, and is able to correct for time-shifts and to improve the quantitative forecast. The study illustrates how a combination of forecasts and post-processing improves the lead time for early flood warnings by 2 to 3 days, while remaining reliable also in the short-range. © 2009 Royal Meteorological Society.},
   author = {J. Thielen and K. Bogner and F. Pappenberger and M. Kalas and M. del Medico and A. de Roo},
   doi = {10.1002/met.140},
   issn = {14698080},
   issue = {1},
   journal = {Meteorological Applications},
   keywords = {ARMAXSS,COSMO-LEPS,Discharge post-processing,ECMWF,EFAS,Early flood warning,Ensemble flood forecasting,Skill scores},
   note = {<b>Objective:</b><br/>To explore the predictability of floods using forecast of different nature (probabilistic or deterministic) and lead time (monthly, medium-range and short-range).<br/>To analyze the ability of post-processing to improve quantitatively forecasts.<br/><b>Data:</b><br/><br/>They analyze a flood event in the Danube river in Romania in October 2007.<br/><br/><b>Conclusions:</b><br/>Monthly forecasts can indicate potential flooding with 3-4 weeks in advance, but without a precise location in space and time.<br/>Medium-range forecasts show good results qualitative and quantitatively, being able to predict floods 7-9 days in advance.<br/>Deterministic forecasts are less coherent than probabilistic ones. This is applicable both at medium and short-range forecasts.<br/>Post-processing discharge ensembles can reduce uncertainty.},
   pages = {77-90},
   publisher = {John Wiley and Sons Ltd},
   title = {Monthly-, medium-, and short-range flood warning: Testing the limits of predictability},
   volume = {16},
   year = {2009},
}
@article{Wetterhall2013,
   abstract = {Hydrological ensemble prediction systems (HEPS) have in recent years been increasingly used for the operational forecasting of floods by European hydrometeorological agencies. The most obvious advantage of HEPS is that more of the uncertainty in the modelling system can be assessed. In addition, ensemble prediction systems generally have better skill than deterministic systems both in the terms of the mean forecast performance and the potential forecasting of extreme events. Research efforts have so far mostly been devoted to the improvement of the physical and technical aspects of the model systems, such as increased resolution in time and space and better description of physical processes. Developments like these are certainly needed; however, in this paper we argue that there are other areas of HEPS that need urgent attention. This was also the result from a group exercise and a survey conducted to operational forecasters within the European Flood Awareness System (EFAS) to identify the top priorities of improvement regarding their own system. They turned out to span a range of areas, the most popular being to include verification of an assessment of past forecast performance, a multi-model approach for hydrological modelling, to increase the forecast skill on the medium range (>3 days) and more focus on education and training on the interpretation of forecasts. In light of limited resources, we suggest a simple model to classify the identified priorities in terms of their cost and complexity to decide in which order to tackle them. This model is then used to create an action plan of short-, medium- and long-term research priorities with the ultimate goal of an optimal improvement of EFAS in particular and to spur the development of operational HEPS in general. © 2013 Author(s).},
   author = {F. Wetterhall and F. Pappenberger and L. Alfieri and H. L. Cloke and J. Thielen-Del Pozo and S. Balabanova and J. Daňhelka and A. Vogelbacher and P. Salamon and I. Carrasco and A. J. Cabrera-Tordera and M. Corzo-Toscano and M. Garcia-Padilla and R. J. Garcia-Sanchez and C. Ardilouze and S. Jurela and B. Terek and A. Csik and J. Casey and G. Stankunavičius and V. Ceres and E. Sprokkereef and J. Stam and E. Anghel and D. Vladikovic and C. Alionte Eklund and N. Hjerdt and H. Djerv and F. Holmberg and J. Nilsson and K. Nyström and M. Sušnik and M. Hazlinger and M. Holubecka},
   doi = {10.5194/hess-17-4389-2013},
   issn = {16077938},
   issue = {11},
   journal = {Hydrology and Earth System Sciences},
   pages = {4389-4399},
   publisher = {European Geosciences Union},
   title = {HESS Opinions "forecaster priorities for improving probabilistic flood forecasts"},
   volume = {17},
   year = {2013},
}