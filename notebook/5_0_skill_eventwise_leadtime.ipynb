{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Skill assessment - eventwise\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 16-03-2023<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "In this notebook I will analyse the EFAS skill in predicting flood events in general, i.e., looking whether events where predicted at some point in time, regardless of neither the offset nor the duration of the event.\n",
    "\n",
    "**Questions**:<br>\n",
    "\n",
    "* [ ] Take into account the model spread?\n",
    "* [ ] Aggregate results by river/administrative area? EFAS aims at alerting administrations about incoming events in there administrative area, shouldn't that aggregation be included in the results?\n",
    "* [ ] Remove extremely bad performing stations.\n",
    "\n",
    "**Pending tasks**:<br>\n",
    "\n",
    "* [x] Weighting the model average by the Brier score?\n",
    "* [x] Sort stations by catchment area (or other order)?\n",
    "* [x] Persistence\n",
    "* [ ] Analyse only the periods/stations close to an observed event and compute f1 for this extraction. Later on, on the complementary subset of data another metric must be computed to avoid false positives, p.e., false alarm ratio.\n",
    "\n",
    "\n",
    "\n",
    "**Interesting links**<br>\n",
    "[Evaluation metrics for imbalanced classification](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/)<br>\n",
    "[Cross entropy for machine learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)<br>\n",
    "[Probability metrics for imbalanced classification](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/)<br>\n",
    "[ROC curves and precision-recall curves for imbalanced classification](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/)<br>\n",
    "[Instructions for sending EFAS flood notifications](https://efascom.smhi.se/confluence/display/EDC/Instructions+for+sending%2C+upgrading+and+deactivating+EFAS+Flood+Notifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "os.chdir('../py/')\n",
    "from computations import *\n",
    "from plots import *\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e0fda-fb63-48b5-9cc3-679d702d934a",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d534e-e8e3-410f-8bc1-6ab40dc0c903",
   "metadata": {},
   "source": [
    "### 1.1 Notification criteria\n",
    "\n",
    "#### Probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4265e9c6-daf0-4538-b5f4-504bf79f0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability thresholds\n",
    "thresholds = np.arange(.05, .96, .05).round(2)\n",
    "# thresholds = np.round(sigmoid(np.linspace(-10, 10, 50)), 5)\n",
    "probabilities = xr.DataArray(thresholds, dims=['probability'], coords={'probability': thresholds})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b5514-f62d-4efc-81d1-bf88a9849dfd",
   "metadata": {},
   "source": [
    "#### Persistence\n",
    "\n",
    "A list of tuples with two values: the first value is the width of the window rolling sum, and the second value the minimum number of positives in that window so that a notification is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd790a6-704e-402c-8d1a-79a25cc1ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = [(2, 2), (2, 3), (3, 3), (2, 4), (3, 4)]\n",
    "persistence = {'/'.join([str(i) for i in pers]): pers for pers in persistence}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537e30d-2cc1-4560-895e-480c70d3a79d",
   "metadata": {},
   "source": [
    "#### Leadtime\n",
    "\n",
    "Notifications are only sent with a minimum leadtime (h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0059b69c-bdd6-4369-9e0f-d0a9415f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_leadtime = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c8ad7-1c86-4146-800b-0b6fc8be8862",
   "metadata": {},
   "source": [
    "### 1.2 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a0a6a6-bcbd-4056-8b6c-0eae6a8dacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'EFAS'\n",
    "\n",
    "# path where results will be saved\n",
    "results_path = '../results/'\n",
    "path_out = f'{results_path}skill/{name}/eventwise/'\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf9fefe-8375-44af-b1c0-f1886b763c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate reference system when plotting maps\n",
    "proj = ccrs.LambertAzimuthalEqualArea(central_longitude=10, central_latitude=52, false_easting=4321000, false_northing=3210000, globe=ccrs.Globe(ellipse='GRS80'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1667-e5d2-45fb-83a1-647f9c9c4e7a",
   "metadata": {},
   "source": [
    "## 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b284a86-2d64-4907-a259-e3c67b6de2d5",
   "metadata": {},
   "source": [
    "### 2.1 Stations\n",
    "\n",
    "I load all the stations that where selected in a previous [notebook](3_0_select_stations.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad2ba6a-e095-4c1c-8f2c-3738dee497be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. stations:\t\t\t900\n"
     ]
    }
   ],
   "source": [
    "# load selected points for all the catchments\n",
    "stations = pd.DataFrame()\n",
    "catchments = []\n",
    "folders = os.listdir(f'{results_path}reporting_points/')\n",
    "for folder in folders:\n",
    "    try:\n",
    "        stn_cat = pd.read_csv(f'{results_path}reporting_points/{folder}/points_selected.csv', index_col='station_id')\n",
    "        stations = pd.concat((stations, stn_cat))\n",
    "        catchments.append(folder)\n",
    "    except:\n",
    "        continue\n",
    "print('no. stations:\\t\\t\\t{0}'.format(stations.shape[0]))\n",
    "\n",
    "# remove unnecessary attributes\n",
    "stations.drop(['rl1.5', 'rl2', 'rl10', 'rl20', 'rl50', 'rl100', 'rl200', 'rl500', 'strahler', 'pfafstetter', 'country'], axis=1, inplace=True)\n",
    "\n",
    "# convert into integers\n",
    "stations[['X', 'Y', 'area', 'rl5']] = stations[['X', 'Y', 'area', 'rl5']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6898ef9-0c78-4b77-959b-fb87de8716c5",
   "metadata": {},
   "source": [
    "### 1.2 Forecast: exceedance\n",
    "\n",
    "I load the preprocessed forecast data. In a previous [notebook](4_0_forecast_exceedance_review.ipynb), the forecasted discharge was converted to probability of exceeding the 5-year return period threshold. The ouput of that process is a NetCDF file per station with the forecasted probability of exceedance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c38388-8774-48d5-b80a-a9ae0884243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find NetCDF files\n",
    "path_forecast = '../data/exceedance/forecast/'\n",
    "files = glob.glob(f'{path_forecast}*.nc')\n",
    "files = [file for file in files if int(file.split(sep='\\\\')[-1].split('/')[-1].split('.')[0]) in stations.index]\n",
    "\n",
    "# load data to a chunked DataArray\n",
    "fore_exc = xr.open_mfdataset(files, combine='nested', concat_dim='id', chunks={'id': 1}).exceedance\n",
    "fore_exc['id'] = fore_exc.id.astype(int)\n",
    "\n",
    "# study period based on the extent of the forecast data\n",
    "#start, end = [(fore_exc.forecast[i] + fore_exc.leadtime[i]).data for i in [0, -1]]\n",
    "#start, end = [(date - np.datetime64('1970-01-01T01:00:00Z')) / np.timedelta64(1, 's') for date in [start, end]]\n",
    "#start, end = [datetime.fromtimestamp(timestamp) for timestamp in [start, end]]\n",
    "\n",
    "#print(f'Study period\\nstart:\\t{start}\\nend:\\t{end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e04c8-3020-45b6-a0f6-96ecae47fcff",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Reformat data\n",
    "\n",
    "To be able to compute skill in a simple manner, I need to reshape the exceedance forecast. Up until now, the exceedance forecast is saved in a matrix where the `forecast` is one dimension, and the `leadtime` is another dimension, in a way that there aren't missing values in the matrix. However, this shape is not convenient for comparing against the observed exceedances of the discharge threshold. Instead, I will reshape the original exceendance forecast into a new matrix in which a dimension named `datetime` represents actual date and time, and another dimension represents leadtime. In this way, a column in the matrix represents the same timestep and can be easily compared against observations. The drawback of this approach is that it will create missing values in the lower-left and upper-right corners of the matrix; therefore, I will remove these parts of the matrix (`trim=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f7332-fd90-4896-8eb5-0ff41f5ede86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the DataArray of forecasted exceedance\n",
    "pred = reshape_DataArray(fore_exc, trim=True, chunks={'id': 1})\n",
    "del fore_exc\n",
    "\n",
    "print(pred.dims)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c23cc9-47d1-45a9-9387-66788d90aed1",
   "metadata": {},
   "source": [
    "### 1.3 Reanalysis: exceedance & events\n",
    "\n",
    "I load the preprocessed reanalysis data. In a previous [notebook](2_2_reanalysis_preprocessing.ipynb), the reanalysis discharge data was preprocessed to create timeseries of exceedance over the 5-year return period threshold.\n",
    "\n",
    "Out of the exceedance timeseries I calculate another timeseries of the onset of flood events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c62d689-6c50-4751-9f83-68ebaff3ec20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m rean_exc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_reanalysis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/exceedance_rl5.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m rean_exc\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m rean_exc\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m rean_exc \u001b[38;5;241m=\u001b[39m rean_exc\u001b[38;5;241m.\u001b[39mloc[pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mpred\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mdata), stations\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# compute onsets of the flood events\u001b[39;00m\n\u001b[0;32m      9\u001b[0m rean_onsets \u001b[38;5;241m=\u001b[39m rean_exc\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mdiff(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "path_reanalysis = '../data/exceedance/reanalysis/'\n",
    "\n",
    "# load probability of exceeding the discharge threshold in the REANALYSIS data\n",
    "rean_exc = pd.read_parquet(f'{path_reanalysis}/exceedance_rl5.parquet')\n",
    "rean_exc.columns = rean_exc.columns.astype(int)\n",
    "rean_exc = rean_exc.loc[pd.to_datetime(pred.datetime.data), stations.index.tolist()]\n",
    "\n",
    "# compute onsets of the flood events\n",
    "rean_onsets = rean_exc.astype(int).diff(axis=0) == 1\n",
    "rean_onsets.iloc[0,:] = rean_exc.iloc[0,:]\n",
    "\n",
    "# create a DataArray with the number of observed events per station\n",
    "n_events_obs = xr.DataArray(rean_onsets.sum(), dims=['id'], coords={'id': rean_onsets.columns.tolist()})\n",
    "stations['n_events_5'] = n_events_obs.to_pandas()\n",
    "print('no. events:\\t\\t\\t{0}'.format(n_events_obs.sum().data))\n",
    "del rean_onsets, rean_exc\n",
    "\n",
    "# select stations with events\n",
    "mask_stn = (n_events_obs > 0).to_pandas()\n",
    "print('no. stations with events:\\t{0}'.format(mask_stn.sum()))\n",
    "\n",
    "# colormap used for the maps\n",
    "cmap, norm = create_cmap('Reds', np.arange(0, n_events_obs.max(), 1), 'no. events', [0, (.5, .5, .5, .5)])\n",
    "plot_map_stations(stations.X, stations.Y, n_events_obs.to_pandas(), mask=~mask_stn,\n",
    "                  cmap=cmap, norm=norm, size=5, figsize=(8, 8),alpha=.7)\n",
    "plt.colorbar(plot_map_stations.colorbar, shrink=.33, label='no. events')\n",
    "plt.savefig(f'{path_out}/map_observed_events.png', dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9aac8-121d-4fa4-abcf-994389d8e552",
   "metadata": {},
   "source": [
    "> ***Figure 1**. Number of observed flood events in the selected reporting points.Small gray dots represent points without observed events.*\n",
    "\n",
    "The number of events has increased after giving priority to reporting points upstream during the selection process. Out of the 900 selected reporting points, 322 suffered a flood event in the last two years, summing up to a total of 526 observed flood events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ce24a-bac0-479a-acc1-1759afb27150",
   "metadata": {},
   "source": [
    "##### Reformat data\n",
    "\n",
    "I convert the reanalysis data into a `xarray.DataArray`, which will be useful in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518df41-719d-4c4f-9d7b-e3b63af21aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataArray with observed threshold exceedance\n",
    "obs = df2da(rean_exc, dims=['id', 'datetime'], plot=False, figsize=(16, 20), title='observed exceendace')\n",
    "del rean_exc\n",
    "\n",
    "# expected probability of an exceedance\n",
    "obs = obs.astype(int)\n",
    "\n",
    "print(obs.dims)\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24db9-d53a-4325-9360-463ce52cb737",
   "metadata": {},
   "source": [
    "The following plot is a graphical explanation of the new format in which forecast exceedance probability is stored. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9a1f0-2e0d-475a-9b19-cc35c9ff5b6b",
   "metadata": {},
   "source": [
    "## 3 Assess skill\n",
    "\n",
    "In this section I will compute the skill of the EFAS predictions in different ways. In all the following sections I will work with three metrics: recall, precision and the f1-score. The three metrics are based in the contingency table of hits ($TP$ for true positives), false alarms ($FP$ for false positives) and misses ($FN$ for false negatives).\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$f1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FN + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a33f6-8bc8-4bf6-b873-296924918c03",
   "metadata": {},
   "source": [
    "### 3.1 Brier score: a probability assessment\n",
    "\n",
    "This score is specific for binary classification problems and, since it evaluates only positive cases, it is suited for imbalance datasets. The Brier score is the mean square error between predicted and expeted probabilities:\n",
    "\n",
    "$$BS = \\frac{\\sum (p_{pred} - p_{obs})^2}{N}$$ \n",
    "\n",
    "As an error mesure, the Brier score is better the closer to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551575e-3695-49a4-a6f6-9055f03dd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared error\n",
    "se = (obs - pred)**2\n",
    "\n",
    "# Brier skill of each model and station\n",
    "brier = se.mean(['id', 'datetime'])\n",
    "\n",
    "# save the Brier score matrix\n",
    "brier.to_netcdf(f'{path_out}brier.nc')\n",
    "\n",
    "# plot Brier score according to model and leadtime\n",
    "plot_DataArray(brier, xtick_step=1, ytick_step=1, cmap='magma_r', xlabel='leadtime (h)',\n",
    "        figsize=(7.5, 2), cbar_kws={'label': 'Brier (-)', 'shrink': .666})\n",
    "# plt.savefig(f'{path_out}leadtime_Brier_all_stations.jpg', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# plot Brier score according to model and station\n",
    "#plot_DataArray(se.mean(['leadtime', 'datetime']).transpose(), yticklabels=se.model.data, ytick_step=1, xticklabels=se.id.data, xtick_step=30,\n",
    "#        cmap='magma_r', cbar_kws={'label': 'Brier (-)', 'shrink': .666})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f0af6-8081-46b3-b885-44f0d4f07b62",
   "metadata": {},
   "source": [
    "> ***Figure 2**. Brier score depending on the meteorological forcing and the leadtime.*\n",
    "\n",
    "Figure 2 proves that probabilistic forcings have a higher skill (lower Brier score) than the deterministic ones. The skill improves with shorter leadtimes, with the exception of the first day. This matrix of model and leadtime skill can be interesting to weigh models when calculating total probability.\n",
    "\n",
    "The Brier score has the shortcoming that it's dimensionless (in our case values seem always very low), so it's difficult to compare models. To work around this issue, the **Brier skill score** computes the skill of a model relative to a benchmark model, allowing model comparison:\n",
    "\n",
    "$$BSS = 1 - \\frac{BS_i}{BS_{ref}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9478cbf-8720-4766-94a5-d6915dcc8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brier skill score in terms of NWP model\n",
    "BSS = 1 - brier.mean('leadtime') / brier.sel(model='EUD').mean('leadtime')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.scatter(range(0, 4), BSS.data, s=8)\n",
    "ax.hlines(0, -1, 5, 'k', ls='--', lw=1, zorder=0)\n",
    "ax.set_xticks(range(0, 4), BSS.model.data)\n",
    "ax.set(xlim=(-.5, 3.5), ylim=(-.5, .5), ylabel='Brier skill score (-)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663a626-4baf-428b-abbd-cd82172335b1",
   "metadata": {},
   "source": [
    "> ***Figure 3**. Brier skill score taking the ECMWF deterministic as a benchmark model.*\n",
    "\n",
    "The previous plot shows the Brier skill score using as a benchmark the ECMWF deterministic EUD. Positive values represent models better than the reference, whereas negative values represent models worse than the reference. As seen previously, the two probabilistic models have a higher skill than the reference, with ECMWF ensemble as the best forcing. The DWD is the worst of all the forcings. As expected, skill degrades with increasing leadtimes.\n",
    "\n",
    "We can also look at the geographical distribution of the Brier score according to the meteorological forcing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac059f5f-dc01-4288-a79a-1698268be338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a map of the Brier score \n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), subplot_kw={'projection': proj}, constrained_layout=True)\n",
    "for ax, model in zip(axes.flatten(), brier.model.data):\n",
    "    plot_map_stations(stations.X, stations.Y, se.mean(['model', 'leadtime', 'datetime']).to_pandas(), ax=ax,\n",
    "                      rivers=None, size=5, cmap='magma_r', title=model)#, vmin=0, vmax=.022)\n",
    "fig.colorbar(plot_map_stations.colorbar, ax=axes[:,1], shrink=.333, label=f'Brier score (-)');\n",
    "\n",
    "# plt.savefig(f'{path_out}eventwise_Brier_all_stations.jpg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c02902-7f06-4700-9b79-9afe0fbbe9e2",
   "metadata": {},
   "source": [
    "> ***Figure 4**. Brier score by reporting point and meteorological forcing.*\n",
    "\n",
    "There are a lot of reporting points with Brier equal to 0, most of them represent stations with neither observed nor predicted events. There are cluster of worse-performing stations in Northern Norway, central and North-East Spain, and the Balcans.\n",
    "\n",
    "It might be interesting to remove, or look deeper at these reporting points, so they don't affect the overall outcome of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b206c0a-c1e3-4cfd-8c6b-94ede97e0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find stations with poor performance\n",
    "# BSS_stn = (1 - se.mean(['datetime', 'model', 'leadtime']) / se.mean())\n",
    "# plt.scatter(range(BSS_stn.shape[0]), BSS_stn, s=5, alpha=.33)\n",
    "# bad_stations = BSS_stn.loc[BSS_stn < -10].id.data\n",
    "# stations.loc[bad_stations].sort_values('catchment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa55160-c6a2-456f-950d-b3af15a8dac2",
   "metadata": {},
   "source": [
    "### 3.2 Where events predicted at any point in time?\n",
    "\n",
    "The objective of this section is to check if observed events where predicted at all, no matter leadtime.\n",
    "\n",
    "To convert exceedance probability for each meteo forcing into predicted events we need to come up with a total exceedance probability, i.e., combine the probabilities of each meteo forcing into a single probability value. This total probability will later be compared against a probability threshold to discern events. I will test four different approaches:\n",
    "\n",
    "* `current`:  the current notification criteria. At least a probabilistic and deterministic model must exceed the probability threshold.\n",
    "* `model_mean`: a simple mean over the 4 forcings.\n",
    "* `member_weighted`: a mean weighted by the number of members that each meteo forcing contains. In this approach the probabilistic models, specifically that from ECMWF, prevail.\n",
    "* `brier_weighted`: a mean weighted by the previously calculated Brier score. This is an approach in between the simple mean and the mean weighted by the number of members. Probabilistic forcings will prevail because they proved to be more skillful, but their relative importance is lower than in the previous approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28940917-a845-47be-a0a2-b3d288fe7842",
   "metadata": {},
   "source": [
    "#### 3.2.1 Exceedance over probability threshold\n",
    "\n",
    "In this section I will first compute the total probability using the four approaches previously explained, and then compare the results against the probability thresholds. The outcome will be a boolean dataset of exceedances thresholds named `pred_exc` with 5 dimensions (approach, id, leadtime, datetime, probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7338ad6-2688-4435-8edd-f4fbdcd1c133",
   "metadata": {},
   "source": [
    "##### Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cceb3ed-72c3-4acf-9811-c2ae2cf2c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceedance according to current criteria\n",
    "deterministic = (pred.sel(model=['EUD', 'DWD']) >= probabilities).any('model')\n",
    "probabilistic = (pred.sel(model=['EUE', 'COS']) >= probabilities).any('model')\n",
    "current = deterministic & probabilistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd0298-c39a-492f-8302-3325bfb57c1a",
   "metadata": {},
   "source": [
    "##### Model mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01af47e-014b-4af3-be6f-d649e2b19a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceedance according to mean over models\n",
    "model_mean = pred.mean('model', skipna=True) >= probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80bc9ef-13e6-4636-84e5-7319e9f832d2",
   "metadata": {},
   "source": [
    "##### Member weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19b519-b182-42d2-b369-6173d1e93c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataArray with weights for each model and leadtime\n",
    "max_leadtime = max(model['leadtimes'] for key, model in models.items())\n",
    "leadtimes = pd.timedelta_range(timedelta(hours=6), periods=max_leadtime, freq='6h')\n",
    "weights = np.zeros((len(models), max_leadtime))\n",
    "for i, (key, model) in enumerate(models.items()):\n",
    "    leadtime = model['leadtimes']\n",
    "    member = model['members']\n",
    "    weights[i,:leadtime] = member\n",
    "weights = xr.DataArray(weights, dims=['model', 'leadtime'], coords={'model': list(models), 'leadtime': leadtimes})\n",
    "\n",
    "# exceedance according to the mean over models weighted by the number of members\n",
    "weights_member = weights.isel(leadtime=slice(None, None, 2))\n",
    "weights_member['leadtime'] = pred.leadtime\n",
    "weights_member /= weights_member.sum('model')\n",
    "member_weighted = pred.weighted(weights_member).mean('model', skipna=True) >= probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce7c19-5dcf-469b-9661-b22e6e2d2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bae8a-17d5-435e-aba2-f658a8b4c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar weights based on the no. members\n",
    "weights_member.to_netcdf(f'{path_out}weights_member.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1dd49-3a9a-4cd3-be9f-0772407aea4f",
   "metadata": {},
   "source": [
    "##### Brier weighted\n",
    "\n",
    "To create a weight based on the Brier score I need to invert the values, since lower Brier score values represent better models. Therefore, I compute the inverse of the squared Brier score. To normalize weights (values between 0 and 1) I divide the previous weight matrix by its sum over models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565b503-50ef-43b2-95b7-4058c3e4833a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exceedance according to the mean over models weighted by the inverse Brier score\n",
    "brier = se.mean(['id', 'datetime'])\n",
    "weights_brier = 1 / brier**2\n",
    "weights_brier /= weights_brier.sum('model')\n",
    "brier_weighted = pred.weighted(weights_brier.fillna(0)).mean('model', skipna=True) >= probabilities\n",
    "# exportar weights based on the Brier score\n",
    "weights_brier.to_netcdf(f'{path_out}weights_brier.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986de516-b223-4737-9a02-aba3af078f5c",
   "metadata": {},
   "source": [
    "##### Combine approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcc9eb-b971-4806-9386-e3c4e9aa299a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge all total probability approaches in a single DataArray\n",
    "pred_exc = xr.Dataset({\n",
    "                        'current': current,\n",
    "                        'model_mean': model_mean,\n",
    "                        'member_weighted': member_weighted,\n",
    "                        'brier_weighted': brier_weighted,\n",
    "                        }).to_array(dim='approach')\n",
    "\n",
    "pred_exc.dims, pred_exc.shape\n",
    "\n",
    "# del pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184cba5-be1b-4e7b-8403-4355483f15a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# heatmap of weights\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(6, 3), constrained_layout=True, sharex=True, sharey=True)\n",
    "Weights = xr.Dataset({'no. member': weights_member, 'Brier score': weights_brier})\n",
    "for i, (ax, (var, da)) in enumerate(zip(axes, Weights.items())):\n",
    "    htm = plot_DataArray(da, vmin=0, vmax=1, ax=ax, ytick_step=1, xtick_step=1, title=f'weighted by {var}', cbar_kws={'shrink': .66})\n",
    "    if i == len(axes) - 1:\n",
    "        ax.set_xlabel('leadtime (h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25459929-bba5-4591-8608-b57c8c67a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exc.to_netcdf(f'{path_out}predicted_exceedanc_over_thresholds.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebc6c1-2470-4292-a97a-22c3eb0023c9",
   "metadata": {},
   "source": [
    "> ***Figure 5**. Weighting matrixes used to compute total probability according to the number of members of the meteorological forcing (top) or the skill measured in terms of Brier score (bottom).*\n",
    "\n",
    "The previous plot shows the weighting factors for the mean weighted by the number of members of each model (top), and the mean weighted by the performace of the model measured in terms of Brier score (bottom). When taking into account the number of members, obviously COSMO-LEPS and EUE get the vast majority of the weight, rendering no importance to any of the deterministic models. Between the two probabilistic models, EUE prevails over COSMO-LEPS, even though we've seen that their performance is comparable. However, when weighting according to performance, COSMO-LEPS and EUE get a very similar value during the leadtime span for which COSMO-LEPS is available. The probabilistic models have lower weights, but not as insignificat as when weighting by the number of members. In this sense, this last weighting method is a halfway point between the simple model mean and the mean weighted by the number of members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8430c1-5592-4e83-ada1-1ab5de7593a0",
   "metadata": {},
   "source": [
    "#### 3.2.2 Compute skill\n",
    "\n",
    "The computation of skill consists on three steps:\n",
    "\n",
    "1. Define events according to the notification criteria (persistence and minimum leadtime). The probability threshold was already taken into account when computing the exceedance over threshold in the previous section.\n",
    "\n",
    "2. Compute hits, misses and false alarms by comparing observation and predictions. At this step a window function is applied to count as hits predictions that have minor time lags compared with the observation.\n",
    "\n",
    "3. Compute skill metrics (recall, precicion and f1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69761486-4af0-45b9-bef1-9f3c113b8474",
   "metadata": {},
   "source": [
    "##### Define events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd3e4b-c267-4344-8259-42b9095ece38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted events \n",
    "pred_events = {label: compute_events(pred_exc, persistence=pers, resample='d', min_leadtime=min_leadtime) for label, pers in persistence.items()}\n",
    "pred_events = xr.Dataset(pred_events).to_array(dim='persistence').chunk({'id': 1})\n",
    "# convert leadtime in hours to leadtime in days\n",
    "pred_events['leadtime'] = np.ceil(pred_events.leadtime / 24).astype(int)\n",
    "\n",
    "# calculate number of predicted events\n",
    "n_events_pred = count_events(pred_events).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbef3c-6231-44ad-8e33-c66e1b1a06e9",
   "metadata": {},
   "source": [
    "##### Hits, misses and false alarms\n",
    "\n",
    "In the computation of hits, misses and false alarms I have applied a centered window function of width 5, which allows for a time lag between prediction and observations of 2 timestep (12 h) both before or after the observed events. \n",
    "\n",
    "I have also tried an asymmetrical (right-sided) window function of width 3 that allows only for 2-timestep lag (12 h) when the forecast predicts the event sooner than it actually happened. In the posterior skill analysis this approach proved slightly poorer (maximum f1-score of 0.40 instead of 0.51) than when applying the centered window.\n",
    "\n",
    "Personally, I think that applying a centered window is correct. Since one of the notification criteria states that notifications are only raised for leadtimes larger than 48 h, the fact that the prediction is 12 h delayed will still give, in the worst case, 36 h of notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67cc5c-24c8-4bb8-8875-1dea7f3da75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lt = 1\n",
    "hits_lt = compute_hits(obs, pred_events.sel(leadtime=lt), center=True, w=5).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b2dee-3377-44fe-b808-58348a1b6842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hits_lt.to_netcdf(f'{path_out}hits_leadtime{lt:>02}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c798a9-59cc-457f-b9ff-510391318a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_lt.to_netcdf'{path_out}hits_leadtime{lt:>02}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925b27a-6dfb-461a-acf3-ff86f8fab6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fafc7-f98f-4f35-9b59-e4697ef11e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the hit NetCDF files already exist and load them\n",
    "hit_files = [file for file in glob.glob(f'{path_out}*.nc') if file[-17:-15] in ['TP', 'FN', 'FP']]\n",
    "if len(hit_files) == 3:\n",
    "    print('Loading previously computed hit files')\n",
    "    hits_lt = xr.open_mfdataset(hit_files).compute()\n",
    "    hits_lt.close()\n",
    "else:\n",
    "    print('Computing hits')\n",
    "    # otherwise, compute hits, misses and false alarms\n",
    "    hits_lt = [compute_hits(obs, pred_events.sel(leadtime=lt), center=True, w=5) for lt in pred_events.leadtime.data]\n",
    "    hits_lt = xr.concat(hits_lt, dim='leadtime').compute()\n",
    "    # export results as NetCDF\n",
    "    for label, da in hits_lt.items():\n",
    "        file_out = f'{path_out}{label}_by_leadtime.nc'\n",
    "        print(f'Exporting file {file_out}')\n",
    "        da.to_netcdf(file_out)\n",
    "if hits_lt.leadtime.dtype == 'int64':\n",
    "    hits_lt['leadtime'] = np.ceil(hits_lt.leadtime / 24).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957ceae-b185-4796-a0a5-666d04425dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hits regardless of leadtime\n",
    "hits = xr.Dataset({'TP': hits['TP'].max('leadtime'),\n",
    "                    'FN': hits['FN'].min('leadtime'),\n",
    "                    'FP': hits['FP'].max('leadtime')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d715613-88bb-45e2-a0f6-ec01c693ab7f",
   "metadata": {},
   "source": [
    "##### Compute skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e02c6-dd38-41d4-8678-5bde636eb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute skill metrics\n",
    "skill_lt = xr.Dataset({'recall': hits_lt.TP.sum('id') / (hits_lt.TP.sum('id') + hits_lt.FN.sum('id')),\n",
    "                   'precision': hits_lt.TP.sum('id') / (hits_lt.TP.sum('id') + hits_lt.FP.sum('id')),\n",
    "                   'f1': 2 * hits_lt.TP.sum('id') / (2 * hits_lt.TP.sum('id') + hits_lt.FP.sum('id') + hits_lt.FN.sum('id'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ba2c6-27df-45bc-976a-6a7719772ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute skill regardless of leadtime\n",
    "skill = xr.Dataset({'recall': hits.TP.sum('id') / (hits.TP.sum('id') + hits.FN.sum('id')),\n",
    "                   'precision': hits.TP.sum('id') / (hits.TP.sum('id') + hits.FP.sum('id')),\n",
    "                   'f1': 2 * hits.TP.sum('id') / (2 * hits.TP.sum('id') + hits.FP.sum('id') + hits.FN.sum('id'))})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187ccde-ae4f-40f4-9133-0624919c4b05",
   "metadata": {},
   "source": [
    "#### 3.2.3 Analyse overall skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a88276-ef94-4c90-996d-008f92c425e2",
   "metadata": {},
   "source": [
    "##### Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742472fb-2f63-4af7-858b-c31120c6de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols, nrows = len(hits_lt), len(hits_lt.persistence)\n",
    "ig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows), sharex=True, sharey=True)\n",
    "\n",
    "for j, var in enumerate(['TP', 'FN', 'FP']):\n",
    "    \n",
    "    for i, pers in enumerate(hits_lt.persistence.data):\n",
    "        ax = axes[i,j]\n",
    "        # prob = hits_lt['TP'].sel(persistence=pers).sum('id').mean('approach').max('leadtime').idxmax('probability')\n",
    "        prob = skill['f1'].sel(persistence=pers).mean('approach').max('leadtime').idxmax('probability')\n",
    "        ax.plot(hits_lt.leadtime, hits_lt[var].sel(probability=prob, persistence=pers).sum('id').data, lw=.8, alpha=.66)\n",
    "        # if var == 'TP':\n",
    "        #     fp = n_events_pred.sel(probability=prob, persistence=pers).sum('id')\n",
    "        #     for k, app in enumerate(fp.approach):\n",
    "        #         ax.plot(n_events_pred.leadtime, fp.sel(approach=app).data, lw=.5, ls='--', alpha=.66, c=f'C{k}')\n",
    "        ax.hlines(n_events_obs.sum(), hits_lt.leadtime[0], hits_lt.leadtime[-1], lw=.8, ls='-', color='k', label='obs')\n",
    "        if i == 0:\n",
    "            ax.set_title(var)\n",
    "        elif i == nrows - 1:\n",
    "            ax.set_xlabel('leadtime (d)')\n",
    "            xmin, xmax = hits_lt.leadtime.min(), hits_lt.leadtime.max()\n",
    "            ax.set(xlim=(xmin, xmax), ylim=(0, None))\n",
    "            xticks = np.arange(xmin, xmax + 1)\n",
    "            ax.set_xticks(xticks)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'persistence: {pers}\\nprobability: {prob.data * 100:.0f}%')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c382dc-005e-4b51-b904-a881aeab2848",
   "metadata": {},
   "source": [
    "> ***Figure X**. Number of hits, misses and false alarms depending on leadtime. Rows represent different persistence criterion; for each persistence, only the results for the best-performing (in terms of f1) probability threshold are shown. Each solid line corresponds to one of the approached to compute total probability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a82646-8ca3-4dc2-97b1-5aab00a0be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = hits_lt.probability[::3]\n",
    "ncols, nrows = len(probs), len(hits_lt.persistence)\n",
    "ig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows), sharex=True, sharey=True)\n",
    "\n",
    "for i, pers in enumerate(hits_lt.persistence.data):\n",
    "    for j, prob in enumerate(probs.data):\n",
    "        ax = axes[i,j]\n",
    "        # true positives (FP)\n",
    "        tp = hits_lt['TP'].sel(probability=prob, persistence=pers).sum('id')\n",
    "        for k, app in enumerate(hits_lt.approach):\n",
    "            ax.plot(hits_lt.leadtime, tp.sel(approach=app).data, lw=.8, alpha=.66, c=f'C{k}', label=f'TP ({app.data})')\n",
    "        # total predicted events: TP + FP\n",
    "        tp_fp = n_events_pred.sel(probability=prob, persistence=pers).sum('id')\n",
    "        for k, app in enumerate(hits_lt.approach):\n",
    "            ax.plot(hits_lt.leadtime, tp_fp.sel(approach=app).data, lw=.5, ls='--', alpha=.66, c=f'C{k}', label=f'pred ({app.data})')\n",
    "        # total observed events: TP + FN\n",
    "        ax.hlines(n_events_obs.sum(), hits_lt.leadtime[0], hits_lt.leadtime[-1], lw=.8, ls='-', color='k', label='obs')\n",
    "        if i == 0:\n",
    "            ax.set_title(f'P >= {prob * 100:.0f}%')\n",
    "        elif i == nrows - 1:\n",
    "            ax.set_xlabel('leadtime (d)')\n",
    "            xmin, xmax = hits_lt.leadtime.min(), hits_lt.leadtime.max()\n",
    "            ax.set(xlim=(xmin, xmax), ylim=(0, None))\n",
    "            xticks = np.arange(xmin, xmax + 1)\n",
    "            ax.set_xticks(xticks)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'persistence: {pers}')\n",
    "        # f1 = skill['f1'].sel(persistence=pers, probability=prob).mean(['approach', 'leadtime'])\n",
    "        # lt = skill['f1'].sel(persistence=pers, probability=prob).mean('approach').idxmax('leadtime')\n",
    "        # text = f'f1 = {f1.data:.2f} ({lt.data:.0f}d)'\n",
    "        f1 = skill['f1'].sel(persistence=pers, probability=prob).mean('approach').max('leadtime')\n",
    "        text = f'f1 = {f1.data:.2f}'\n",
    "        ax.text(.975, .975, text, horizontalalignment='right', verticalalignment='top', transform=ax.transAxes)\n",
    "        \n",
    "# plt.legend(ax.get_legend_handles_labels(), ncol=3, loc=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5e0d4-f26c-418e-a900-24b153eb587e",
   "metadata": {},
   "source": [
    "> ***Figure X**. Hits, misses and false alarms depending on leadtime. Each column represents a pobability threshold and each row a persistence criterion. For each graph, the horizontal black line represents the total number of observed events (526). The colourful, solid lines are the hits for each of the approached used to compute total probability. The distance between the horizontal, black line and the colour, solid lines are the misses. The colourful, dashed lines are the total number of predicted events; the distance between these dashed lines and its respective solid line is the number of false alarms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767136cb-0ea3-457a-886e-c1ef71096622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hits, misses and false alarms\n",
    "lineplot_skill(hits.sum('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ab312-f61f-4b92-8b8c-ad50c69196d8",
   "metadata": {},
   "source": [
    "> ***Figure 6**. Counts of true positives (TP), false negatives (FN) and false positives (FP) depending on the probability threshold. Every row in the graph represents a different persistence criterion. The 4 lines in each plot represent the 4 methods used to combine the 4 meteorological forcings into total probability. The Y axis (counts) is in logarithmic scale because the different order of magnitude of FP compared with TP and FN.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070942b9-0b8d-438c-8234-cdb65c8452f9",
   "metadata": {},
   "source": [
    "Insights of the previous figure:\n",
    "\n",
    "**True positives (TP)**\n",
    "\n",
    "The number of true positives has a general negative trend with higher probability thresholds. As this threshold increases, we predict less events and, hence, we hit less observed events. This trend is shown for every persistence criterion and approach method. Therefore, the smaller the probability threshold, the higher the TP.\n",
    "\n",
    "The persistence criterion affects the number of true positives. Stricter criteria generate less predicted events and, hence, less TP.\n",
    "\n",
    "The spread between approach method is negiglible and constant throughout different persistences.\n",
    "\n",
    "The maximum value of TP, obtained by the most relaxed probability and persistence criteria, is around 400. This value is approximately 20 % lower than the number of observed events (526), which means that no matter the notification criteria, we can't predict all the events.\n",
    "\n",
    "**False negatives (FN)**\n",
    "\n",
    "The analysis of the false negatives is similar to that of true positives, but with an opposite trend regarding the probability criterion. Lower porbability thresholds predict a larger number of events, so fewer observed events will be missed. As we will see later, the effect is the opposite when looking at the false positives; however, the slope of the relation probability-FN is much lower than that of the relation probability-FP. In general, this relation is fairly linear, except for very low probability values.\n",
    "\n",
    "The persistence criterion has a minor effect on the false negatives, with larger values for stricter persistence values.\n",
    "\n",
    "**False positives (FP)**\n",
    "\n",
    "FP is the most sensitive variable, both regarding the probability and the persistence criteria. \n",
    "\n",
    "Regarding the probability threshold, the slope of the FP plots is much larger than FP or FN. As we raise the probability threshold, we are more certain that an event will happen when we forecast it, and the number of false positives decreases. The relation between the probability threshold and the number of false positives is fairly linear, except for the approach *member_weighted*. Therefore, the optimal probability threshold will be always very high (if not the maximum) to limit the number of false positives.\n",
    "\n",
    "Another approach to limit the number of false positives is to add the persistence criterion. When doing away with persistence (top-right plot), FP ranges from a few thousands to a few hundreds depending on the probability threshold. As we apply more restrictive persistence (going down the graph), the amount of FP is reduced drastically to a range between less than a thousand and a few dozens for the most restrictive persistence (bottom-right plot).\n",
    "\n",
    "FP is also the variable that shows a larger spread among approach methods, even though all the methods show a similar trend. The stricter the persistence criterion, the narrower the spread.\n",
    "<br>\n",
    "<br>\n",
    "> <font color='royalblue'>The most important idea to extract from the previous figure is that **the most sensitive variable is the number of false positives** and that the optimal notification criteria will be the one that minimizes this value without compromising excesively the true positives and false negatives.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfb589-d9f9-4716-acc9-1c22f10672bb",
   "metadata": {},
   "source": [
    "##### Skill: recall, precision and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a4d18-7c17-4844-9313-e9aca473cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols, nrows = len(skill), len(skill.persistence)\n",
    "ig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows), sharex=True, sharey=True)\n",
    "\n",
    "for j, var in enumerate(['recall', 'precision', 'f1']):\n",
    "    \n",
    "    for i, pers in enumerate(skill.persistence.data):\n",
    "        ax = axes[i,j]\n",
    "        prob = skill['f1'].sel(persistence=pers).mean('approach').max('leadtime').idxmax('probability')\n",
    "        ax.plot(skill.leadtime, skill[var].sel(probability=prob, persistence=pers).data, lw=.8, alpha=.66)\n",
    "        if i == 0:\n",
    "            ax.set_title(var)\n",
    "        elif i == nrows - 1:\n",
    "            ax.set_xlabel('leadtime (d)')\n",
    "            xmin, xmax = skill.leadtime.min(), skill.leadtime.max()\n",
    "            ax.set(xlim=(xmin, xmax), ylim=(0, 1))\n",
    "            xticks = np.arange(xmin, xmax + 1)\n",
    "            ax.set_xticks(xticks)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'persistence: {pers}\\nprobability: {prob.data * 100:.0f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52955b-b92c-410f-9413-e7efa4902abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = skill.probability[::3]\n",
    "ncols, nrows = len(probs), len(skill.persistence)\n",
    "ig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows), sharex=True, sharey=True)\n",
    "\n",
    "for i, pers in enumerate(skill.persistence.data):\n",
    "    for j, prob in enumerate(probs.data):\n",
    "        ax = axes[i,j]\n",
    "        # true positives (FP)\n",
    "        skill_pp = skill.sel(probability=prob, persistence=pers)\n",
    "        for ls, lw, metric in zip([':', '--', '-'], [.5, .5, .8], ['recall', 'precision', 'f1']):\n",
    "            # ax.plot(skill.leadtime, skill_pp['recall'], lw=.8, alpha=.66, label=f'TP ({app.data})')\n",
    "            # ax.plot(skill.leadtime, skill_pp['precision'], lw=.8, alpha=.66, label=f'TP ({app.data})')\n",
    "            for k, app in enumerate(skill.approach):\n",
    "                ax.plot(skill.leadtime, skill_pp[metric].sel(approach=app), lw=lw, alpha=.66, c=f'C{k}', ls=ls)#, label=f'TP ({app.data})')\n",
    "#         # total predicted events: TP + FP\n",
    "#         tp_fp = n_events_pred.sel(probability=prob, persistence=pers)\n",
    "#         for k, app in enumerate(skill.approach):\n",
    "#             ax.plot(skill.leadtime, tp_fp.sel(approach=app).data, lw=.5, ls='--', alpha=.66, c=f'C{k}', label=f'pred ({app.data})')\n",
    "#         # total observed events: TP + FN\n",
    "#         ax.hlines(n_events_obs.sum(), skill.leadtime[0], skill.leadtime[-1], lw=.8, ls='-', color='k', label='obs')\n",
    "        if i == 0:\n",
    "            ax.set_title(f'P >= {prob * 100:.0f}%')\n",
    "        elif i == nrows - 1:\n",
    "            ax.set_xlabel('leadtime (d)')\n",
    "            xmin, xmax = skill.leadtime.min(), skill.leadtime.max()\n",
    "            ax.set(xlim=(xmin, xmax), ylim=(0, 1))\n",
    "            xticks = np.arange(xmin, xmax + 1)\n",
    "            ax.set_xticks(xticks)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'persistence: {pers}')\n",
    "#         # f1 = skill['f1'].sel(persistence=pers, probability=prob).mean(['approach', 'leadtime'])\n",
    "#         # lt = skill['f1'].sel(persistence=pers, probability=prob).mean('approach').idxmax('leadtime')\n",
    "#         # text = f'f1 = {f1.data:.2f} ({lt.data:.0f}d)'\n",
    "#         f1 = skill['f1'].sel(persistence=pers, probability=prob).mean('approach').max('leadtime')\n",
    "#         text = f'f1 = {f1.data:.2f}'\n",
    "#         ax.text(.975, .975, text, horizontalalignment='right', verticalalignment='top', transform=ax.transAxes)\n",
    "        \n",
    "# plt.legend(ax.get_legend_handles_labels(), ncol=3, loc=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5aa28-44f0-4898-8e37-5666c0f2ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = skill.probability[::3]\n",
    "ncols, nrows = len(probs), len(skill.persistence)\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows), sharex=True, sharey=True)\n",
    "\n",
    "for i, pers in enumerate(skill.persistence.data):\n",
    "    for j, prob in enumerate(probs.data):\n",
    "        ax = axes[i,j]\n",
    "        skill_pp = skill.sel(probability=prob, persistence=pers)\n",
    "        for ls, lw, c, metric in zip([':', ':', '-'], [.5, .5, .8], ['steelblue', 'firebrick', 'k'], ['recall', 'precision', 'f1']):\n",
    "            ax.plot(skill.leadtime, skill_pp[metric], ls=ls, lw=lw, c=c, alpha=.8, label=metric)\n",
    "        if i == 0:\n",
    "            ax.set_title(f'P >= {prob * 100:.0f}%')\n",
    "        elif i == nrows - 1:\n",
    "            ax.set_xlabel('leadtime (d)')\n",
    "            xmin, xmax = skill.leadtime.min(), skill.leadtime.max()\n",
    "            ax.set(xlim=(xmin, xmax), ylim=(0, 1))\n",
    "            xticks = np.arange(xmin, xmax + 1)\n",
    "            ax.set_xticks(xticks)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'persistence: {pers}')\n",
    "    \n",
    "hndls, lbls = ax.get_legend_handles_labels()\n",
    "hndls = hndls[::len(skill.approach)]\n",
    "lbls = lbls[::len(skill.approach)]\n",
    "fig.legend(hndls, lbls, bbox_to_anchor=[0., -.05, .5, .1], ncol=len(skill));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517e242-ac42-4964-b35c-f8ae06926845",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = skill\n",
    "\n",
    "# plot skill metrics\n",
    "lineplot_skill(ds, rowdim='persistence', bestvar='f1', yscale='linear', round=1, )#save=f'{path_out}skill_eventwise_(all_stations).jpg')\n",
    "best_criteria = lineplot_skill.best_criteria\n",
    "# print('Best criteria:')\n",
    "# print('--------------\\n')\n",
    "\n",
    "# best_criteria = ds['f1'].argmax(list(ds.dims))\n",
    "\n",
    "# for dim in ds.dims:\n",
    "#     print('{0}:\\t{1}'.format(dim, ds.isel(best_criteria)[dim].data))\n",
    "\n",
    "# print()\n",
    "# for var in list(ds):\n",
    "#     print('{0:>10} = {1:.3f}'.format(var, ds[var].isel(best_criteria).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748ebe2-4dd9-4ec2-91fe-a8cac7a2eb91",
   "metadata": {},
   "source": [
    "> ***Figure 7**. EFAS skill predicting flood events in terms of $recall$, $precision$ and $f1$. Every plot shows the variability of the skill metric depending on the probability threshold for the 4 methods used to compute total probability. Every row represents a different persistence criterion, with stricter persistence as we go down the figure. Crosses represent the probability that optimizes $f1$ for that specific persistence and approach method; the same probability is used for the $recall$, $precision$ and $f1$ plots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748fef6-37f7-40a0-ae96-293d725e7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = skill_lt.sel(persistence=best_criteria['persistence'])\n",
    "\n",
    "# plot skill metrics\n",
    "lineplot_skill(ds, rowdim='leadtime', bestvar='f1', yscale='linear', round=1, )#save=f'{path_out}skill_eventwise_(all_stations).jpg')\n",
    "\n",
    "# print('Best criteria:')\n",
    "# print('--------------\\n')\n",
    "\n",
    "# best_criteria = ds['f1'].argmax(list(ds.dims))\n",
    "\n",
    "# for dim in ds.dims:\n",
    "#     print('{0}:\\t{1}'.format(dim, ds.isel(best_criteria)[dim].data))\n",
    "\n",
    "# print()\n",
    "# for var in list(ds):\n",
    "#     print('{0:>10} = {1:.3f}'.format(var, ds[var].isel(best_criteria).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c2d29-9287-4c56-8f93-8226074c9a9b",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "As expected, both $recall$ and $precision$ show monotic and opposite trends along the probability axes. $Recall$ decreases with higher probability thresholds, since the number of true positives is maximized and the number of false negatives minimized (see Figure X). This trend is mostly linear, but for the lowest probability thresholds, and similar over approach methods and persistence criteria. \n",
    "\n",
    "**Precision**\n",
    "\n",
    "On the contrary, $precision$ increases with higher probability, which means that the reduction of false positives prevails over a smaller reduction in false positives (see Figure X). The behabiour of presicion is more erratic both in terms of persistence and approach method, so we can't specify a general trend. The best example of this erratic behaviour is the plot for persistence 1 (no persistence), where every model behaves differently; some seem to have an asintotic behaviour (_current_, *member_weighted*), whereas the others are insentive for lower probabilities and then increase dramatically.\n",
    "\n",
    "**f1**\n",
    "\n",
    "Before getting into details, a reminder that $f1$ is the harmonic mean of $recall$ and $precision$, being a proper metric to combine the other two metrics in unbalanced problems such as this one.\n",
    "\n",
    "Luckily, $f1$ does not have a monotonic behaviour, which would cause that the optimal probability threshold would be either very low or very large. Instead, we can see two different behaviours depending on the persistence criterion. On the one hand, the more relax persistence criteria (1/1, 2/2 and 2/4) show an asymptotic behaviour, i.e., from a certain probability onwards, the f1 score is basically constant. On the other hand, the stricter persistence critera (3/3, 3/4 and 4/4) show a wide plateau over which f1 is constant, but the skill worsens for very large probability thresholds. Interestingly, the f1 score of this plateau/asymptote is the same (or almost the same) for every approach method; the difference between methods is the probability threshold at which the plateau is reached and at which f1 peaks. Therefore, the method used to compute the total probability is not determining the overall skill.\n",
    "\n",
    "Persistence affects the results in two ways. First, it defines the f1 score of the plateau/asymptote. The highest f1 score (0.46) is reached for persistence 2/4, but other persistence criteria have close results. However, very strict criteria (3/3, 3/4 and 4/4) reduce the skill to values closer to 0.40. Second, persistence affects the optimal probability threshold. There seems to be a trade-off between persistence and probability, in a sense that relaxed persistence requires higher probability thresholds, whereas strict persistence allows for smaller probability thresholds. The highest f1 score (0.46) is reached for a probability threshold of 50%.\n",
    "\n",
    "> <font color='royalblue'>From these results we can extract that **$f1$ can be optimised in terms of persistence and probability threshold**. The approach used to compute total probability, however, does not affect the maximum $f1$ score. There's a **trade-off between persistence and probability**, since together they must constrain the amount of false positives, hence, improve $precision$ and $f1$. **The highest performing combination of criteria is that with a probability threshold of 50%, a persistence of 2 out of 4 forecast, and for which total probability was computed using the Brier score as a weighing factor**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b371fb-3ae7-44f3-8f21-4fca4383fad6",
   "metadata": {},
   "source": [
    "#### 3.2.4 Anaylse skill by reporting points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8859c6f-19b1-44f5-bfb1-055e96bb9c05",
   "metadata": {},
   "source": [
    "##### Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e760c-a6c9-47f8-88ee-cbebe95daf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hits, misses and false alarm for each station based on the best criteria\n",
    "hits_stn = hits.isel(best_model)\n",
    "for var, da in hits_stn.items():\n",
    "    # convert xr.DataArray to pd.Series\n",
    "    stations[title] = da.to_pandas()\n",
    "\n",
    "map_hits(stations, cols=['TP', 'FN', 'FP'], mask=mask_stn,\n",
    "         save=f'{path_out}hits_eventwise_maps_reporting_points.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfec845-0646-4e38-913e-a636101fa0cc",
   "metadata": {},
   "source": [
    "> ***Figure 8**. Maps of hits, misses and false alarms for each reporting point with the best performing criteria. The colour scale changes depending on the variable with the rational that red (darker red) means worse values, whereas blue (darker blue) better values. In the case of hits (TP) and misses (FN) a mask has been applied to remove reporting points with no observed events (gray points), since none of these variables make sense if there are no observations to predict or miss.*\n",
    "\n",
    "The figure above shows the number of hits (TP), misses (FN) and false alarms (FP) by reporting point for the best-performing criteria. Red colours represent bad behaviour, whereas blue colours good behabiour.\n",
    "\n",
    "**Hits (TP)**\n",
    "\n",
    "The reporting points shown in this plot are only those with at least one observed event (the rest are \"masked\" and shown with gray points). The number of reporting points with at least one observed event is 322; from those, the model didn't predict any event in more than a half (178). If we look at the total amount of events, less than a half of the observed events were correctly predicted (213/526). There doesn't seem to be a clear geographic pattern neither for the bad nor the good performing points.\n",
    "\n",
    "**Misses (FN)**\n",
    "\n",
    "As in the previous case, this plot only shows points with observed events (322). Out of these reporting points, approximately a third had no misses. In total, more than half of the events were missed (313 out of 526).\n",
    "\n",
    "**False alarms (FP)**\n",
    "\n",
    "The best criteria would raise 188 false alarms in 125 reporting points. Some points would report as many as 7 false alarm, which may be an indicator of a specific problem in the hydrological simulation of those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f45de7-be37-4fad-9199-fa88ecbb5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station with higher amount of misses\n",
    "stations.loc[stations.FN > 4].sort_values('FN', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09838944-5a63-4f5a-a12b-06728e5a4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations with a higher amount of false alarms\n",
    "stations.loc[stations.FP > 2].sort_values('FP', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32f2f9-4a88-4dd4-b0ca-92a78d72ab75",
   "metadata": {},
   "source": [
    "##### Skill: recall, precision and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b80370-be87-4f77-91b2-478a6a53e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "stations['recall'] = stations.TP / (stations.TP + stations.FN)\n",
    "stations['precision'] = stations.TP / (stations.TP + stations.FP)\n",
    "stations['f1'] = 2 * stations.TP / (2 * stations.TP + stations.FN + stations.FP)\n",
    "\n",
    "map_skill(stations, cols=['recall', 'precision', 'f1'], bins=50, cmap=cmap_f1, norm=norm_f1,\n",
    "          save=f'{path_out}skill_eventwise_maps_reporting_points.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd04e56-8172-429a-a570-b3796d5d1757",
   "metadata": {},
   "source": [
    "> ***Figure 9**. EFAS skill (in terms of recall, precision and f1) disaggregated by reporting points. Gray dots represent reporting points for which the metric cannot be calculated (division 0 by 0); these points vary from one metric to another. The texts at the bottom show the number of zeros or ones in the metric out of the number of reporting points for which it was possible to compute the metric.*\n",
    "\n",
    "For the majority of reporting points the metrics can't be computed. The numberr of reported points is 900, whereas the metric with more values (f1) could only be computed for 388 stations. This means that when analysing the results by reporting points, almost two third of them are discarded.\n",
    "\n",
    "The histograms at the bottom show the distribution of the metric values. In all three metrics the results are polarised, with most of the points having either a metric value of 0 or 1.\n",
    "\n",
    "**Recall**\n",
    "\n",
    "Recall could be computed for the 322 stations that have observed events. Out of those, in more than a half (178) none of the observed events were detected (recall = 0). On the contrary, in a third of the stations all the events were detected (recall = 1). \n",
    "\n",
    "**Precision**\n",
    "\n",
    "Precistion could be computed in less reporting points than recall (234 out of 900). Approximately in half of the points (109) all the predicted events were correct (precision = 1), whereas in another half (90) none of the predicted events were right (precision = 0)\n",
    "\n",
    "**f1**\n",
    "\n",
    "F1 could be computed in more points than the previous two metrics (388 out of 900). However, it's the metric with worse results. Approximately two thirds (244 out of 388) of the points show now skill (f1 = 0), and only a fifth (81 out of 388) a perfect skill (f1 = 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1329d7-645f-4197-bbe7-711315b4aefd",
   "metadata": {},
   "source": [
    "##### Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d943be4-1778-4648-9e11-1bfcdd13559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_persistence = skill.sel(persistence='2/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ccd3b-7a90-49fb-96c5-6dbfaf5efddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = 'model_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b73f-d4f4-45be-9f02-0121d3b88894",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "for label in skill.approach.data:\n",
    "    ax.plot(skill.sel(persistence='2/4', approach=label).recall.data,\n",
    "            skill.sel(persistence='2/4', approach=label).precision.data,\n",
    "            label=label)\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1), xlabel='recall', ylabel='precision', title='precision-recall curve') \n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910556c8-c48e-4c9c-833c-95783e8cae59",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfadac-a82a-4b7a-9381-2c03d5a99b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e9324-8b04-41f7-9c65-7b12850a9a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
