{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2765e5b3-1233-4484-a0c6-2ccf2ee56dbb",
   "metadata": {},
   "source": [
    "# Select stations\n",
    "***\n",
    "\n",
    "__Author__: Chus Casado<br>\n",
    "__Date__:   13-01-2023<br>\n",
    "\n",
    "__Introduction__:<br>\n",
    "The objectives of this notebook are two:\n",
    "\n",
    "* To select a subset of study points in the catchment.\n",
    "* To count the number of flood events that occurred in the catchment during the period from 14-10-2020 and 31-12-2022. \n",
    "\n",
    "I consider a flood event each time that the discharge time series goes over the 5-year discharge return period, which is not the same as the number of timesteps at which discharge exceeds the threshod. For every exceeding period, we count one event, no matter if the event lasted 6 h or several days.\n",
    "\n",
    "To avoid that the correlation between river points affects the result of the EFAS notification skill assessment, I will try different procedures to select study points.\n",
    "\n",
    "To make the study more comprehensible and to make sure that the data of the reporting points is correct, the whole notebook focuses on the Ebro River catchment.\n",
    "\n",
    "__Tasks to do__:<br>\n",
    "* [x] Remove stations too close. \n",
    "    * [x] According to area increment between stations in the same river.\n",
    "    * [x] According to correlation between the discharge timeseries. From each pair of highly correlated stations one must be removed; the election of which one from the pair to remove is important!\n",
    "    * [x]\n",
    "    * [x] According to correlation only for highflows. I consider high flow when discharge exceeds the 1.5 return period\n",
    "    \n",
    "Possible solutions:\n",
    "* [x] Apply the correlation procedure event-wise, so that it doesn't eliminate the minor events\n",
    "* [x] Apply the correlation procedure to the subset of stations with some flood event, so that a station with no events does not remove a highly correlated station that in fact had events.\n",
    "* Change thresholds:\n",
    "    * [x] Increase the area increment threshold in order to remove more stations, hence, reduce correlation.\n",
    "    * [x] Increase the correlation threshold to be more restrictive and remove less stations.Possible solutions:\n",
    "* [x] Apply the correlation procedure event-wise, so that it doesn't eliminate the minor events\n",
    "* [x] Apply the correlation procedure to the subset of stations with some flood event, so that a station with no events does not remove a highly correlated station that in fact had events.\n",
    "* Change thresholds:\n",
    "    * [x] Increase the area increment threshold in order to remove more stations, hence, reduce correlation.\n",
    "    * [x] Increase the correlation threshold to be more restrictive and remove less stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cac69ff-7d9a-4d9d-80d7-65da1da0590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "os.chdir('../py/')\n",
    "from notifications import *\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36dfba-a39b-454d-867f-8acbb75c35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation threshold\n",
    "rho = .9\n",
    "\n",
    "# area threshold\n",
    "area = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4595919-0225-4190-b7c1-70e527908f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments = ['Liri', 'Evros', 'Strimonas', 'Drin', 'Kuban', 'Kymijoki', 'Dal', 'Gota', 'Arbogaan',\n",
    "              'Ljusnan', 'Ljungan', 'Indalsalven', 'Angermanalven', 'Umealven', 'Skelleftealven', \n",
    "              'Pitealven', 'Lueleaelven', 'Ranealven', 'Kalix', 'Torne', 'Ikjoki', 'Oulujoki',\n",
    "              'Kokem', 'Drammen', 'Lagen', 'Skien', 'Lagarfljot', 'Jkuls Fjllum', 'Skjalfandafljot',\n",
    "              'Heradsvotn', 'Hvita', 'Oelfusa', 'Kola', 'Paatsjoki', 'Tana', 'Altaelva', 'Vefsna',\n",
    "              'Namsen', 'Gaula', 'Driva', 'Rauma', 'Helge', 'Parnu', 'Lielupe', 'Scheldt', 'Mondego']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fdb07-e98b-4af2-9da6-f7daf5ce15a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LIRI\n",
      "----\n",
      "no. stations:\t2219\n",
      "no. stations in the Liri catchment:\t4\n",
      "Discharge timeseries:\n",
      "3231\ttimesteps\n",
      "4\tstations\n",
      "no. stations with at least one event:\t4\n",
      "total no. events:\t\t\t5\n"
     ]
    }
   ],
   "source": [
    "for catchment in catchments:\n",
    "    \n",
    "    print(f'\\n{catchment.upper()}')\n",
    "    print('-' * len(catchment))\n",
    "\n",
    "    out_folder = f'results/{catchment}/'\n",
    "    if os.path.exists(out_folder) is False:\n",
    "        os.makedirs(out_folder)\n",
    "\n",
    "\n",
    "    # load table of fixed reporing points\n",
    "    stations = pd.read_csv('../data/Station-2022-10-27v12.csv', index_col='station_id')\n",
    "    stations.index = stations.index.astype(str)\n",
    "    # filter stations and fields\n",
    "    mask = (stations['DrainingArea.km2.LDD'] >= area) & (stations.FixedRepPoint == True) & ((stations.EC_calib != 0) & (stations.EC_calib != np.nan))\n",
    "    stations = stations.loc[mask, ['StationName', 'LisfloodX', 'LisfloodY', 'DrainingArea.km2.LDD', 'Catchment', 'River', 'EC_Catchments', 'Country code']]\n",
    "    stations.columns = stations.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country']\n",
    "    stations[['strahler', 'pfafstetter']] = np.nan\n",
    "\n",
    "    # load shapefile with edited river and catchment names\n",
    "    points_edited = gpd.read_file('../data/GIS/fixed_report_points_500.shp')\n",
    "    points_edited.set_index('station_id', inplace=True, drop=True)\n",
    "    points_edited = points_edited[['StationNam', 'LisfloodX', 'LisfloodY', 'DrainingAr', 'Subcatchme',\n",
    "                                   'River', 'Catchment', 'Country co', 'strahler', 'pfafstette']]\n",
    "    points_edited.columns = stations.columns\n",
    "\n",
    "    # correct names of catchments and rivers\n",
    "    ids = list(set(stations.index).intersection(points_edited.index))\n",
    "    for id in ids:\n",
    "        for var in ['subcatchment', 'river', 'catchment']:\n",
    "            if points_edited.loc[id, var] != np.nan:\n",
    "                stations.loc[id, var] = points_edited.loc[id, var]\n",
    "\n",
    "    # add subcatchment and river order\n",
    "    stations.loc[ids, ['strahler', 'pfafstetter']] = points_edited.loc[ids, ['strahler', 'pfafstetter']]\n",
    "\n",
    "    # rename columns\n",
    "    #stations.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country', 'strahler', 'subcatchment_order']\n",
    "\n",
    "    print('no. stations:\\t{0}'.format(stations.shape[0]))\n",
    "\n",
    "    # extract stations in the catchment\n",
    "    stn_cat = stations.loc[stations.catchment == catchment].copy()\n",
    "    stn_cat.sort_values(['area'], ascending=True, inplace=True)\n",
    "    stn_cat.sort_values('pfafstetter', ascending=False, inplace=True)\n",
    "    print('no. stations in the {0} catchment:\\t{1}'.format(catchment, stn_cat.shape[0]))\n",
    "\n",
    "    # shapefile with rivers\n",
    "    try:\n",
    "        rivers_shp = gpd.read_file(f'../data/GIS/rivers_{catchment}.shp')\n",
    "    except:\n",
    "        rivers_shp = gpd.read_file(f'../data/GIS/RiversForWebPage_EU_ready.shp')\n",
    "        rivers_shp = rivers_shp.loc[rivers_shp.BASIN == catchment]\n",
    "\n",
    "    # load thresholds and extract 5-year discharge\n",
    "    path_reanalysis = f'../data/thresholds/'\n",
    "    thresholds = xr.open_dataset(f'{path_reanalysis}return_levels.nc')\n",
    "\n",
    "    # coordinate arrays necessary to extract the data\n",
    "    x = xr.DataArray(stn_cat.X, dims='id')\n",
    "    y = xr.DataArray(stn_cat.Y, dims='id')\n",
    "\n",
    "    # extract thresholds for each return period and station\n",
    "    variables = list(thresholds.keys())\n",
    "    for var in variables:\n",
    "        # extract X-year discharge for the stations\n",
    "        da = thresholds[var].sel(x=x, y=y, method='nearest')\n",
    "\n",
    "        # add threshold to the stations data frame\n",
    "        stn_cat[var] = da.data\n",
    "\n",
    "    # load discharge reanalysis data\n",
    "    files = glob.glob(f'../data/discharge/reanalysis/*202*.csv')\n",
    "    if 'dis' in locals():\n",
    "        del dis\n",
    "    for file in files:\n",
    "        temp = pd.read_csv(file, parse_dates=True, index_col=0)\n",
    "        if 'dis' in locals():\n",
    "            dis = pd.concat((dis, temp), axis=0)\n",
    "        else:\n",
    "            dis = temp.copy()\n",
    "        del temp\n",
    "\n",
    "    # cut the timeseries from 14-10-2020 12 pm and the selected reporting points\n",
    "    dis_cat = dis.loc['2020-10-14 12:00:00':, stn_cat.index]\n",
    "\n",
    "    print('Discharge timeseries:\\n{0}\\ttimesteps\\n{1}\\tstations'.format(*dis_cat.shape))\n",
    "\n",
    "    # return period\n",
    "    rp = 5\n",
    "\n",
    "    # IDENTIFY EVENTS\n",
    "    # ---------------\n",
    "\n",
    "    # identify events only with upper bound\n",
    "    events = identify_events(dis_cat, stn_cat[f'rl{rp}'])\n",
    "\n",
    "    # count number of events per station\n",
    "    col = f'n_events_{rp}'\n",
    "    stn_cat[col] = events.sum()\n",
    "    print('no. stations with at least one event:\\t{0}'.format((stn_cat[col] > 0).sum()))\n",
    "    print('total no. events:\\t\\t\\t{0}'.format(stn_cat[col].sum()))\n",
    "\n",
    "\n",
    "    if events.sum().sum() > 0:\n",
    "\n",
    "        # PLOT MAP\n",
    "        # # --------\n",
    "        # plt.figure()\n",
    "        # plot_events_map(stn_cat.X, stn_cat.Y, stn_cat[col], rivers=rivers_shp, size=20)\n",
    "\n",
    "        # PLOT TIMESERIES\n",
    "        # ---------------\n",
    "\n",
    "        # Select the stations in a catchment\n",
    "        mask = stn_cat[col] > 0\n",
    "        stns = stn_cat.loc[mask].index#.sort_values(['subcatchment_order', 'strahler']).index\n",
    "        # plot timeseries \n",
    "        for stn in stns:\n",
    "            title = '{0} - {1} ({2}) - {3:.0f} km2 ({4:.0f})'.format(stn, *stn_cat.loc[stn, ['river', 'subcatchment', 'area', 'pfafstetter']])\n",
    "            plot_events_timeseries(dis_cat[stn], events[stn], thresholds=stn_cat.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "                                   title=title, save=f'{out_folder}/no_events_{stn}.png')\n",
    "\n",
    "    # correlation matrix\n",
    "    corr = dis_cat.corr(method='spearman')\n",
    "    # keep only upper diagonal\n",
    "    corr = filter_correlation_matrix(corr, rho=None)\n",
    "\n",
    "    # compute exceedance of the correlation threshold\n",
    "    highly_correlated = corr > rho\n",
    "    highly_correlated = highly_correlated.astype(int)\n",
    "    highly_correlated[highly_correlated == 0] = np.nan\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    sns.heatmap(corr, vmin=0, vmax=1, cmap='Blues', cbar_kws={'label': 'Spearman correlation (-)', 'shrink': .66});\n",
    "    sns.heatmap(highly_correlated, cmap='Reds', vmin=0.5, vmax=1.5, alpha=.75, cbar=None);\n",
    "\n",
    "    # sort stations according to number of flood events\n",
    "    stn_cat_3 = stn_cat.copy()\n",
    "    stn_cat_3.sort_values([col], ascending=True, inplace=True)\n",
    "\n",
    "    # correlation matrix\n",
    "    corr_3 = dis_cat[stn_cat_3.index.to_list()].corr(method='spearman')\n",
    "    # remove highly correlated stations\n",
    "    corr_3 = filter_correlation_matrix(corr_3, rho=rho)\n",
    "    stn_cat_3 = stn_cat_3.loc[corr_3.index]\n",
    "\n",
    "    print('no. original points in the {0} catchment:\\t{1}'.format(catchment, stn_cat.shape[0]))\n",
    "    print('no. filtered points in the {0} catchment:\\t{1}'.format(catchment, stn_cat_3.shape[0]))\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(corr_3, vmin=0, vmax=1, cmap='Blues',\n",
    "                cbar_kws={'label': 'Spearman correlation (-)', 'shrink': .66});\n",
    "\n",
    "    stn_sets = {'all': stn_cat,\n",
    "                'correlation_event-sorted': stn_cat_3}\n",
    "\n",
    "    # sort all subsets equally\n",
    "    for key, stns in stn_sets.items():\n",
    "        stns.sort_values(['area'], ascending=True, inplace=True)\n",
    "        stns.sort_values('pfafstetter', ascending=False, inplace=True)\n",
    "\n",
    "    # table summarizing no. stations and events with every station set\n",
    "    summary = pd.DataFrame(index=stn_sets.keys(), columns=['no_stations', 'p_stations_event', 'no_events'])\n",
    "    for i, (key, stns) in enumerate(stn_sets.items()):\n",
    "        summary.loc[key] = stns.shape[0], sum(stns[col] > 0), stns[col].sum()\n",
    "    summary.p_stations_event /= summary.no_stations\n",
    "    print(summary)\n",
    "\n",
    "    if events.sum().sum() > 0:\n",
    "        # plot maps of no. events for each of the station sets\n",
    "        proj = ccrs.LambertAzimuthalEqualArea(central_longitude=10, central_latitude=52, false_easting=4321000, false_northing=3210000,\n",
    "                                                  globe=ccrs.Globe(ellipse='GRS80'))\n",
    "        nrows = 1\n",
    "        ncols = int(np.ceil(len(stn_sets) / nrows))\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 6 * nrows), \n",
    "                                 subplot_kw={'projection': proj}, constrained_layout=True)\n",
    "        vmax = max(stn_sets['all'][col].max(), 2)\n",
    "        for ax, (key, stns) in zip(axes.flatten(), stn_sets.items()):\n",
    "            plot_events_map(stns.X, stns.Y, stns[col], rivers=rivers_shp, size=20, ax=ax, title=key, vmax=vmax, cbar=True)\n",
    "        plt.colorbar(plot_events_map.colorbar, location='bottom', shrink=.4, label='no. events', ax=axes[:])\n",
    "        plt.savefig(f'{out_folder}/no_events_map_{key}.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # plot timeline of threshold exceedances\n",
    "    fig, axes = plt.subplots(nrows=2, figsize=(12, 8), constrained_layout=True, sharex=True, sharey=True)\n",
    "    thrs = ['rl2', 'rl5', 'rl20']\n",
    "    for ax, (key, stns) in zip(axes, stn_sets.items()):\n",
    "        exceedances_timeline(dis_cat, stns, thresholds=thrs, grid=True, title=key, ax=ax)\n",
    "    plt.savefig(f'{out_folder}/exceedance_timeline.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    n_stn_sub = pd.DataFrame(dtype=float)\n",
    "    for key, stns in stn_sets.items():\n",
    "        n_stn_sub = pd.concat((n_stn_sub, stns.subcatchment.value_counts()), axis=1)\n",
    "    n_stn_sub.columns = stn_sets.keys()\n",
    "\n",
    "    print(n_stn_sub)\n",
    "\n",
    "    # export selected points\n",
    "    stn_sets['correlation_event-sorted'].to_csv(f'results/{catchment}/points_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdd272-ae8f-45aa-b5da-c347f0f3ab62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
