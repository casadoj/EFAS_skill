{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654baf6d-f8eb-49dc-a90d-d8dd284d451d",
   "metadata": {},
   "source": [
    "# Summarize results\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 20-02-2024<br>\n",
    "\n",
    "**Introduction**:<br>\n",
    "This notebook creates a table comparing the optimised criteria and the associated skill for every `leadtime` and `model` defined in the configuration file. If several f-scores were tested, it will load the results of all f-scores and compare them too.\n",
    "\n",
    "The inputs are the points selected in [notebook 5](5_select_points.ipynb), the confusion matrices calculated in notebook [notebook_4](4_confusion_matrix.ipynb), and the notification criteria optimised in [notebook_6](6_skill.ipynb).\n",
    "\n",
    "The output of this notebook is a comparative table exported as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f417aff7-e56d-4497-8e8e-727ea08402f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# from datetime import datetime, timedelta\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from config import Config\n",
    "from compute import hits2skill, limit_leadtime, compute_skill\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a899f4-7f7b-4404-a63e-f1344fa1f331",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549d222b-d9bc-4feb-a3ee-ff214a3f9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('../conf')\n",
    "config = Config.load_from_yaml(config_path / 'config_COMB_leadtime_ranges.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e915052-7b61-41cc-981c-1b135c8b2357",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea338ed0-5ebe-4565-8f2c-e2398bc6c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "AREA_THRESHOLD = config.reporting_points['area']\n",
    "\n",
    "# reporting points\n",
    "PATH_STATIONS = config.reporting_points['output']\n",
    "FILE_STATIONS = f'reporting_points_selected_{AREA_THRESHOLD}km2.parquet'\n",
    "\n",
    "# catchments\n",
    "CATCHMENTS = config.reporting_points['catchments']\n",
    "if CATCHMENTS is None:\n",
    "    CATCHMENTS = [f for f in os.listdir(PATH_STATIONS) if (PATH_STATIONS / f).is_dir()]\n",
    "else:\n",
    "    if isinstance(CATCHMENTS, str):\n",
    "        CATCHMENTS = [CATCHMENTS]\n",
    "\n",
    "# minimum performance required from the reporting points\n",
    "MIN_KGE = config.reporting_points['KGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b30d3-d313-4db6-b0f7-f4aa5acbf8b4",
   "metadata": {},
   "source": [
    "### 1.2 Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce26e71-7110-466e-bcd2-b2343ecf72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of experiment: individual models (NWP) or combined (COMB)\n",
    "EXP = config.confusion_matrix['experiment']\n",
    "\n",
    "# return period\n",
    "RP = config.discharge['return_period']['threshold']\n",
    "\n",
    "# lead time ranges (it may be removed)\n",
    "LEADTIME = config.confusion_matrix['leadtime']\n",
    "\n",
    "# parameters of the rolling window used to compute hits\n",
    "WINDOW = config.confusion_matrix['window']\n",
    "\n",
    "# path that contains the NetCDFs with hit, misses and false alarms pro\n",
    "PATH_IN = config.confusion_matrix['output']\n",
    "if LEADTIME is None:\n",
    "    time_agg = 'all_leadtimes'\n",
    "elif len(LEADTIME) == 10:\n",
    "    time_agg = 'daily'\n",
    "elif len(LEADTIME) == 20:\n",
    "    time_agg = '12h'\n",
    "else:\n",
    "    time_agg = '_'.join([str(lt + 12) for lt in LEADTIME])\n",
    "PATH_IN = PATH_IN / f'{RP}/{EXP}/{time_agg}/window_{WINDOW}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65389aa3-8726-45ee-adec-5930ce6c8fb9",
   "metadata": {},
   "source": [
    "### 1.3 Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5783c025-4ff1-4352-8494-38397e83f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current operationa criteria\n",
    "CURRENT_CRITERIA = config.skill['current_criteria']\n",
    "\n",
    "# fixed notification criteria\n",
    "MIN_LEADTIME = config.skill['leadtime']\n",
    "MIN_AREA = config.skill['area']\n",
    "\n",
    "# path where results were saved\n",
    "PATH_OUT_ROOT = config.skill['output']\n",
    "if MIN_KGE is not None:\n",
    "    kge = f'kge_{MIN_KGE}'\n",
    "else:\n",
    "    kge = 'no_kge'\n",
    "PATH_OUT_ROOT = PATH_OUT_ROOT / f'{RP}/{EXP}/{time_agg}/window_{WINDOW}/{kge}'\n",
    "\n",
    "# coefficient of the fbeta-score\n",
    "betas = [float(f[1:]) for f in os.listdir(PATH_OUT_ROOT) if (PATH_OUT_ROOT / f).is_dir()]\n",
    "betas = [1 if beta == 1.0 else beta for beta in betas]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827e1d7-7c0c-43de-b83f-4409dba98094",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 Data\n",
    "\n",
    "### 2.1 Reporting points\n",
    "\n",
    "I load all the stations that where selected in a previous [notebook](3_0_select_stations.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7758ac43-6d08-465c-913f-f09d2adc33f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce5e8a016c2471a928c0944df0f5e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All points\n",
      "----------\n",
      "no. reporting points:\t\t1979\n",
      "no. stations with events:\t871\n",
      "no. observed events:\t\t1406\n",
      "\n",
      "Points selected for optimization\n",
      "-------------------------------\n",
      "no. reporting points:\t\t1239\n",
      "no. stations with events:\t489\n",
      "no. observed events:\t\t719\n"
     ]
    }
   ],
   "source": [
    "# load selected reporting points\n",
    "if 'stations' in locals():\n",
    "    del stations \n",
    "for catchment in tqdm_notebook(CATCHMENTS):\n",
    "    file = PATH_STATIONS / catchment / FILE_STATIONS\n",
    "    if file.is_file():\n",
    "        df = pd.read_parquet(file)\n",
    "    else:\n",
    "        continue\n",
    "    if 'stations' in locals():\n",
    "        stations = pd.concat((stations, df), axis=0)\n",
    "    else:\n",
    "        stations = df.copy()\n",
    "\n",
    "# mask stations with events\n",
    "col_events = f'obs_events_{RP}'\n",
    "stations_w_events = (stations[col_events] > 0)\n",
    "\n",
    "print('All points')\n",
    "print('----------')\n",
    "print(f'no. reporting points:\\t\\t{stations.shape[0]}')\n",
    "print('no. stations with events:\\t{0}'.format(stations_w_events.sum()))\n",
    "print('no. observed events:\\t\\t{0}'.format(stations[col_events].sum()))\n",
    "\n",
    "# select stations according to catchment area\n",
    "if MIN_AREA > AREA_THRESHOLD:\n",
    "    stations_optimize = stations.loc[stations.area >= MIN_AREA].index\n",
    "else:\n",
    "    stations_optimize = stations.index\n",
    "\n",
    "print('\\nPoints selected for optimization')\n",
    "print('-------------------------------')\n",
    "print(f'no. reporting points:\\t\\t{len(stations_optimize)}')\n",
    "print('no. stations with events:\\t{0}'.format((stations.loc[stations_optimize, col_events] > 0).sum()))\n",
    "print('no. observed events:\\t\\t{0}'.format(stations.loc[stations_optimize, col_events].sum()))\n",
    "\n",
    "# suffix that will be used when saving plots\n",
    "suffix = f'{MIN_AREA}km2_{len(stations_optimize)}points'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c959c12-b037-40a9-aa8e-62cb4e4e2803",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20be9f29-bce6-4f46-9d1f-b873eb8d79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hits for each station\n",
    "hits_stn = xr.open_mfdataset(f'{PATH_IN}/*.nc', combine='nested', concat_dim='id')\n",
    "\n",
    "# extract selected stations\n",
    "stations = stations.loc[set(stations.index).intersection(hits_stn.id.data)]\n",
    "hits_stn = hits_stn.sel(id=stations.index.to_list()).compute()\n",
    "\n",
    "# convert to NaN lead times that can't be reached due to model limitations or persistence\n",
    "hits_stn = limit_leadtime(hits_stn, exp=EXP)\n",
    "\n",
    "# subset of the 'hits' dataset with the stations selected for the optimization\n",
    "stations_optimize = list(set(stations_optimize).intersection(hits_stn.id.data))\n",
    "hits_opt = hits_stn.sel(id=stations_optimize).sum('id', skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7799fd9-4cb1-4580-b7ff-092d314d1e05",
   "metadata": {},
   "source": [
    "### 2.3 Optimised criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff220536-3de4-498a-ae31-14fbb61495f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criteria = {}\n",
    "for beta in betas:\n",
    "    metric = f'f{beta}'\n",
    "    file = glob.glob(f'{PATH_OUT_ROOT}/{metric}/*{suffix}.pkl')[0]\n",
    "    opt_crit = pickle.load(open(file, 'rb'))\n",
    "    \n",
    "    # if criteria was fitted for a single lead time value\n",
    "    if LEADTIME is not None:\n",
    "        for lt, crit in opt_crit.items():\n",
    "            if metric in crit:\n",
    "                del crit[metric]\n",
    "            criteria[f'{metric}_{lt}'] = crit\n",
    "            if EXP == 'COMB':\n",
    "                criteria[f'{metric}_{lt}']['current'] = CURRENT_CRITERIA\n",
    "    \n",
    "    # if criteria was fitted for several lead time ranges\n",
    "    else:\n",
    "        for lt, crit1 in opt_crit.items():\n",
    "            if EXP == 'COMB':\n",
    "                current_criteria['leadtime'] = lt\n",
    "                criteria[f'current_{lt}'] =  current_criteria.copy()\n",
    "            for key2, crit2 in crit1.items():\n",
    "                if metric in crit2:\n",
    "                    del crit2[metric]\n",
    "                crit2['leadtime'] = lt\n",
    "                criteria[f'{metric}_{lt}_{key2}'] = crit2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af8555-835c-4d54-a967-b530054a5cb2",
   "metadata": {},
   "source": [
    "## 3 Analysis\n",
    "\n",
    "In this section I will compute the skill of the EFAS predictions in different ways. In all the following sections I will work with three metrics: $recall$, $precision$ and the $f_{beta}$ score. The three metrics are based in the contingency table of hits ($TP$ for true positives), false alarms ($FP$ for false positives) and misses ($FN$ for false negatives).\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$f_{beta} = \\frac{(1 + \\beta^2) \\cdot TP}{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcb592-bb09-4c56-a14f-32b82726e186",
   "metadata": {},
   "source": [
    "### 3.1 Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4295f5b5-4db4-4c8e-9ecc-1e3cc477e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform criteria into a DataFrame\n",
    "summary = pd.DataFrame(dtype=float)\n",
    "for i, crtr in criteria.items():\n",
    "    df = pd.DataFrame(crtr).T\n",
    "    df.index = [i] * df.shape[0]\n",
    "    summary = pd.concat((summary, df), axis=0)\n",
    "summary['window'] = WINDOW\n",
    "summary['KGE'] = MIN_KGE\n",
    "# summary['OF'] = [x.split('_')[0] if x.split('_')[0] != 'current' else '' for x in summary.index]\n",
    "summary[['OF', 'leadtime']] = [x.split('_') if x.split('_')[0] != 'current' else '' for x in summary.index]\n",
    "summary.leadtime = summary.leadtime.astype(int)\n",
    "if LEADTIME is None:\n",
    "    summary = summary[['model', 'window', 'KGE', 'OF', 'probability', 'persistence']]\n",
    "else:\n",
    "    summary = summary[['model', 'window', 'KGE', 'OF', 'leadtime', 'probability', 'persistence']]\n",
    "summary.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82097bd6-335d-44ad-8144-3c1949bb64a2",
   "metadata": {},
   "source": [
    "### 3.2 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11aebb93-32e2-4d15-8e5f-422abd900d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hits, misses and false alarms\n",
    "cols = ['TP', 'FN', 'FP']\n",
    "summary[cols] = np.nan\n",
    "for i in range(summary.shape[0]):\n",
    "    crtr = summary.loc[i, ['model', 'leadtime', 'probability', 'persistence']].to_dict()\n",
    "    summary.loc[i, cols] = hits_opt.sel(crtr).to_pandas().astype(int)\n",
    "summary['no_events'] = summary.TP + summary.FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fd2c6-7021-446e-9e68-ead2039dca61",
   "metadata": {},
   "source": [
    "### 3.3 Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "550ed004-ed0d-48b1-ba55-f935c08d7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute skill\n",
    "for beta in betas:\n",
    "    recall, precision, fscore = compute_skill(summary.TP,\n",
    "                                              summary.FN,\n",
    "                                              summary.FP,\n",
    "                                              beta=beta)\n",
    "    summary[['recall', 'precision', f'f{beta}']] = pd.concat((recall, precision, fscore), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b4f89-4aa2-46c5-b7fb-711e86aa449a",
   "metadata": {},
   "source": [
    "### 3.4 Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e214543d-544c-4531-8e40-eb2e21e0e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXP == 'COMB':\n",
    "    # simplify model names\n",
    "    summary.model = [''.join([x[0].upper() for x in model.split('_')]) for model in summary.model]\n",
    "    # identify current approach\n",
    "    mask1 = summary.model == ''.join([x[0].upper() for x in CURRENT_CRITERIA['model'].split('_')])\n",
    "    mask2 = summary.probability == CURRENT_CRITERIA['probability']\n",
    "    mask3 = summary.persistence == CURRENT_CRITERIA['persistence']\n",
    "    summary.loc[mask1 & mask2 & mask3, 'model'] = 'current'\n",
    "\n",
    "# reorganize columns\n",
    "if LEADTIME is None:\n",
    "    summary.sort_values(['model', 'OF'], inplace=True)\n",
    "else:\n",
    "    summary.sort_values(['leadtime', 'model', 'OF'], inplace=True)\n",
    "\n",
    "# export\n",
    "summary.to_csv(PATH_OUT_ROOT / 'optimal_skill_and_criteria_by_fscore.csv', float_format='%.3f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
