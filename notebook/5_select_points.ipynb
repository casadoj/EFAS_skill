{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2765e5b3-1233-4484-a0c6-2ccf2ee56dbb",
   "metadata": {},
   "source": [
    "# Select stations\n",
    "***\n",
    "\n",
    "__Author__: Chus Casado<br>\n",
    "__Date__:   16-02-2024<br>\n",
    "\n",
    "__Introduction__:<br>\n",
    "This notebook does a selection of reporting points based on the correlation between the reanalysis discharge time series. The selection is done on a catchment basis. From every pair of reporting points with a Spearman correlation coefficient larger than a given value, either the upstream or downstream one is kept depending on the value of the attribute `upstream` in the configuration file.\n",
    "\n",
    "As a result, the notebook generates a folder for each catchment with a series of plots (hydrograph with flood events, correlation matrix, maps of reporting points...), a CSV file with the original and selected number of reporting points and observed events, and a PARQUET file with the table of attributes of the selected reporting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cac69ff-7d9a-4d9d-80d7-65da1da0590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "path_root = os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm_notebook\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from config import Config\n",
    "from compute import identify_events\n",
    "from selection import filter_correlation_matrix\n",
    "from plot.timeseries import plot_events_timeseries, exceedances_timeline\n",
    "from plot.maps import create_cmap, map_events, map_stations\n",
    "from plot.results import plot_correlation_matrix\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82637d-50dc-4bb2-a379-d17ab1788ce3",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f3639f-3e9f-4d3f-a9c8-e3b3f84fe3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('../conf')\n",
    "config = Config.load_from_yaml(config_path / 'config_COMB_leadtime_ranges.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7f828-4c76-45f1-b991-0e9e3dcdb7da",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bf4072-4a34-47ac-aa99-9a11a79082b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = config.reporting_points['area']\n",
    "\n",
    "# reporting points\n",
    "path_stations = config.reporting_points['output']\n",
    "file_stations = path_stations / f'reporting_points_over_{area_threshold}km2.parquet'\n",
    "\n",
    "# catchments\n",
    "catchments = config.reporting_points['catchments']\n",
    "\n",
    "# correlation threshold\n",
    "rho = config.reporting_points['selection']['rho']\n",
    "ascending = config.reporting_points['selection']['upstream']\n",
    "\n",
    "# rivers\n",
    "rivers_shp = config.reporting_points['rivers']\n",
    "if rivers_shp is not None:\n",
    "    rivers = gpd.read_file(rivers_shp)\n",
    "    \n",
    "# minimum performance required from the reporting points\n",
    "min_kge = config.reporting_points['KGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168fac6-e60d-49c9-86b2-ec11c0ea32ca",
   "metadata": {},
   "source": [
    "### 1.2 Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0461404f-2ca0-4f52-a985-4cb8705b6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where I have saved the raw discharge data\n",
    "path_discharge = config.discharge['output']['reanalysis']\n",
    "\n",
    "# start and end of the study period\n",
    "start = config.discharge['study_period']['start']\n",
    "end = config.discharge['study_period']['end']\n",
    "\n",
    "# return period\n",
    "rp = config.discharge['return_period']['threshold']\n",
    "col_events = f'obs_events_{rp}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b1d77-99b1-44d8-b899-22eadcd9e37b",
   "metadata": {},
   "source": [
    "## 2 Data\n",
    "### 2.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3e5464-18d6-4055-b9db-bc903c628921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. reporting points:\t\t1979\n",
      "no. catchments:\t\t\t327\n"
     ]
    }
   ],
   "source": [
    "# load table of fixed reporting points\n",
    "stations = pd.read_parquet(file_stations)\n",
    "stations[['X', 'Y', 'area']] = stations[['X', 'Y', 'area']].astype(int)\n",
    "\n",
    "# remove points with a performance (KGE) lower than the established threshold\n",
    "if min_kge is not None:\n",
    "    mask_kge = ~(stations.KGE <= min_kge)\n",
    "    stations = stations.loc[mask_kge]\n",
    "    \n",
    "# select stations that belong to the selected catchments\n",
    "if catchments is None:\n",
    "    catchments = stations.catchment.unique().tolist()\n",
    "else:\n",
    "    if isinstance(catchments, str):\n",
    "        catchments = [catchments]\n",
    "stations = stations.loc[stations.catchment.isin(catchments),:]\n",
    "    \n",
    "print(f'no. reporting points:\\t\\t{stations.shape[0]}')\n",
    "print(f'no. catchments:\\t\\t\\t{len(catchments)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496b594-3131-44f8-84d9-210b916dd66c",
   "metadata": {},
   "source": [
    "## 3 Analysis: selection of reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237804a3-04f8-4566-aacf-56566e7d20c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202c8f29928c4a1ebee4bc4c82c810a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DANUBE\n",
      "------\n",
      "original no. reporting points:\t\t432\n",
      "discharge timeseries:\t\t\t3919 timesteps\t432 stations\n",
      "no. stations with at least one event:\t203\n",
      "total no. events:\t\t\t300\n",
      "selected no. reporting points:\t\t147\n",
      "\n",
      "         no_stations  p_stations_event no_events\n",
      "set                                             \n",
      "all              432             0.470       300\n",
      "selected         147             0.537       131\n",
      "\n",
      "ELBE\n",
      "----\n",
      "original no. reporting points:\t\t65\n",
      "discharge timeseries:\t\t\t3919 timesteps\t65 stations\n",
      "no. stations with at least one event:\t7\n",
      "total no. events:\t\t\t12\n",
      "selected no. reporting points:\t\t26\n",
      "\n",
      "         no_stations  p_stations_event no_events\n",
      "set                                             \n",
      "all               65             0.108        12\n",
      "selected          26             0.231        10\n",
      "\n",
      "ODER\n",
      "----\n",
      "original no. reporting points:\t\t85\n",
      "discharge timeseries:\t\t\t3919 timesteps\t85 stations\n",
      "no. stations with at least one event:\t28\n",
      "total no. events:\t\t\t29\n",
      "selected no. reporting points:\t\t35\n",
      "\n",
      "         no_stations  p_stations_event no_events\n",
      "set                                             \n",
      "all               85             0.329        29\n",
      "selected          35             0.400        15\n",
      "\n",
      "RHINE\n",
      "-----\n",
      "original no. reporting points:\t\t147\n",
      "discharge timeseries:\t\t\t3919 timesteps\t147 stations\n",
      "no. stations with at least one event:\t96\n",
      "total no. events:\t\t\t116\n"
     ]
    }
   ],
   "source": [
    "for catchment in tqdm_notebook(catchments):    \n",
    "\n",
    "    print(f'\\n{catchment.upper()}')\n",
    "    print('-' * len(catchment))\n",
    "\n",
    "    path_out = path_stations / f'{catchment}'\n",
    "    path_out.mkdir(parents=True, exist_ok=True)\n",
    "    file_out = path_out / f'reporting_points_over_{area_threshold}km2.parquet'\n",
    "    if file_out.exists():\n",
    "        continue\n",
    "        \n",
    "    # EXTRACT/IMPORT DATA\n",
    "    # ---------------------\n",
    "    \n",
    "    # extract stations in the catchment\n",
    "    stns_all = stations.loc[stations.catchment == catchment].copy()\n",
    "    stns_all.sort_values('area', ascending=ascending, inplace=True)\n",
    "    stns_all.sort_values('pfafstetter', ascending=ascending, inplace=True)\n",
    "    print('original no. reporting points:\\t\\t{1}'.format(catchment, stns_all.shape[0]))\n",
    "\n",
    "    # extract rives in the catchment, if the GeoDataFrame exists\n",
    "    if 'rivers' in globals():\n",
    "        rivers_ctch = rivers.loc[rivers.BASIN == catchment]\n",
    "    else:\n",
    "        rivers_ctch = None\n",
    "\n",
    "    # import timeseries of reanalysis discharge\n",
    "    dis = {stn: xr.open_dataarray(path_discharge / f'{stn:04}.nc').to_pandas() for stn in stns_all.index}\n",
    "    dis = pd.DataFrame(dis)\n",
    "    print('discharge timeseries:\\t\\t\\t{0} timesteps\\t{1} stations'.format(*dis.shape))\n",
    "\n",
    "    # IDENTIFY EVENTS\n",
    "    # ---------------\n",
    "\n",
    "    events = identify_events(dis, stns_all[f'rl{rp}'])\n",
    "    stns_all[col_events] = events.sum()\n",
    "    print('no. stations with at least one event:\\t{0}'.format((stns_all[col_events] > 0).sum()))\n",
    "    print('total no. events:\\t\\t\\t{0}'.format(stns_all[col_events].sum()))\n",
    "    \n",
    "    if stns_all[col_events].sum() > 0:\n",
    "\n",
    "        # plot timeseries\n",
    "        # ...............\n",
    "        mask = stns_all[col_events] > 0 # select the stations with flood events\n",
    "        for stn in stns_all.loc[mask].index:\n",
    "            title = '{0} - {1} ({2}) - {3:.0f} km2 ({4:.0f})'.format(stn, *stns_all.loc[stn, ['river', 'subcatchment', 'area', 'pfafstetter']])\n",
    "            plot_events_timeseries(dis[stn],\n",
    "                                   events[stn],\n",
    "                                   thresholds=stns_all.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "                                   title=title,\n",
    "                                   save=path_out / f'{stn:04}_observed_events.jpg')\n",
    "\n",
    "    # SELECT STATIONS\n",
    "    # ---------------\n",
    "\n",
    "    if stns_all.shape[0] > 1:\n",
    "        # correlation matrix\n",
    "        corr = dis.corr(method='spearman')\n",
    "        # keep only upper diagonal\n",
    "        corr = filter_correlation_matrix(corr,\n",
    "                                         rho=None)\n",
    "        # plot correlation matrix of all reporting points\n",
    "        plot_correlation_matrix(corr,\n",
    "                                rho,\n",
    "                                save=path_out / f'correlation_matrix_all_points.jpg')\n",
    "\n",
    "        # filter out highly correlated stations with fewer observed events\n",
    "        # ................................................................\n",
    "\n",
    "        # sort stations according to number of flood events\n",
    "        stns_sel = stns_all.copy()\n",
    "        stns_sel.sort_values(col_events, ascending=True, inplace=True)\n",
    "        # correlation matrix\n",
    "        corr_sel = dis[stns_sel.index.to_list()].corr(method='spearman')\n",
    "        # remove highly correlated stations\n",
    "        corr_sel = filter_correlation_matrix(corr_sel,\n",
    "                                             rho=rho)\n",
    "        stns_sel = stns_sel.loc[corr_sel.index]\n",
    "        # plot correlation matrix of selected reporting points\n",
    "        if corr_sel.shape[0] > 1:\n",
    "            plot_correlation_matrix(corr_sel,\n",
    "                                    rho,\n",
    "                                    save=path_out / f'correlation_matrix_selected_points.jpg')\n",
    "        print('selected no. reporting points:\\t\\t{0}'.format(stns_sel.shape[0]))\n",
    "\n",
    "        \n",
    "    # organize all sets of stations in a single dictionary\n",
    "    if stns_all.shape[0] > 1:    \n",
    "        stns_sets = {'all': stns_all,\n",
    "                    'selected': stns_sel}\n",
    "    else:\n",
    "        stns_sets = {'all': stns_all}\n",
    "        \n",
    "    # sort all subsets equally\n",
    "    for key, stns in stns_sets.items():\n",
    "        stns.sort_values('area', ascending=ascending, inplace=True)\n",
    "        stns.sort_values('pfafstetter', ascending=ascending, inplace=True)\n",
    "\n",
    "    # ANALYSE RESULTS\n",
    "    # ---------------\n",
    "    \n",
    "    # table summarizing no. stations and events with every station set\n",
    "    summary = pd.DataFrame(index=stns_sets.keys(),\n",
    "                           columns=['no_stations', 'p_stations_event', 'no_events'])\n",
    "    summary.index.name = 'set'\n",
    "    for i, (key, stns) in enumerate(stns_sets.items()):\n",
    "        summary.loc[key] = stns.shape[0], sum(stns[col_events] > 0), stns[col_events].sum()\n",
    "    summary.p_stations_event /= summary.no_stations\n",
    "    summary.p_stations_event = summary.p_stations_event.astype(float)\n",
    "    print()\n",
    "    print(summary.round(3))\n",
    "    summary.to_csv(f'{path_out}summary.csv', float_format='%.3f')\n",
    "\n",
    "    # plot maps of observed events\n",
    "    if stns_all[col_events].sum() > 0:\n",
    "        extent = [stns_all.lon.min() - 1,\n",
    "                  stns_all.lon.max() + 1,\n",
    "                  stns_all.lat.min() - 1,\n",
    "                  stns_all.lat.max() + 1]\n",
    "        cmap, norm = create_cmap('Oranges',\n",
    "                                 np.arange(stns_all[col_events].max() + 2),\n",
    "                                 'no. events',\n",
    "                                 [0, (0.41176, 0.41176, 0.41176, 1)])\n",
    "        for label, stns in stns_sets.items():\n",
    "            map_events(stns.X,\n",
    "                       stns.Y,\n",
    "                       stns[col_events],\n",
    "                       rivers=rivers_ctch,\n",
    "                       s=20,\n",
    "                       alpha=1,\n",
    "                       cmap=cmap,\n",
    "                       norm=norm,\n",
    "                       title=f'{catchment} - {label}',\n",
    "                       extent=extent,\n",
    "                       save=path_out / f'map_observed_events_{label}_points.jpg')\n",
    "\n",
    "    # plot timeline of threshold exceedances\n",
    "    height_ratios = [df.shape[0] for label, df in stns_sets.items()]\n",
    "    figsize = (12, max(2, np.sum(height_ratios) / 20))\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=True)\n",
    "    gs = fig.add_gridspec(nrows=len(height_ratios), height_ratios=height_ratios)\n",
    "    for i, (key, stns) in enumerate(stns_sets.items()):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        exceedances_timeline(dis,\n",
    "                             stns,\n",
    "                             thresholds=['rl2', 'rl5', 'rl20'],\n",
    "                             grid=True,\n",
    "                             title=f'{key} reporting points',\n",
    "                             ax=ax)\n",
    "        if i != len(height_ratios) - 1:\n",
    "            ax.set_xticklabels([])\n",
    "    plt.savefig(f'{path_out}/exceedance_timeline.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # export selected points\n",
    "    stns_sel.to_parquet(path_out / f'reporting_points_selected_over_{area_threshold}km2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26de534-23d2-4d29-8220-796c8d7a743d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
