{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2765e5b3-1233-4484-a0c6-2ccf2ee56dbb",
   "metadata": {},
   "source": [
    "# Select stations\n",
    "***\n",
    "\n",
    "__Author__: Chus Casado<br>\n",
    "__Date__:   14-06-2023<br>\n",
    "\n",
    "__Introduction__:<br>\n",
    "This notebook does a selection of reporting points based on the correlation between the reanalysis discharge time series. The selection is done on a catchment basis. From every pair of reporting points with a Spearman correlation coefficient larger than a given value, either the upstream or downstream one is kept depending on the value of the attribute `upstream` in the configuration file.\n",
    "\n",
    "As a result, the notebook generates a folder for each catchment with a series of plots (hydrograph with flood events, correlation matrix, maps of reporting points...), a CSV file with the original and selected number of reporting points and observed events, and a PARQUET file with the table of attributes of the selected reporting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cac69ff-7d9a-4d9d-80d7-65da1da0590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import yaml\n",
    "\n",
    "os.chdir('../py/')\n",
    "from compute import identify_events\n",
    "from extract import filter_correlation_matrix\n",
    "from plot.timeseries import plot_events_timeseries, exceedances_timeline\n",
    "from plot.maps import create_cmap, map_events\n",
    "from plot.results import plot_correlation_matrix\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82637d-50dc-4bb2-a379-d17ab1788ce3",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f3639f-3e9f-4d3f-a9c8-e3b3f84fe3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7f828-4c76-45f1-b991-0e9e3dcdb7da",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bf4072-4a34-47ac-aa99-9a11a79082b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = cfg.get('reporting_points', {}).get('area', 500)\n",
    "\n",
    "# catchments to be analysed\n",
    "catchments = cfg.get('reporting_points', {}).get('catchments', {})\n",
    "if isinstance(catchments, str):\n",
    "    catchments = [catchments]\n",
    "    \n",
    "# correlation threshold\n",
    "rho = cfg.get('reporting_points', {}).get('selection', {}).get('rho', None)\n",
    "ascending = cfg.get('reporting_points', {}).get('selection', {}).get('upstream', True)\n",
    "\n",
    "# reporting points\n",
    "path_stations = cfg.get('reporting_points', {}).get('output', '../results/reporting_points/')\n",
    "file_stations = f'{path_stations}reporting_points_over_{area_threshold}km2.parquet'\n",
    "\n",
    "# rivers\n",
    "file_rivers = cfg.get('reporting_points', {}).get('input', {}).get('rivers', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168fac6-e60d-49c9-86b2-ec11c0ea32ca",
   "metadata": {},
   "source": [
    "### 1.2 Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461404f-2ca0-4f52-a985-4cb8705b6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where I have saved the raw discharge data\n",
    "path_discharge = cfg.get('discharge', {}).get('output', {}).get('reanalysis', f'../data/discharge/reanalysis/')\n",
    "\n",
    "# start and end of the study period\n",
    "start = cfg.get('discharge', {}).get('study_period', {}).get('start', None)\n",
    "if isinstance(start, str):\n",
    "    start = datetime.strptime(start, '%Y-%m-%d %H:%M')\n",
    "end = cfg.get('discharge', {}).get('study_period', {}).get('end', None)\n",
    "if isinstance(end, str):\n",
    "    end = datetime.strptime(end, '%Y-%m-%d %H:%M')\n",
    "\n",
    "# return period\n",
    "rp = cfg.get('discharge', {}).get('return_period', {}).get('threshold', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b1d77-99b1-44d8-b899-22eadcd9e37b",
   "metadata": {},
   "source": [
    "## 2 Data\n",
    "### 2.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e5464-18d6-4055-b9db-bc903c628921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load table of fixed reporting points\n",
    "stations = pd.read_parquet(file_stations)\n",
    "\n",
    "# extract reporting points of the catchments of interest\n",
    "if catchments is not None:\n",
    "    stations = stations.loc[stations.catchment.isin(catchments)]\n",
    "else:\n",
    "    catchments = np.sort(stations.catchment.unique())\n",
    "\n",
    "print('no. stations:\\t{0}'.format(stations.shape[0]))\n",
    "\n",
    "# shapefile of rivers of Europe\n",
    "if file_rivers is not None:\n",
    "    rivers_shp = gpd.read_file(file_rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496b594-3131-44f8-84d9-210b916dd66c",
   "metadata": {},
   "source": [
    "### 3. Analysis: selection of reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419b9be-852e-4983-b9dd-adef44b67802",
   "metadata": {},
   "outputs": [],
   "source": [
    "for catchment in catchments:\n",
    "    \n",
    "    print(f'\\n{catchment.upper()}')\n",
    "    print('-' * len(catchment))\n",
    "\n",
    "    path_out = f'{path_stations}{catchment}/'\n",
    "    if os.path.exists(path_out) is False:\n",
    "        os.makedirs(path_out)\n",
    "        \n",
    "    file_out = f'{path_out}reporting_points_over_{area_threshold}km2.parquet'\n",
    "    if os.path.exists(file_out):\n",
    "        continue\n",
    "        \n",
    "    # EXTRACT/IMPORT DATA\n",
    "    # ---------------------\n",
    "    \n",
    "    # extract stations in the catchment\n",
    "    stns_all = stations.loc[stations.catchment == catchment].copy()\n",
    "    stns_all.sort_values('area', ascending=ascending, inplace=True)\n",
    "    stns_all.sort_values('pfafstetter', ascending=ascending, inplace=True)\n",
    "    print('original no. reporting points:\\t\\t{1}'.format(catchment, stns_all.shape[0]))\n",
    "\n",
    "    # extract rives in the catchment\n",
    "    if file_rivers is not None:\n",
    "        rivers = rivers_shp.loc[rivers_shp.BASIN == catchment]\n",
    "    else:\n",
    "        rivers = None\n",
    "\n",
    "    # import timeseries of reanalysis discharge\n",
    "    dis = {stn: xr.open_dataarray(f'{path_discharge}{stn:04}.nc').to_pandas() for stn in stns_all.index}\n",
    "    dis = pd.DataFrame(dis)\n",
    "    print('discharge timeseries:\\t\\t\\t{0} timesteps\\t{1} stations'.format(*dis.shape))\n",
    "\n",
    "    # IDENTIFY EVENTS\n",
    "    # ---------------\n",
    "\n",
    "    events = identify_events(dis, stns_all[f'rl{rp}'])\n",
    "    stns_all['n_events_obs'] = events.sum()\n",
    "    print('no. stations with at least one event:\\t{0}'.format((stns_all.n_events_obs > 0).sum()))\n",
    "    print('total no. events:\\t\\t\\t{0}'.format(stns_all.n_events_obs.sum()))\n",
    "    \n",
    "    if stns_all.n_events_obs.sum() > 0:\n",
    "\n",
    "        # plot timeseries\n",
    "        # ...............\n",
    "        mask = stns_all.n_events_obs > 0 # select the stations with flood events\n",
    "        for stn in stns_all.loc[mask].index:\n",
    "            title = '{0} - {1} ({2}) - {3:.0f} km2 ({4:.0f})'.format(stn, *stns_all.loc[stn, ['river', 'subcatchment', 'area', 'pfafstetter']])\n",
    "            plot_events_timeseries(dis[stn], events[stn], thresholds=stns_all.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "                                   title=title, save=f'{path_out}/{stn:04}_observed_events.png')\n",
    "\n",
    "    # SELECT STATIONS\n",
    "    # ---------------\n",
    "\n",
    "    if stns_all.shape[0] > 1:\n",
    "        # correlation matrix\n",
    "        corr = dis.corr(method='spearman')\n",
    "        # keep only upper diagonal\n",
    "        corr = filter_correlation_matrix(corr, rho=None)\n",
    "        # plot correlation matrix of all reporting points\n",
    "        plot_correlation_matrix(corr, rho, save=f'{path_out}correlation_matrix_all_points.jpg')\n",
    "\n",
    "        # filter out highly correlated stations with fewer observed events\n",
    "        # ................................................................\n",
    "\n",
    "        # sort stations according to number of flood events\n",
    "        stns_sel = stns_all.copy()\n",
    "        stns_sel.sort_values('n_events_obs', ascending=True, inplace=True)\n",
    "        # correlation matrix\n",
    "        corr_sel = dis[stns_sel.index.to_list()].corr(method='spearman')\n",
    "        # remove highly correlated stations\n",
    "        corr_sel = filter_correlation_matrix(corr_sel, rho=rho)\n",
    "        stns_sel = stns_sel.loc[corr_sel.index]\n",
    "        # plot correlation matrix of selected reporting points\n",
    "        if corr_sel.shape[0] > 1:\n",
    "            plot_correlation_matrix(corr_sel, rho, save=f'{path_out}correlation_matrix_selected_points.jpg')\n",
    "        print('selected no. reporting points:\\t\\t{0}'.format(stns_sel.shape[0]))\n",
    "\n",
    "        \n",
    "    # organize all sets of stations in a single dictionary\n",
    "    if stns_all.shape[0] > 1:    \n",
    "        stns_sets = {'all': stns_all,\n",
    "                    'selected': stns_sel}\n",
    "    else:\n",
    "        stns_sets = {'all': stns_all}\n",
    "        \n",
    "    # sort all subsets equally\n",
    "    for key, stns in stns_sets.items():\n",
    "        stns.sort_values('area', ascending=ascending, inplace=True)\n",
    "        stns.sort_values('pfafstetter', ascending=ascending, inplace=True)\n",
    "\n",
    "    # ANALYSE RESULTS\n",
    "    # ---------------\n",
    "    \n",
    "    # table summarizing no. stations and events with every station set\n",
    "    summary = pd.DataFrame(index=stns_sets.keys(), columns=['no_stations', 'p_stations_event', 'no_events'])\n",
    "    summary.index.name = 'set'\n",
    "    for i, (key, stns) in enumerate(stns_sets.items()):\n",
    "        summary.loc[key] = stns.shape[0], sum(stns.n_events_obs > 0), stns.n_events_obs.sum()\n",
    "    summary.p_stations_event /= summary.no_stations\n",
    "    summary.p_stations_event = summary.p_stations_event.astype(float)\n",
    "    print()\n",
    "    print(summary.round(3))\n",
    "    summary.to_csv(f'{path_out}summary.csv', float_format='%.3f')\n",
    "\n",
    "    # plot maps of observed events\n",
    "    if stns_all.n_events_obs.sum() > 0:\n",
    "        cmap, norm = create_cmap('Oranges', np.arange(stns_all.n_events_obs.max() + 2), 'no. events', [0, (0.41176, 0.41176, 0.41176, 1)])\n",
    "        for label, stns in stns_sets.items():\n",
    "            map_events(stns, 'n_events_obs', rivers=rivers, s=10, alpha=1, cmap=cmap, norm=norm, title=catchment,\n",
    "                       save=f'{path_out}map_observed_events_{label}_points.jpg')\n",
    "\n",
    "    # plot timeline of threshold exceedances\n",
    "    height_ratios = [df.shape[0] for label, df in stns_sets.items()]\n",
    "    figsize = (12, max(2, np.sum(height_ratios) / 20))\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=True)\n",
    "    gs = fig.add_gridspec(nrows=len(height_ratios), height_ratios=height_ratios)\n",
    "    for i, (key, stns) in enumerate(stns_sets.items()):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        exceedances_timeline(dis, stns, thresholds=['rl2', 'rl5', 'rl20'], grid=True, title=f'{key} reporting poitns', ax=ax)\n",
    "        if i != len(height_ratios) - 1:\n",
    "            ax.set_xticklabels([])\n",
    "    plt.savefig(f'{path_out}/exceedance_timeline.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # export selected points\n",
    "    stns_sel.to_parquet(f'{path_out}reporting_points_selected_over_{area_threshold}km2.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
