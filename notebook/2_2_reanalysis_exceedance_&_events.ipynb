{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2765e5b3-1233-4484-a0c6-2ccf2ee56dbb",
   "metadata": {},
   "source": [
    "# Reanalysis: exceedance and events\n",
    "***\n",
    "\n",
    "__Author__: Chus Casado<br>\n",
    "__Date__: 23-01-2023<br>\n",
    "\n",
    "__Introduction__:<br>\n",
    "The objetctive of this notebook is to count the number of flood events that occurred in the 2-year period from 14-10-2020 and 09-10-2022. I consider a flood event each time that the discharge time series of a reporting point goes over the 5-year discharge return period, which is not the same as the number of timesteps at which discharge exceeds the threshod. For every exceeding period, we count one event, no matter if the event lasted 6 h or several days.\n",
    "\n",
    "Critera used to select an event:\n",
    "\n",
    "* [x] A period of time in which discharge exceeds the 5-year return period.\n",
    "* [ ] That period of time must last for a fixed number of timesteps.\n",
    "* [x] Between two events, discharge must be at some point lower than the 2-year return period.\n",
    "\n",
    "__Tasks to do__:<br>\n",
    "* [ ] Remove stations too close. \n",
    "    * For that, it would be necessary that the river in which the station is located would be correct (I saw errors in the Danube, Ebro, Duero, Tajo).\n",
    "    * What if we use only calibration points? They avoid stations with less than 10% increment in catchment area.\n",
    "* [ ] Check observed data for the station 2996 Burguillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12ad6b4-05e2-45c1-b1fd-108774dca9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "os.chdir('../py/')\n",
    "from notifications import *\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35423c-bf40-4eb7-9fef-08fcddb0c0dd",
   "metadata": {},
   "source": [
    "## 1 Data\n",
    "\n",
    "Three types of data is used:\n",
    "* A table (CSV) of reporting points.\n",
    "* A NetCDF file with the discharge thresholds for the whole EFAS domain.\n",
    "* A set of CSV files with the reanalysis discharge timeseries extracted at the reporting points.Three types of data is used:\n",
    "\n",
    "### 1.1 Reporting points\n",
    "\n",
    "Load the table with all EFAS reporting points and filter those points for which discharge data will be extracted. At the moment, only reporting points fulfilling the folowing criteria are selected:\n",
    "* Catchment area larger than 'area_threshold' (500 km2).\n",
    "* The point is considered a fixed reporting point.\n",
    "* The point was used for the calibration of LISFLOOD. That means that the field 'EC_Calib' is different from 0 or NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2641f74-e398-41b5-9599-c635440eb5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. stations:\t2219\n"
     ]
    }
   ],
   "source": [
    "# area threshold\n",
    "area = 500\n",
    "\n",
    "# load table of fixed reporing points\n",
    "stations = pd.read_csv('../data/Station-2022-10-27v12.csv', index_col='station_id')\n",
    "stations.index = stations.index.astype(str)\n",
    "# filter stations and fields\n",
    "mask = (stations['DrainingArea.km2.LDD'] >= area) & (stations.FixedRepPoint == True) & ((stations.EC_calib != 0) & (stations.EC_calib != np.nan))\n",
    "stations = stations.loc[mask, ['StationName', 'LisfloodX', 'LisfloodY', 'DrainingArea.km2.LDD', 'Catchment', 'River', 'EC_Catchments', 'Country code']]\n",
    "stations.columns = stations.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country']\n",
    "stations[['strahler', 'pfafstetter']] = np.nan\n",
    "\n",
    "# load shapefile with edited river and catchment names\n",
    "points_edited = gpd.read_file('../data/GIS/fixed_report_points_500.shp')\n",
    "points_edited.set_index('station_id', inplace=True, drop=True)\n",
    "points_edited = points_edited[['StationNam', 'LisfloodX', 'LisfloodY', 'DrainingAr', 'Subcatchme',\n",
    "                               'River', 'Catchment', 'Country co', 'strahler', 'pfafstette']]\n",
    "points_edited.columns = stations.columns\n",
    "\n",
    "# correct names of catchments and rivers\n",
    "ids = list(set(stations.index).intersection(points_edited.index))\n",
    "for id in ids:\n",
    "    for var in ['subcatchment', 'river', 'catchment']:\n",
    "        if points_edited.loc[id, var] != np.nan:\n",
    "            stations.loc[id, var] = points_edited.loc[id, var]\n",
    "        \n",
    "# add subcatchment and river order\n",
    "stations.loc[ids, ['strahler', 'pfafstetter']] = points_edited.loc[ids, ['strahler', 'pfafstetter']]\n",
    "\n",
    "# rename columns\n",
    "#stations.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country', 'strahler', 'subcatchment_order']\n",
    "\n",
    "print('no. stations:\\t{0}'.format(stations.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3596560-5fc7-4f6b-99f7-50bf5d4b174b",
   "metadata": {},
   "source": [
    "#### Analysis of autocorrelation across stations\n",
    "\n",
    "In this section we will explore all reporting points river by river, and we will remove the points whose catchment area does not increase that of the preceding station by a threshold (10 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5464a0-8ba1-4336-bad8-35409a779f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter stations accordint to increase in catchment area\n",
    "# stations_sel = filter_points(stations_sel, threshold=.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811139e-9fe4-4206-b16c-6fffa7ed3711",
   "metadata": {},
   "source": [
    "Applying the threshold on the increase of catchment area we have removed more than 300 reporting points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e0285-a775-49a5-8417-aa6f21c71651",
   "metadata": {},
   "source": [
    "### 1.2 Discharge thresholds\n",
    "\n",
    "The discharge thresholds are the discharge values for return periods 1.5, 2, 5, 10, 20, 50, 100, 200 and 500 years. The data is supplied in a NetCDF file that contains all the river network in Europe. This NetCDF is loaded as an _xarray_ and the values corresponding to the selected reporting points is extracted and included in the reporting points dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41c37a0-4d55-4d61-a669-008a2181abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load thresholds and extract 5-year discharge\n",
    "path_reanalysis = f'../data/CDS/thresholds/'\n",
    "thresholds = xr.open_dataset(f'{path_reanalysis}return_levels.nc')\n",
    "\n",
    "# coordinate arrays necessary to extract the data\n",
    "x = xr.DataArray(stations.X, dims='id')\n",
    "y = xr.DataArray(stations.Y, dims='id')\n",
    "    \n",
    "# extract thresholds for each return period and station\n",
    "variables = list(thresholds.keys())\n",
    "for var in variables:\n",
    "    # extract X-year discharge for the stations\n",
    "    da = thresholds[var].sel(x=x, y=y, method='nearest')\n",
    "    \n",
    "    # add threshold to the stations data frame\n",
    "    stations[var] = da.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ec01e-4a66-4645-9b48-ceb090004335",
   "metadata": {},
   "source": [
    "### 1.3 Discharge reanalysis\n",
    "\n",
    "This data represents EFAS simulation results when forced with observed meteorological data. It will be regarded as our observed discharge, even though it is modelled.\n",
    "\n",
    "The original NetCDF files were previously preprocessed in another [notebook](2_1_reanalysis_preprocessing.ipynb). As a result we have for each year a CSV file with the discharge timeseries for the EFAS reporting points. Since there's only forecast discharge for EFAS v4.0 since the 14-10-2020 12 pm, this will be the starting point for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c78585e-d6e5-410f-ab0b-524ac1353a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge timeseries:\n",
      "3231\ttimesteps\n",
      "2219\tstations\n"
     ]
    }
   ],
   "source": [
    "# load discharge reanalysis data\n",
    "files = glob.glob(f'../data/CDS/reanalysis/*202*.csv')\n",
    "if 'dis' in locals():\n",
    "    del dis\n",
    "for file in files:\n",
    "    temp = pd.read_csv(file, parse_dates=True, index_col=0)\n",
    "    if 'dis' in locals():\n",
    "        dis = pd.concat((dis, temp), axis=0)\n",
    "    else:\n",
    "        dis = temp.copy()\n",
    "    del temp\n",
    "    \n",
    "# cut the timeseries from 14-10-2020 12 pm and the selected reporting points\n",
    "dis = dis.loc['2020-10-14 12:00:00':, stations.index]\n",
    "\n",
    "print('Discharge timeseries:\\n{0}\\ttimesteps\\n{1}\\tstations'.format(*dis.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a1731-9ff4-4b6c-bed9-94ad06c3ee94",
   "metadata": {},
   "source": [
    "### 1.4 Exceedance reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c03788e5-64f5-4271-b04b-96fff925687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return period\n",
    "rp = 5\n",
    "\n",
    "# compute exceedance\n",
    "exc = dis >= stations[f'rl{rp}']\n",
    "\n",
    "# export\n",
    "exc.to_parquet(f'../data/exceedance/reanalysis/exceedance_rl{rp}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f647cf-59a4-481c-8919-bbf4d4d14888",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 Events\n",
    "\n",
    "First, I will count the number of events that exceeded the 5-year return period, without taking into account if they belong to the same flood. Then, I will try different combinations of thresholds with the objective of skipping exceedances of the 5-year return period that correspond to a single flood event (for instance, discharge goes up and down around the threshold).\n",
    "\n",
    "I have created a function call 'identify_events' that includes both procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04b0a174-1650-464d-a6b3-53a3b087cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary where all the results will be saved\n",
    "events = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7e9b0-d56c-469b-96a5-5edb6096a51b",
   "metadata": {},
   "source": [
    "### 2.1 Single exceendance threshold\n",
    "#### 2.1.1 Exceeding Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8077c801-c225-4d70-ac7a-2af3632c8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return period\n",
    "rp = 5\n",
    "\n",
    "# IDENTIFY EVENTS\n",
    "# ---------------\n",
    "\n",
    "# identify events only with upper bound\n",
    "key = str(rp)\n",
    "events[key] = identify_events(dis, stations[f'rl{rp}'])\n",
    "events[key].to_parquet(f'../data/exceedance/reanalysis/events_rl{rp}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1b664-0055-413d-af54-eab322aec023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count number of events per station\n",
    "col = f'n_events_{rp}'\n",
    "stations_sel[col] = events[key].sum()\n",
    "print('no. stations with at least one event:\\t{0}'.format((stations_sel[col] > 0).sum()))\n",
    "print('total no. of events:\\t\\t\\t{0}'.format(stations_sel[col].sum()))\n",
    "\n",
    "# PLOT MAP\n",
    "# --------\n",
    "\n",
    "out_folder = f'results/no_events/{rp}/'\n",
    "if os.path.exists(out_folder) is False:\n",
    "    os.makedirs(outfolder)\n",
    "plot_events_map(stations_sel.X, stations_sel.Y, stations_sel[col], save=f'{out_folder}/no_events_map.png')\n",
    "\n",
    "# PLOT TIMESERIES\n",
    "# ---------------\n",
    "\n",
    "# Select the stations in a catchment\n",
    "catchment = 'Ebro'\n",
    "mask = (stations_sel.catchment == catchment) & (stations_sel[col] > 0)\n",
    "stns_catchment = stations_sel.loc[mask].sort_values(['river', 'area']).index\n",
    "\n",
    "# plot timeseries of the station with more events\n",
    "# stn = stations_sel[col].idxmax() # in total\n",
    "# stn = stn_catchment[col].idxmax() # in that catchment\n",
    "# plot_events_timeseries(dis[stn], events[key][stn], thresholds=stations_sel.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "#                        title='{0} - {1} ({2})'.format(stn, *stations_sel.loc[stn, ['StationName', 'Catchment']]),\n",
    "#                        save=None)\n",
    "\n",
    "# plot timeseries for all the stations in the catchment\n",
    "for stn in stns_catchment:\n",
    "    plot_events_timeseries(dis[stn], events[key][stn], thresholds=stations_sel.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "                           title='{0} - {1} ({2})'.format(stn, *stations_sel.loc[stn, ['name', 'catchment']]),\n",
    "                          )#save=f'{out_folder}/no_events_{stn.zfill(4)}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29cb3a-529f-4576-8f80-8b2958d65c08",
   "metadata": {},
   "source": [
    "### 2.2 Double threshold\n",
    "#### 2.2.1 Exceeding Q5 and below Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11beff8-4ff5-481f-bdb9-1c42700db9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return periods for the upper and lower bound\n",
    "ub = 5\n",
    "lb = 2\n",
    "\n",
    "# IDENTIFY EVENTS\n",
    "# ---------------\n",
    "\n",
    "# identify events only with upper bound\n",
    "key = f'{ub}_{lb}'\n",
    "events[key] = identify_events(dis, stations_sel[f'rl{ub}'], stations_sel[f'rl{lb}'])\n",
    "\n",
    "# count number of events per station\n",
    "col = f'n_events_{ub}_{lb}'\n",
    "stations_sel[col] = events[key].sum()\n",
    "print('no. stations with at least one event:\\t{0}\\t({1} with less events)'.format((stations_sel[col] > 0).sum(),\n",
    "                                                                      ((events['5'].sum() - events[key].sum()) > 0).sum()))\n",
    "print('total no. of events:\\t\\t\\t{0}\\t({1} less)'.format(stations_sel[col].sum(),\n",
    "                                                         events['5'].sum().sum() - events[key].sum().sum()))\n",
    "\n",
    "# PLOT MAP\n",
    "# --------\n",
    "\n",
    "out_folder = f'results/no_events/{ub}-{lb}/'\n",
    "if os.path.exists(out_folder) is False:\n",
    "    os.makedirs(outfolder)\n",
    "    \n",
    "plot_events_map(stations_sel.X, stations_sel.Y, stations_sel[col], save=f'{out_folder}/no_events_map.png')\n",
    "\n",
    "# PLOT TIMESERIES\n",
    "# ---------------\n",
    "\n",
    "# select the station with more difference in the number of events\n",
    "dif = events['5'].sum() - events[key].sum()\n",
    "dif = dif[dif > 0]\n",
    "stn = dif.idxmax()\n",
    "\n",
    "for stn in dif.index:\n",
    "    plot_events_timeseries(dis[stn], events[key][stn], events['5'][stn], thresholds=stations_sel.loc[stn, ['rl1.5', 'rl2', 'rl5', 'rl20']],\n",
    "                           title='{0} - {1} ({2})'.format(stn, *stations_sel.loc[stn, ['name', 'catchment']]),\n",
    "                          )#save=f'{out_folder}/no_events_{stn.zfill(4)}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac08dd-b840-4306-b529-690a5321108f",
   "metadata": {},
   "source": [
    "### 2.3 Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf3d99-74a1-4181-91a1-d34a3e4f5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment = 'Ebro'\n",
    "stn_catchment = stations_sel.loc[stations_sel.catchment == catchment].sort_values(['area', 'river'], ascending=False)\n",
    "\n",
    "print('no. stations:\\t{0}'.format(stn_catchment.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf2e77-ad44-4bf1-95c6-be98f12f4fe0",
   "metadata": {},
   "source": [
    "I delete stations in the catchment with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bfdf8-779e-42e6-8750-8d0c0182a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr = dis[stn_catchment.index].corr(method='spearman').abs()\n",
    "rho = .85\n",
    "\n",
    "# remove the upper diagonal\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(corr.shape[1]):\n",
    "        if j >= i:\n",
    "            corr.iloc[i,j] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b2315-ae65-4be2-8afe-0c8d82a93638",
   "metadata": {},
   "source": [
    "Let's see what rivers have highly correlated stations ($\\rho \\ge 0.85$). For each station with at least one correlation higher than the threshold, I will plot its ID, river and he name of the rivers to which its highly correlated stations belong. I want to check whether rivers far apart in the catchment or only close by rivers show correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa840f-a923-4d4d-8aec-d1008892bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stn in corr.index:\n",
    "    exc = corr.loc[stn] > rho\n",
    "    if exc.any():\n",
    "        stns_corr = stn_catchment.loc[exc]\n",
    "        rivers = stns_corr.river.unique()\n",
    "        if len(rivers) > 1:\n",
    "            print(stn, stn_catchment.loc[stn, 'river'], '\\t', rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79276da-e3a2-49f6-a9de-e3fcb28e5da9",
   "metadata": {},
   "source": [
    "There's a cluster of correlated stations in the Aragon river and its tributaries: Arga (Araquil), Esca and Irati. The Ega and Zadorra rivers, close to the the Aragon, belong also to this group of stations. More interestingly, there are stations in the Ebro river that belong to this cluster.\n",
    "\n",
    "There's a second cluster of correltaed stations in the rivers Noguera Pallaresa, Esera and Noguera Ribagorzana (all of them rivers in the Segre subcatchment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9c5eb-09e0-4ec3-907b-f72dc01607a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show correlation between stations in the Ebro river\n",
    "compare_discharge(dis, ['606', '2849'], stations_sel.rl5)\n",
    "compare_discharge(dis, ['606', '644'], stations_sel.rl5)\n",
    "compare_discharge(dis, ['606', '652'], stations_sel.rl5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e6609-b408-4974-9819-437b662e0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated stations\n",
    "rho = .85\n",
    "corr_ = corr.copy()\n",
    "for stn in corr.index[1:]:\n",
    "    if (corr_.loc[stn] >= rho).any():\n",
    "        corr_.drop(stn, axis=0, inplace=True)\n",
    "        corr_.drop(stn, axis=1, inplace=True)\n",
    "\n",
    "# plot correlation matrix\n",
    "sns.heatmap(corr_, xticklabels=corr_.index, yticklabels=corr_.columns, vmin=0, vmax=1, cmap='viridis');\n",
    "\n",
    "print('no. stations selected:\\t{0}'.format(corr_.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe6666-04bb-4266-9de6-0ed0cf39ed36",
   "metadata": {},
   "source": [
    "Once we have filtered out highly correlated stations, we can check how many flood events remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c165551-1f11-4f4f-9721-6b0bd789d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnts = events['5'].loc[:, corr_.index]\n",
    "print('no. total events:\\t{0}'.format(vnts.sum().sum()))\n",
    "\n",
    "mask_col = vnts.any(axis=0)\n",
    "mask_row = vnts.any(axis=1)\n",
    "vnts = vnts.loc[mask_row, mask_col]\n",
    "print('no. stations:\\t\\t{0}'.format(vnts.shape[1]))\n",
    "print('no. timesteps:\\t\\t{0}'.format(vnts.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c6788-3648-458d-910c-d69e7157477e",
   "metadata": {},
   "source": [
    "There is a total of 15 events that happened in 11 different stations. There's one event that ocurred at the same time in two stations, let's find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f827ef-f992-4013-b066-c393b48c2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = []\n",
    "for t in range(vnts.shape[0]):\n",
    "    stns = vnts.loc[:, vnts.iloc[t]].columns.tolist()\n",
    "    if len(stns) > 1:\n",
    "        ts.append(t)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c9e70-8646-4fc4-8f6f-c3d734009973",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 3\n",
    "\n",
    "stns = vnts.loc[:, vnts.iloc[t]].columns.tolist()\n",
    "compare_discharge(dis, stns, stations_sel.rl5)\n",
    "stations_sel.loc[stns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128e508-bda9-4cca-b3ef-2da4d6e20493",
   "metadata": {},
   "source": [
    "The stations belong to the rivers Jiloca and Huerva, two close rivers, but in different subcatchments. That's why they suffered a flood event at the same time, but their correlation is rather low.\n",
    "\n",
    "I wonder how many stations remain in each river after removing highly correlated stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33b188-af63-4a6e-81d2-b3848a1eac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations in the catchment selected according to correlation\n",
    "stns_corr = stations_sel.loc[corr_.index]\n",
    "\n",
    "# compare data for stations in the same river\n",
    "stns_by_river = stns_corr.river.value_counts()\n",
    "for river in stns_by_river.index:\n",
    "    if stns_by_river[river] > 1:\n",
    "        stns_river = stns_corr.loc[stns_corr.river == river].index\n",
    "        if len(stns_river) > 1:\n",
    "            compare_discharge(dis, stns_river, stations_sel.rl5, title=river, alpha=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11929319-81b4-4d91-9f39-6b78f23e6596",
   "metadata": {},
   "source": [
    "#### Overlapping among events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb01594-0020-423e-bc88-b4c2f1f931f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discharge series for the catchment\n",
    "dis_cat = dis[stn_catchment.index]\n",
    "# discharge series for the stations selected according to correlation\n",
    "dis_corr = dis[stns_corr.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a62b1-8a10-4a76-b9d6-b22fe605e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceedances_timeline(dis, stn_catchment, thresholds=['rl5', 'rl20'], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ceffe7-b1c9-489f-9ad8-387346925704",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceedances_timeline(dis, stns_corr, thresholds=['rl5', 'rl20'], yticks=True, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650da97-eb3e-4a48-89fc-2519884db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "stns = ['618', '613', '633', '642', '4548']\n",
    "compare_discharge(dis, stns, stations_sel.rl5)\n",
    "stn_catchment.loc[stns, ['area', 'catchment', 'river']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df1fe1-72c9-4ab2-9ff8-fb05adb7a0e1",
   "metadata": {},
   "source": [
    "This 5 stations belong to the the Jalon (Jalon, Piedra and Jiloca rivers) and the Huerva subcatchments, which are adjacent. All of them had a flood event in September 2021, which is obviously somehow correlated. The Spearman correlation coefficient is high in some cases (up to 0.76), but below the threshold that discards stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9466fa-b0d9-4665-9928-1c9380a21252",
   "metadata": {},
   "outputs": [],
   "source": [
    "stns = ['606', '652', '594', '615']\n",
    "compare_discharge(dis, stns, stations_sel.rl5)\n",
    "stn_catchment.loc[stns, ['area', 'catchment', 'river']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9286cf-017e-49de-aca9-7ba49d97b878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
