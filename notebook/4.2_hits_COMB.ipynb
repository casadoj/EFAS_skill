{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Hits, misses and false alarms\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 14-02-2024<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "This notebooks computes the hits (true positives, $TP$), misses (false negatives, $FN$) and false alarms (false positives, $FP$) for all the selected reporting points and the complete study period.\n",
    "\n",
    "The input data are the data sets of exceedance over threshold both for the reanalsysis (\"observed\") and the forecast.\n",
    "\n",
    "The confusion matrix (hits, misses, false alarms) is computed for all the possible combinations of three criteria: model combination method (named _approach_), persistence, and probability threshold. The values of persisntece and probability thresholds for which the confusion matrix will be evaluated are defined by the user.\n",
    "\n",
    "The result is a new set of NetCDF files (one for station) that contains matrixes of hits, misses and false alarms for every combination of the criteria.\n",
    "\n",
    "**Interesting links**<br>\n",
    "[Evaluation metrics for imbalanced classification](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/)<br>\n",
    "[Cross entropy for machine learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)<br>\n",
    "[Probability metrics for imbalanced classification](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/)<br>\n",
    "[ROC curves and precision-recall curves for imbalanced classification](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/)<br>\n",
    "[Instructions for sending EFAS flood notifications](https://efascom.smhi.se/confluence/display/EDC/Instructions+for+sending%2C+upgrading+and+deactivating+EFAS+Flood+Notifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from config import Config\n",
    "from compute import *\n",
    "from seasonal import disaggregate_by_season\n",
    "from convert import reshape_DataArray, dict2da\n",
    "from plot.results import plot_DataArray, plot_weights, plot_brier_skill\n",
    "from plot.maps import create_cmap, map_events\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3066430b-5c0e-43ed-973d-d340ddcbe7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default text font size\n",
    "plt.rc('font', size=15)\n",
    "# Set the axes title font size\n",
    "plt.rc('axes', titlesize=16)\n",
    "# Set the axes labels font size\n",
    "plt.rc('axes', labelsize=15)\n",
    "# Set the font size for x tick labels\n",
    "plt.rc('xtick', labelsize=13)\n",
    "# Set the font size for y tick labels\n",
    "plt.rc('ytick', labelsize=13)\n",
    "# Set the legend font size\n",
    "plt.rc('legend', fontsize=13)\n",
    "# Set the font size of the figure title\n",
    "plt.rc('figure', titlesize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e0fda-fb63-48b5-9cc3-679d702d934a",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abf99cb-11ee-422c-aba3-bc60ed6330d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('../conf')\n",
    "config = Config.load_from_yaml(config_path / 'config_COMB_all_leadtimes.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730eecb-e002-43fb-a253-e580dc3498a9",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b51867-7a46-457d-8652-78fa28827c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = config.reporting_points['area']\n",
    "\n",
    "# fixed notification criteria for the optimization process\n",
    "area_optimization = config.skill['area']\n",
    "\n",
    "# reporting points\n",
    "path_stations = config.reporting_points['output']\n",
    "file_stations = path_stations / f'reporting_points_over_{area_threshold}km2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0aa2c-d205-4715-9a35-b16e9d3bba58",
   "metadata": {},
   "source": [
    "### 1.2 Exceedance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce3024d-d294-426d-81ed-79bc59a7635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return period\n",
    "rp = config.discharge['return_period']['threshold']\n",
    "\n",
    "# path where results from the preprocessing steps were saved\n",
    "path_reanalysis = config.exceedance['output']['reanalysis'] / f'{rp}'\n",
    "path_forecast = config.exceedance['output']['forecast'] / f'{rp}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d534e-e8e3-410f-8bc1-6ab40dc0c903",
   "metadata": {},
   "source": [
    "### 1.1 Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c088689c-1e5d-400e-b573-a43959664fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability thresholds\n",
    "probability_range = config.hits['criteria']['probability']\n",
    "probability = np.arange(*probability_range).round(3)\n",
    "probability = xr.DataArray(probability, dims=['probability'], coords={'probability': probability})\n",
    "\n",
    "# persistence\n",
    "persistence = config.hits['criteria']['persistence']\n",
    "persistence = {'/'.join([str(i) for i in pers]): pers for pers in persistence}\n",
    "\n",
    "# lead time ranges\n",
    "leadtime = config.hits['leadtime']\n",
    "\n",
    "# parameters of the rolling window used to compute hits\n",
    "window = config.hits['window']\n",
    "center = config.hits['center']\n",
    "\n",
    "# dissagregate the analysis by seasons?\n",
    "seasonality = config.hits['seasonality']\n",
    "\n",
    "# path where the dataset of hits, misses and false alarms will be saved\n",
    "path_out_root = config.hits['output']\n",
    "path_out_root = Path(path_out_root) / f'{rp}'\n",
    "path_weights = path_out_root / 'COMB' / 'weighing'\n",
    "if leadtime is None:\n",
    "    folder = 'all_leadtimes'\n",
    "elif len(leadtime) == 10:\n",
    "    folder = 'daily'\n",
    "elif len(leadtime) == 20:\n",
    "    folder = '12h'\n",
    "else:\n",
    "    folder = '_'.join([str(lt + 12) for lt in leadtime])\n",
    "path_out = path_out_root / 'COMB' / f'{folder}'\n",
    "if seasonality:\n",
    "    path_out += 'seasonal/'\n",
    "for path in [path_out_root, path_out, path_weights]:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1667-e5d2-45fb-83a1-647f9c9c4e7a",
   "metadata": {},
   "source": [
    "## 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b284a86-2d64-4907-a259-e3c67b6de2d5",
   "metadata": {},
   "source": [
    "### 2.1 Reporting points\n",
    "\n",
    "I load all the stations that where selected in a previous [notebook](3_0_select_stations.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaff83-b602-41a5-ba88-9fe9c03e0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporing points\n",
    "stations = pd.read_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92410ba-c400-4ee7-bfda-148de5b2bcfb",
   "metadata": {},
   "source": [
    "### 2.2 Exceedance reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07621a2-bea9-4502-ad61-d09ecb06acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the REANALYSIS data\n",
    "rean_exc = xr.open_mfdataset(f'{path_reanalysis}/*.nc', combine='nested', concat_dim='id')['exceedance']\n",
    "rean_exc = rean_exc.sel(id=stations.index).compute()\n",
    "\n",
    "if seasonality:\n",
    "    rean_exc = disaggregate_by_season(rean_exc)\n",
    "\n",
    "print(rean_exc.dims)\n",
    "print(rean_exc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693d76c-5439-4f44-952d-9ac44228a2bd",
   "metadata": {},
   "source": [
    "### 2.3 Exceedance forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb17994-bddc-44e1-b172-64d41b7180f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the FORECAST data\n",
    "fore_exc = xr.open_mfdataset(f'{path_forecast}/*.nc', combine='nested', concat_dim='id')\n",
    "fore_exc['id'] = fore_exc.id.astype(int)\n",
    "fore_exc = fore_exc.sel(id=stations.index)\n",
    "\n",
    "# reshape the DataArray of forecasted exceedance\n",
    "fore_exc = xr.Dataset({label: reshape_DataArray(da, trim=True) for label, da in fore_exc.items()})\n",
    "fore_exc = fore_exc.transpose('id', 'datetime', 'leadtime', 'model')\n",
    "\n",
    "# extract starting and ending dates\n",
    "if ('start' not in locals()) or ('end' not in locals()):\n",
    "    start = pd.to_datetime(max(rean_exc.datetime.min().data, fore_exc.datetime.min().data))\n",
    "    end = pd.to_datetime(min(rean_exc.datetime.max().data, fore_exc.datetime.max().data))\n",
    "else:\n",
    "    start = max(start, rean_exc.datetime.min(), fore_exc.datetime.min())\n",
    "    end = min(end, rean_exc.datetime.max(), fore_exc.datetime.max())\n",
    "\n",
    "# recalculate the exceedance datasets to convert the 3 classes (>Q5, >0.95¬∑Q5, <0.95¬∑Q5) to only 2 (exceedance, non-exceedance)\n",
    "rean_exc, fore_exc = recompute_exceedance(rean_exc.sel(datetime=slice(start, end)),\n",
    "                                          fore_exc['high'].sel(datetime=slice(start, end)),\n",
    "                                          fore_exc['low'].sel(datetime=slice(start, end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691e19e-e785-4a14-abda-46a23c8236ea",
   "metadata": {},
   "source": [
    "### 2.4 Weighting factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a4726-4c1a-4054-9534-3f3a642e48cc",
   "metadata": {},
   "source": [
    "**Brier weighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23f7eb-6c30-4aa5-a85f-6093d20d16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE BRIER SCORE\n",
    "brier = {}\n",
    "for model in fore_exc.model.data:\n",
    "    # extract model forecast\n",
    "    pred =  fore_exc.sel(model=model).dropna('leadtime', how='any').transpose()\n",
    "    # squared error\n",
    "    se = (pred - rean_exc)**2\n",
    "    # Brier score\n",
    "    brier[model] = se.mean(['id', 'datetime'])\n",
    "\n",
    "# convert dictionary to DatArray\n",
    "brier = dict2da(brier, dim='model')\n",
    "\n",
    "# CONVERT BRIER SCORE INTO WEIGHTING FACTORS\n",
    "# e = 1e-5\n",
    "# weights_brier = (brier.max() + e - brier) / (brier.max() + e - (brier.min() - e))\n",
    "w = 7\n",
    "weights_brier = brier**-w\n",
    "weights_brier /= weights_brier.sum('model')\n",
    "\n",
    "# export\n",
    "brier.to_netcdf(path_weights / 'brier_scores.nc')\n",
    "weights_brier.to_netcdf(path_weights / 'weights_brier.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f0d9e-af59-4dc8-b70b-5b757f7bc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# climatology\n",
    "Pclim = 0\n",
    "se= (Pclim - rean_exc)**2\n",
    "brier_clim = se.mean(['id', 'datetime'])\n",
    "\n",
    "# Brier skill score\n",
    "BSS = (brier_clim - brier) / brier_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e464cb8-a67e-4544-bf43-14211c546fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of 'skill_opt' to change acronyms\n",
    "ds = BSS.copy()\n",
    "ds['model'] = ds['model'].where(ds['model'] != 'EUD', 'HRES')\n",
    "ds['model'] = ds['model'].where(ds['model'] != 'EUE', 'ENS')\n",
    "ds = ds.reindex(model=['ENS', 'COS', 'HRES', 'DWD'])\n",
    "\n",
    "# plot the Brier skill score\n",
    "top = cm.get_cmap('Blues_r', 128)\n",
    "bottom = cm.get_cmap('Oranges', 128)\n",
    "newcolors = np.vstack((top(np.linspace(0.2, .95, 128)),\n",
    "                       bottom(np.linspace(.05, .8, 128))))\n",
    "BuOr = ListedColormap(newcolors, name='BlueOrange')\n",
    "\n",
    "df = plot_brier_skill(ds,\n",
    "                      cmap=BuOr,\n",
    "                      ylim=(-.12, 1.02),\n",
    "                      save=path_weights / 'Brier_skill_score.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729463a1-75ab-4340-95f6-8319dcc34963",
   "metadata": {},
   "source": [
    ">***Figure 1**. Brier skill score for every meteorological model and lead time. The benchmark is a model whose probability of exceedance is 0.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382de67e-fcf7-4844-955d-51527a23a601",
   "metadata": {},
   "source": [
    "**Member weighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3178a-2cd2-4f29-8e07-97d83da518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weights by number of members\n",
    "weights_member = weights_brier.copy()\n",
    "for model in models:\n",
    "    weights_member.loc[{'model': model}] = weights_member.sel(model=model).where(weights_brier.sel(model=model).isnull(), models[model]['members'])\n",
    "weights_member /= weights_member.sum('model')\n",
    "\n",
    "# export\n",
    "weights_member.to_netcdf(path_weights / 'weights_member.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb1628-11a0-4903-92ee-bcf98d90a481",
   "metadata": {},
   "source": [
    "**Model mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8953d-4182-48ad-a059-c6c8fcc45253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weights by number of members\n",
    "weights_mean = weights_brier.copy()\n",
    "weights_mean = weights_mean.where(weights_mean.isnull(), other=1)\n",
    "weights_mean /= weights_mean.sum('model')\n",
    "\n",
    "# export\n",
    "weights_mean.to_netcdf(path_weights / 'weights_mean.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41ad2a-0c1b-4510-ada0-050bedcb6951",
   "metadata": {},
   "source": [
    "**Compare weighing methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b363a4-366d-4ba3-b495-ccfbd7305799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of weights\n",
    "Weights = xr.Dataset({'model mean': weights_mean, 'member weighted': weights_member, 'Brier weighted': weights_brier})\n",
    "Weights['model'] = Weights['model'].where(Weights['model'] != 'EUD', 'HRES')\n",
    "Weights['model'] = Weights['model'].where(Weights['model'] != 'EUE', 'ENS')\n",
    "Weights = Weights.reindex(model=['ENS', 'COS', 'HRES', 'DWD'])\n",
    "\n",
    "plot_weights(Weights,\n",
    "             cmap=BuOr,\n",
    "             offset=-12,\n",
    "             save=path_weights / 'weights.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b15477-90bb-4bdb-8fd6-a4dced598db6",
   "metadata": {},
   "source": [
    ">***Figure 2**. Weights assigned to each NWP model and lead time in the three total probability approaches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ba156-e97c-46b4-bde1-1a35e3e49e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cca45-cccb-49d2-8eb7-75be3cce7f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Observed events\n",
    "#### 3.1.1 Number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f45f60-6a2f-4aa3-8e81-d193605a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the onset of the events\n",
    "onsets =  xr.concat((rean_exc.isel(datetime=0).astype(bool), rean_exc.diff('datetime')), dim='datetime') == 1\n",
    "onsets = onsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# find the offset of the events\n",
    "offsets = xr.concat((rean_exc.isel(datetime=0).astype(bool), rean_exc.diff('datetime')), dim='datetime') == -1\n",
    "offsets = offsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# count number of events\n",
    "col_events = f'obs_events_{rp}'\n",
    "stations[col_events] = onsets.sum('datetime').to_pandas()\n",
    "\n",
    "print('No. stations with observed events:\\t{0}'.format((stations[col_events] > 0).sum()))\n",
    "print('No. observed events:\\t\\t\\t{0}'.format(stations[col_events].sum()))\n",
    "\n",
    "if seasonality:\n",
    "    # compute number of events per season\n",
    "    rean_onsets4s = disaggregate_by_season(rean_onsets, dim='datetime')\n",
    "    cols = [f'obs_events_{rp}_{season}' for season in ['winter', 'spring', 'summer', 'autumn']]\n",
    "    stations[cols] = rean_onsets4s.sum('datetime').to_pandas().transpose()\n",
    "\n",
    "# export the stations table\n",
    "stations.to_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2843be4-55d8-477d-8474-b43c8b3c7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of the number of events\n",
    "map_events(stations.X,\n",
    "           stations.Y, \n",
    "           stations[col_events],\n",
    "           yscale='log', alpha=1,\n",
    "           save=path_out_root / f'map_observed_events_{stations.shape[0]}points.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd2f44-b46b-410f-a9dc-4cdf61e42df8",
   "metadata": {},
   "source": [
    "> ***Figure 3**. Number of observed flood events during the study period.*\n",
    "\n",
    "The geographical distribution of events is not even. There is a higher proportion of stations with events in Central Europe, British Isles and the Mediterranean catchments than in Estearn and North-Eastern Europe. During the study period there were major events in the Rhine, Meuse and Ebro, which can be seen in the map.\n",
    "\n",
    "The reporting points with more than 5 flood events during the study period were removed, since it is suspicious that the 5-year return period was exceeded so many times in only 2 years of study period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497da33-c341-4cb9-aec6-768820a8fe5f",
   "metadata": {},
   "source": [
    "#### 3.1.2 Duration of the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5ee4f-74fe-4fd3-a481-72dedca2c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the duration (in hours) of each event\n",
    "event_durations = {}\n",
    "for stn in tqdm(onsets.id.data):\n",
    "    if stations.loc[stn, col_events] == 0:\n",
    "        continue\n",
    "    # dates of the onsets and offsets of the events\n",
    "    dates_onsets = onsets.sel(id=stn).where(onsets.sel(id=stn), drop=True).datetime.data\n",
    "    dates_offsets = offsets.sel(id=stn).where(offsets.sel(id=stn), drop=True).datetime.data\n",
    "    # corrections in case the amount of onsets and onsets don't match\n",
    "    if len(dates_onsets) > len(dates_offsets):\n",
    "        dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    elif len(dates_onsets) < len(dates_offsets):\n",
    "        dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "    # correction in case the first onset is later that the first offset\n",
    "    else:\n",
    "        if dates_onsets[0] > dates_offsets[0]:\n",
    "            print(stn)\n",
    "            dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "            dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    # convert into hours\n",
    "    durations = (dates_offsets - dates_onsets) / np.timedelta64(1, 'h')\n",
    "    event_durations[stn] = durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00388b-076c-47a5-84f3-c2521ddde9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary of durations into a flattened array\n",
    "durations_all, durations_opt = [], []\n",
    "for stn, arr in event_durations.items():\n",
    "    for d in arr:\n",
    "        durations_all.append(d)\n",
    "        if stations.loc[stn, 'area'] >= area_optimization:\n",
    "            durations_opt.append(d)\n",
    "durations_all = np.array(durations_all)\n",
    "durations_opt = np.array(durations_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a9d88-c48e-4bd4-a668-6aba02bb2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of event duration\n",
    "alpha = .7\n",
    "xmin = 0\n",
    "xmax = 16 * 24\n",
    "bins = np.arange(xmin, xmax + 1, 6).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.histplot(durations_all, ax=ax, alpha=alpha**2, bins=bins, color='steelblue', edgecolor=None, label=f'A ‚â• {area_threshold} km¬≤')\n",
    "sns.histplot(durations_opt, ax=ax, alpha=alpha, bins=bins, color='orange', edgecolor=None, label=f'A ‚â• {area_optimization} km¬≤')\n",
    "ax.set(xlabel='duration (h)', ylabel='no. events', xlim=(xmin, xmax));\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(96))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(24))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(100))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(25))\n",
    "ax.legend(frameon=False);\n",
    "\n",
    "plt.savefig(path_out_root / f'duration_distribution_{stations.shape[0]}points.jpg',\n",
    "            dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d398b-f953-4d35-acca-72592f42a2b5",
   "metadata": {},
   "source": [
    ">***Figure 4**. Distributions of the duration of the observed flood events. In blue the events observed in points with a catchment area of at least 500 km¬≤, in orage those that exceed 2000 km¬≤.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ba493-3f90-4b28-9626-318c50892154",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa46a9-7c95-4632-bc72-c1f7ed6923f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_netcdf = path_out / f'window_{window}'\n",
    "path_netcdf.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for stn in tqdm(stations.index):\n",
    "\n",
    "    # check if the output file already exists\n",
    "    file_out = path_netcdf / f'{stn:>04}.nc'\n",
    "    if os.path.exists(file_out):\n",
    "        continue\n",
    "        \n",
    "    # FORECAST EXCEEDANCE PROBABILITY\n",
    "    forecast = fore_exc.sel(id=stn)\n",
    "\n",
    "    # TOTAL PROBABILITY OF EXCEEDANCE\n",
    "\n",
    "    # exceedance according to 1 deterministic + 1 probabilistic\n",
    "    deterministic = (forecast.sel(model=['EUD', 'DWD']) >= probability).any('model')\n",
    "    probabilistic = (forecast.sel(model=['EUE', 'COS']) >= probability).any('model')\n",
    "    deterministic_probabilistic = deterministic & probabilistic\n",
    "\n",
    "    # exceedance according to mean over models\n",
    "    model_mean = forecast.mean('model', skipna=True) >= probability\n",
    "\n",
    "    # exceedance according to the mean over models weighted by the number of members\n",
    "    member_weighted = forecast.weighted(weights_member.fillna(0)).mean('model', skipna=True) >= probability\n",
    "\n",
    "    # exceedance according to the mean over models weighted by the inverse Brier score\n",
    "    brier_weighted = forecast.weighted(weights_brier.fillna(0)).mean('model', skipna=True) >= probability\n",
    "\n",
    "    # merge all total probability approaches in a single DataArray\n",
    "    total_exc = xr.Dataset({\n",
    "                            '1_deterministic_+_1_probabilistic': deterministic_probabilistic,\n",
    "                            'model_mean': model_mean,\n",
    "                            'member_weighted': member_weighted,\n",
    "                            'brier_weighted': brier_weighted,\n",
    "                            }).to_array(dim='approach')\n",
    "\n",
    "    del forecast\n",
    "\n",
    "    # HITS, MISSES, FALSE ALARMS\n",
    "    \n",
    "    hits = {}\n",
    "    for label, pers in persistence.items():\n",
    "\n",
    "        # compute predicted events\n",
    "        pred = exceedance2events(total_exc, persistence=pers, leadtime=leadtime)\n",
    "\n",
    "        # disaggregate seasonaly\n",
    "        if seasonality:\n",
    "            pred = disaggregate_by_season(pred)\n",
    "\n",
    "        # compute hits, misses and false alarms\n",
    "        # if 'leadtime' in pred.dims:\n",
    "        #     aux = events2hits(rean_exc.sel(id=stn), pred, center=center, w=window)\n",
    "        # else:\n",
    "        #     aux = events2hits(rean_exc, pred, center=center, w=window)\n",
    "        hits[label] = events2hits(rean_exc.sel(id=stn), pred, center=center, w=window)\n",
    "    hits = dict2da(hits, dim='persistence')\n",
    "\n",
    "    print(f'Exporting file {file_out}', end='\\r')\n",
    "    hits.to_netcdf(file_out)\n",
    "\n",
    "    del pred, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc86f5d-c77b-4d86-ad63-39d2c780bffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
