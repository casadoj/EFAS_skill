{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Skill assessment - explanation\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 03-03-2023<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "\n",
    "In this notebook I'll test and explain the procedure to assess the EFAS skill in predicting flood events eventwisely. By eventwisely I mean that I will look at the duration of the whole event, to check if at any point in time during the event there was a notification regarding that event.\n",
    "\n",
    "For the sake of understanding, I will limit the analysis to one station, one meteorological forecast, and a short period of time that includes a flood event. I will be looking at station ID 350 (in the Rhine river) during the month of July 2021, where a major flood event hit this catchment. I will only use the discharge forecasted with the ECMWF ensemble forcing.\n",
    "\n",
    "The notification criteria that I will apply are:\n",
    "\n",
    "* Leadtime larger than 48 h.\n",
    "* Total probability threshold of 30%. To compute total probability I will use the model mean.\n",
    "* For a timestep to be considered an event, the probability threshold must be exceeded for at least 3 consecutive forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "os.chdir('../py/')\n",
    "from compute import *\n",
    "from plot.results import *\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477f5366-f0f1-4293-bbf7-dede3665210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea74215-84d5-4a90-a538-8ed7f45cc7dd",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0901d6a2-a83f-4d91-8d2a-272a111355b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc41484-f71e-41a9-8522-6b0787dcbad7",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e7075f-3927-49b2-98b5-1643197f660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = cfg.get('reporting_points', {}).get('area', 500)\n",
    "\n",
    "# reporting points\n",
    "path_stations = cfg.get('reporting_points', {}).get('output', '../results/reporting_points/')\n",
    "file_stations = f'{path_stations}reporting_points_over_{area_threshold}km2.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94519fd-f17b-4062-a80e-4aedff028793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations and study period\n",
    "stn = 5209# 350 #1358, #350 #934\n",
    "start, end = datetime(2022, 11, 1), datetime(2022, 12, 31)\n",
    "forcing = 'EUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1667-e5d2-45fb-83a1-647f9c9c4e7a",
   "metadata": {},
   "source": [
    "## 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a0a6a6-bcbd-4056-8b6c-0eae6a8dacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'explanation'\n",
    "\n",
    "# path where results will be saved\n",
    "results_path = '../results/'\n",
    "path_out = f'{results_path}skill/{name}/'\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf9fefe-8375-44af-b1c0-f1886b763c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate reference system when plotting maps\n",
    "proj = ccrs.LambertAzimuthalEqualArea(central_longitude=10, central_latitude=52, false_easting=4321000, false_northing=3210000, globe=ccrs.Globe(ellipse='GRS80'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b284a86-2d64-4907-a259-e3c67b6de2d5",
   "metadata": {},
   "source": [
    "### 1.1 Stations\n",
    "\n",
    "I load all the stations in the catchment and select the single station in which I am interested for this test-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429249a8-858f-43b2-bb61-c67db6a38f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. stations:\t\t\t1\n"
     ]
    }
   ],
   "source": [
    "# load table of fixed reporting points\n",
    "stations = pd.read_parquet(file_stations)\n",
    "stations = pd.DataFrame(stations.loc[stn]).transpose()\n",
    "print('no. stations:\\t\\t\\t{0}'.format(stations.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6898ef9-0c78-4b77-959b-fb87de8716c5",
   "metadata": {},
   "source": [
    "### 1.2 Forecast: exceedance\n",
    "\n",
    "I load the preprocessed forecast data. In a previous [notebook](4_0_forecast_exceedance_review.ipynb), the forecasted discharge was converted to probability of exceeding the 5-year return period threshold. The ouput of that process is a NetCDF file per station with the forecasted probability of exceedance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df218526-bf92-4896-8010-514e11be0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the FORECAST data\n",
    "path_forecast = cfg.get('exceedance', {}).get('output', {}).get('forecast', f'../results/exceedance/forecast/')\n",
    "fore_exc = xr.open_dataset(f'{path_forecast}{stn:04d}.nc')\n",
    "\n",
    "# trim data to the study period\n",
    "print(f'Study period\\nstart:\\t{start}\\nend:\\t{end}')\n",
    "fore_exc = fore_exc.sel(forecast=slice(start - timedelta(days=10), end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c23cc9-47d1-45a9-9387-66788d90aed1",
   "metadata": {},
   "source": [
    "### 1.3 Reanalysis: exceedance & events\n",
    "\n",
    "I load the preprocessed reanalysis data. In a previous [notebook](2_2_reanalysis_preprocessing.ipynb), the reanalysis discharge data was preprocessed to create timeseries of exceedance over the 5-year return period threshold.\n",
    "\n",
    "Out of the exceedance timeseries I calculate another timeseries of the onset of flood events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f23e919d-8b9a-43b7-b49a-455cf464cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study period\n",
      "start:\t2022-11-01 00:00:00\n",
      "end:\t2022-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# load \"observed\" exceedances over the discharge threshold\n",
    "path_reanalysis = cfg.get('exceedance', {}).get('output', {}).get('reanalysis', f'../results/exceedance/reanalysis/')\n",
    "rean_exc = xr.open_dataset(f'{path_reanalysis}{stn:04d}.nc')\n",
    "\n",
    "# trim data to the study period\n",
    "print(f'Study period\\nstart:\\t{start}\\nend:\\t{end}')\n",
    "rean_exc = rean_exc.sel(datetime=slice(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11795e02-d2d9-4240-a1e9-da12d7072339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load probability of exceeding the discharge threshold in the REANALYSIS data\n",
    "# rean_exc = pd.read_parquet(f'{path_reanalysis}/exceedance_rl5.parquet')\n",
    "# rean_exc.columns = rean_exc.columns.astype(int)\n",
    "# rean_exc = rean_exc.loc[start:end, stations.index.tolist()]\n",
    "\n",
    "# # compute onsets of the flood events\n",
    "# rean_onsets = rean_exc.astype(int).diff(axis=0) == 1\n",
    "# rean_onsets.iloc[0,:] = rean_exc.iloc[0,:]\n",
    "\n",
    "# # create a DataArray with the number of observed events per station\n",
    "# n_events_obs = xr.DataArray(rean_onsets.sum(), dims=['id'], coords={'id': rean_onsets.columns.tolist()})\n",
    "# print('no. events:\\t\\t\\t{0}'.format(n_events_obs.sum().data))\n",
    "\n",
    "# # select stations with events\n",
    "# mask_stn = (n_events_obs > 0).to_pandas()\n",
    "# print('no. stations with events:\\t{0}'.format(mask_stn.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9a1f0-2e0d-475a-9b19-cc35c9ff5b6b",
   "metadata": {},
   "source": [
    "## 2 Assess skill\n",
    "\n",
    "In this section I will compute the skill of the EFAS predictions in terms of different metrics: recall, precision and the f1-score. The three metrics are based in the contingency table of hits ($TP$ for true positives), false alarms ($FP$ for false positives) and misses ($FN$ for false negatives).\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$f1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FN + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca24d60-b40f-4bb2-a7d9-a37bb9276514",
   "metadata": {},
   "source": [
    "### 2.1 Reformat data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ce24a-bac0-479a-acc1-1759afb27150",
   "metadata": {},
   "source": [
    "#### 2.1.1 Observed probability of exceendace\n",
    "\n",
    "In the case of the observations, I only need to convert the `pandas.DataFrame` into a `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ed374-e827-43e3-bf2f-f626085dafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataArray with observed threshold exceedance\n",
    "obs = df2da(rean_exc, dims=['id', 'datetime'], plot=False, figsize=(16, 20), title='observed exceendace')\n",
    "obs = obs.sel(id=stn).astype(int)\n",
    "\n",
    "print(obs.dims)\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd2602-9f29-4fef-8876-da4735ec1e6b",
   "metadata": {},
   "source": [
    "#### 2.1.2 Predicted probability of exceedance\n",
    "\n",
    "To be able to apply the notification criteria in a simple manner, I need to reshape the exceedance forecast. Up until now, the exceedance forecast is saved in a matrix where the `forecast` is one dimension and the `leadtime` is another dimension (Figure 1). This format is very convinient in therms of storage, since it's very compact. However, this shape is not easy to compare against the observed exceedances of the discharge threshold because the dimensions don't match (`obs` has a single dimension `datetime` that doesn't exist in the exceedance forecast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6fcea-e5d0-4081-a3f6-d6b41358fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataArray of predicted exceedance\n",
    "pred = fore_exc.to_array(dim='id')\n",
    "# extract and plot forecast data in the original format\n",
    "pred = pred.sel(id=stn, model=forcing)#, forecast=slice(st, en))#'2021-06-25', '2021-07-28'))\n",
    "\n",
    "# plot raw data\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('probability', cmaplist, cmap.N)\n",
    "bounds = np.arange(0, 1.01, .05)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "xticklabels = (pred.leadtime / np.timedelta64(1, 'h')).astype(int).data\n",
    "plot_DataArray(pred.sel(forecast=slice(None, None, -1)),\n",
    "               cmap=cmap, norm=norm, ytick_step=2, ylabel='forecast', xtick_step=2, xlabel='leadtime (h)', figsize=(6, 8),\n",
    "               xticklabels=xticklabels, cbar_kws={'shrink': .5, 'label': 'exceedance probability (-)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd28de2-714f-47fe-9e1b-ba8f5384138b",
   "metadata": {},
   "source": [
    ">***Figure 1**. Original format of the forecast data. The matrix represents probability of exceeding the 5-year return period of discharge. It corresponds to a single station an meteorological forcing.*\n",
    "\n",
    "The diagonal shapes in Figure 1 represent the same timestep. As we move backwards in terms of forecast time (moving down in the figure), the leadtime increases (moving right in the figure).\n",
    "\n",
    "Instead, I will reshape the original exceendance forecast into a new matrix in which a dimension named `datetime` represents athe actual date and time, and another dimension represents `leadtime`. In this way, a column in the matrix represents the same timestep and can be easily compared against observations. The drawback of this approach is that it will create missing values in the lower-left and upper-right corners of the matrix, but this only applies to the beginning and end of the study period.\n",
    "\n",
    "To make clear what this reshaping means, I will do it step by step. Figure 2 represents the first transformation. I took the matrix in Figure 1 and every new forecast is shifted 2 timesteps (12 hours) towards the right. The result is a matrix in which every column represents the same timestep. However, the new matrix has a huge amount of gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f43899-2630-4187-9ea8-ea1636a2f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the forecast data and plot it\n",
    "st, en = [pd.to_datetime((pred.forecast[i] + pred.leadtime[i]).data) for i in [0, -1]]\n",
    "coords = {'datetime': pd.date_range(st, en, freq='6h'),\n",
    "          'forecast': pred.forecast}\n",
    "aux = xr.DataArray(coords=coords, dims=list(coords))\n",
    "for j, forecast in enumerate(pred.forecast):\n",
    "    io = j * 2\n",
    "    ie = io + len(pred.leadtime)\n",
    "    aux[io:ie, j] = pred.sel(forecast=forecast)\n",
    "\n",
    "plot_DataArray(aux.transpose().sel(forecast=slice(None, None, -1)),\n",
    "               cmap=cmap, norm=norm, ytick_step=2, ylabel='forecast', xtick_step=4, xlabel='datetime', figsize=(16, 6.5),\n",
    "               cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a02bb-0fc4-4b84-8222-4f8eeb954352",
   "metadata": {},
   "source": [
    "> ***Figure 2**. Result of the first step in the reshaping process. Every forecast has been shifted 2 timesteps (12 h) so that every column in the matrix represents the same timestep.*\n",
    "\n",
    "Another feature of the matrix in Figure 2 is that, for every column, the cells on top represent the forecast closest to that timestep (6-12 h leadtime), whereas the cells at the bottom represent the furthest one (10 days leadtime). I will use this feature in the next step. I will \"project\" the matrix towards the top; it will cause that I lose the forecast date, which I will convert into leadtime. Since every forecast has 40 leadtimes with 6 h resolution, but the forecast frequency is 12 h, every timestep is predicted by 20 forecast. Therefore, the resulting matrix will have 20 leadtimes (not 40) with 12 h resolution (instead of 6 h). \n",
    "\n",
    "Figure 3 shows the result of this projection step. The new matrix has a lot less gaps and keeps the two main features of Figure 2: columns represent the same timestep; the top row corresponds to the closest forecast, whereas the bottom row to the furthest. Actually, the gaps we see in Figure 3 will only occur at the beginning and end of the study period, so when studying 2 years of data (as the case of the main analysis) these gaps are negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2f54f-88e6-4ae8-8246-9215c82942b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reshape = reshape_DataArray(pred, trim=False)\n",
    "print(pred_reshape.dims)\n",
    "print(pred_reshape.shape)\n",
    "\n",
    "plot_DataArray(pred_reshape,\n",
    "               cmap=cmap, norm=norm, xtick_step=4, ytick_step=2, xlabel='datetime', ylabel='leadtime (h)', figsize=(16, 2),\n",
    "               cbar_kws={'shrink': .666, 'label': 'exceedance probability (-)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a9a2f-a214-46de-af6a-716b10997679",
   "metadata": {},
   "source": [
    "> ***Figure 3**. Matrix resulting from projecting the matrix in Figure 2 in the `forecast` dimension.*\n",
    "\n",
    "The last little transformation is removing the periods at the beginning and at the end which contain gaps. The result is Figure 4, which will be the final result of the reshaping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f4b50-84fa-409c-b550-de9e0ff39d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reshape = reshape_DataArray(pred, trim=True)\n",
    "pred_reshape = pred_reshape.loc[dict(datetime=obs.datetime)]\n",
    "print(pred_reshape.dims)\n",
    "print(pred_reshape.shape)\n",
    "\n",
    "plot_DataArray(pred_reshape,\n",
    "               cmap=cmap, norm=norm, xtick_step=4, ytick_step=2, xlabel='datetime', ylabel='leadtime (h)', figsize=(16, 2.2),\n",
    "               cbar_kws={'shrink': .666, 'label': 'exceedance probability (-)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8e623-2cf2-42e3-b7c1-c89b827196f4",
   "metadata": {},
   "source": [
    ">***Figure 4**. Final matrix of the reshaping process. Columns represent timesteps and rows increasing leadtime.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9aadfe-3e43-4352-83f6-57fe7f59ca2b",
   "metadata": {},
   "source": [
    "### 2.2 Skill\n",
    "\n",
    "Once the forecasted dataset is formatted as above explained, we must define what an event is (probability threshold, persistence, minimum leadtime), and then it's \"easy\" to compute the forecasted events and calculate the performance (recall, precision and f1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952be197-a0a6-4da3-9201-e975bd0ab360",
   "metadata": {},
   "source": [
    "#### 2.2.1 Define criteria\n",
    "\n",
    "In this test-case I will use the current notification criteria:\n",
    "\n",
    "* Probability threshold: 30 %.\n",
    "* Persistence of 3 consecutive forecast.\n",
    "* Leadtime larger than 48 h, i.e., 60 h or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83feaf-0999-44ed-81b7-b8cfde1ab1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = .3\n",
    "persistence = (3, 3) # (window width, number of positives in that window)\n",
    "min_leadtime = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9d103-799b-43ae-bdcf-a9cf1de3c2f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.2 Compute events\n",
    "\n",
    "The function `compute_events` applies the previous criteria on the forecasted exceedance probability matrix (Figure 4). It consists of three steps:\n",
    "\n",
    "1. **Probability threshold**. By applying the probability threshold, the matrix is converted into a boolean matrix (exceeds or not exceeds). At this step the matrix keeps its dimensions.\n",
    "```\n",
    "exceedance = (forecast >= probability).astype(int)\n",
    "```\n",
    "\n",
    "2. **Persistence**. A rolling sum over `leadtime` of width equal to the persistence criterion computes the number of consecutive positive forecasts. When this sum is equal to the persitence value, the cell complies with the criterion. At this step, the matrix still keeps its original dimensions.\n",
    "```\n",
    "events = exceedance.rolling({'leadtime': persistence[0]}).sum() >= persistence[1]\n",
    "```\n",
    "\n",
    "3. **Leadtime**. For leadtimes larger than the minimum, we check the timesteps at which there is at least one positive value. The matrix is, hence, reduced to only one dimension: `datetime`.\n",
    "```\n",
    "events.sel(leadtime=slice(min_leadtime, None)).any('leadtime').astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88968d52-433b-491f-9c94-fdd1211891fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf33e1-fde2-40b3-824b-79f72cd23196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e047d1a-7633-40cf-825a-281131d56067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute events\n",
    "pred_events = compute_events(pred_reshape, probability, persistence, by_leadtime=True)#min_leadtime=min_leadtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797642cb-e4f4-49ad-a4a5-a802c0748564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DataArray(pred_events, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bf191-c18e-4f74-bd80-90a0ab5583b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of predicted events\n",
    "n_events_pred = count_events(pred_events).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd0b55-7ce1-4e5e-a2ad-a76e704d864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hits, misses and false alarms\n",
    "if 'leadtime' in pred_events.dims:\n",
    "    hits = [compute_hits(obs, pred_events.sel(leadtime=lt), center=True, w=5) for lt in pred_events.leadtime.data]\n",
    "    hits = xr.concat(hits, dim='leadtime').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e7aa1-0532-4f97-8fba-50ca9c461987",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e853d4-900a-4429-97ad-2d1e1df4fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['TP', 'FN', 'FP']:\n",
    "    plt.plot(hits.leadtime, hits[var])\n",
    "plt.plot(n_events_pred.leadtime, n_events_pred.data, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d65e8-141f-4430-aa78-d2190e0e1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_ = events.copy()\n",
    "for lt in events_.leadtime.data:\n",
    "    events_.loc[dict(leadtime=lt)] = events.sel(leadtime=slice(lt, None)).any('leadtime').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78d692-7f27-4634-80d3-b8eb02f99724",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DataArray(events_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16562ba8-fcf9-4d71-8535-e7836cb085e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d187f8-b094-4565-bcaf-6717ea815acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cb9c42c-1456-4390-8883-431952469aec",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0682330-5d21-42d2-b3a3-aff016404354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute events\n",
    "pred_events = compute_events2D(pred_reshape, probability, persistence, min_leadtime=min_leadtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2420a85-1ac7-4d63-b384-1e947c786a2f",
   "metadata": {},
   "source": [
    "#### 2.2.3 Performance\n",
    "\n",
    "The function `compute_hits` compares the matrixes of observed and predicted events and outputs a dataset that contains the true positives (TP), false negatives (FN) and false positives (FP). Before comparing the two matrixes, the function allows to create a buffer over the prediction. This buffer is meant to consider small lags in time between predicted and the observed events; the buffer can be symmetrical (`center=True`) or asymmetrical (`center=False`) and its width is defined with the attribute `w`.\n",
    "\n",
    "* An observed event is considered a true positive if at least one timestep during the obserserved event was predicted by the buffered matrix. \n",
    "* False negatives are the difference between the number of observed events and the number of true positives.\n",
    "* False positives are the difference between the number of predicted events and the number of true positives.\n",
    "\n",
    "With these three values the performance metrics (recall, precision, f1) can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79b7cf-6595-402a-ae4e-56a957872f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hits, missses and false alarms\n",
    "hits = compute_hits(obs, pred_events, center=True, w=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfc5e5-196d-468d-8b48-86e203fada01",
   "metadata": {},
   "source": [
    "#### 2.2.4 Graphical explanation\n",
    "\n",
    "Figure 5 shows graphically the whole process of computing the skill:\n",
    "\n",
    "1. The timeseries of observed events (exceedances of the 5-year return period of discharge).\n",
    "2. A forecast matrix that shows when the forecasted discharge exceeds the 5-year discharge with a probability larger than 30 %.\n",
    "3. The timeseries of predicted events computed from the previous matrix by applying the persistence criterion.\n",
    "4. A buffer on the timeseries of predicted events.\n",
    "5. The timeseries of true positives, i.e., timesteps at which both the matrix in step 1. and that in step 4. are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fd3b9-37ca-4ee8-9d56-808361194115",
   "metadata": {},
   "outputs": [],
   "source": [
    "das = {'observed events': obs,\n",
    "       'exceedance': compute_events2D.exceedance.sel(leadtime=slice(min_leadtime, None)),\n",
    "       'predicted events': pred_events,\n",
    "       'buffered prediction': compute_hits.buffer,\n",
    "       'true positives': compute_hits.true_positives}\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6), constrained_layout=True)\n",
    "height_ratios = [len(da.leadtime) if 'leadtime' in da.dims else 1 for key, da in das.items()]\n",
    "gs = fig.add_gridspec(nrows=len(height_ratios), height_ratios=height_ratios)\n",
    "\n",
    "for i, (label, da) in enumerate(das.items()):\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    if i + 1 < len(height_ratios):\n",
    "        xticklabels, xlabel = [], None\n",
    "    else:\n",
    "        xticklabels, xlabel = da.datetime.data, 'datetime'\n",
    "    if label == 'exceedance':\n",
    "        ylabel = 'leadtime (h)'\n",
    "    else:\n",
    "        ylabel = None\n",
    "    plot_DataArray(da, xtick_step=4, cbar=False, title=label, xticklabels=xticklabels, xlabel=xlabel,\n",
    "                   ylabel=ylabel, ax=ax, cmap='magma_r')\n",
    "    \n",
    "fig.suptitle('{0} - {1} ({2})'.format(stn, *stations.loc[stn, ['name', 'catchment']]), fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa9d4c-ff20-4446-a4a6-c7b5badad6e1",
   "metadata": {},
   "source": [
    "> ***Figure 5**. Graphical explanation of the skill computation. The top plot shows the exceedances of the 5-year return period in the \"observed\" (water balance) discharge. The second plot shows the timesteps and leadtimes in which the forecast exceeds this 5-year return period with at least a 30 % probability. The third plot shows the timesteps in which the forecast complies with the persistence critera, i.e., at least 3 consecutive forecasts predict an event at that timestep. The fourth plot shows a buffer of 2 timesteps (12 h) over the previous predicted events, to allow for some lag between prediction and observation. The bottom plot shows the true positives, i.e., the timesteps at which both the observed and the buffered predicted events are positive.*\n",
    "\n",
    "Out of the three observed events, the forecast correctly predicts (with these notification criteria) 2 events. Therefore, there is one false negative (miss) and no false positives (false alarm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438eb180-20a0-452c-97a5-bf656d8feb65",
   "metadata": {},
   "source": [
    "## 3 Idea\n",
    "\n",
    "If we look at Figure 4 in 3 dimensions (Figure 6), the matrix can be seen as a 2D probability density function (PDF), where flood events are clusters of higher probability. A large amount of this information (both in terms of probability and duration of the event) is lost when applying the probability threshold criteria. As an example, compare Figure 4 with the second plot in Figure 5. The persistence criterion (and the buffer) seems to be proxies to replace this lost of information.\n",
    "\n",
    "It would be interesting to try another procedure that would find clusters of higher probability (events) and computes a cumulative probability (CDF, cumulative density function) of that event. In this approach, notifications would be sent when the CDF of an events exceeds certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddda5f-df3a-4ba6-bd97-9c14c5b51ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(0, pred_reshape.shape[1]), np.arange(0, pred_reshape.shape[0]))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(xx, yy, pred_reshape.data, cmap=cmap, norm=norm, rcount=80, ccount=480, edgecolor='none', alpha=.666)\n",
    "ax.view_init(45, 70)\n",
    "ax.set(xlabel='datetime', ylabel='leadtime (h)', zlabel='probability')\n",
    "ax.set_xticks(np.arange(0, pred_reshape.shape[1], 16))\n",
    "ax.set_xticklabels(pd.to_datetime(pred_reshape.datetime.data[::16]).date)\n",
    "ax.set_yticks(np.arange(0, pred_reshape.shape[0], 4))\n",
    "ax.set_yticklabels(pred_reshape.leadtime.data[::4])\n",
    "ax.set_title('forecast', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81180179-2766-4932-9eb0-2d1c46ac530d",
   "metadata": {},
   "source": [
    "> ***Figure 6**. 3D representation of the matrix of forecast probability of exceeding the 5-year return period of discharge. The z-axis and colours represent higher probabilities. Flood events are depicted as \"hills\". The volume beneath each of these \"hills\" would be the cumulative probability of the event.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
