{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Forecast preprocessing\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado<br>\n",
    "**Date**: 22-05-2023<br>\n",
    "\n",
    "**Introduction**:<br>\n",
    "This code computes the probability of the forecasted discharge of exceeding a threshold associated with a specific return period.\n",
    "\n",
    "The input data are:\n",
    "* The discharge forecast for the complete set of reporting points, numerical weather prediction (NWP) model and the complete study period. This data is saved in NetCDF format in a hard disk; due to its size it cannot be included in the GitHub repository.\n",
    "* The discharge thresholds associated to each reporting point, from which a specific return period (by default 5 years) will be used.\n",
    "\n",
    "The output is the probability of exceeding the specified return period. This is a matrix of multiple dimensions: station, NWP model, date-time, and lead time. Actually, 2 matrixes are computed, one related to the probability of exceeding the specified threshold ($Q_{rp}$), and another with the probability of exceeding a slighly lower threshold ($0.95\\cdot Q_{rp}$). This lower threshold is used to avoid false positives or false negatives in forecasts very close to the observation, but on opposite sides of the discharge threshold.\n",
    "\n",
    "The results are saved inside the repository as NetCDF, saving one file per reporting point.\n",
    "\n",
    "**Questions**:<br>\n",
    "\n",
    "\n",
    "**Tasks to do**:<br>\n",
    "* [ ] Probably, the `compute_exceedance` function could be improved so the two discharge thresholds are computed at the same time. Since the function needs to load all the forecast discharge records, it is a waste of time the current procedure, in which all the records need to be loaded twice (once for each discharge threshold).\n",
    "\n",
    "**Interesting links**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from computations import *\n",
    "from plots import *\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93f9bd-6b3d-4ee8-a815-4c5048c13ac3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c1f647-ee7e-44ec-a163-b789a8f90795",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# area threshold\n",
    "area_threshold = cfg.get('reporting_points', {}).get('area', 500)\n",
    "\n",
    "# return period\n",
    "rp = cfg.get('return_period', {}).get('threshold', 5)\n",
    "\n",
    "# percent buffer over the discharge threshold\n",
    "reducing_factor = cfg.get('return_period', {}).get('reducing_factor', None)\n",
    "\n",
    "# PATHS\n",
    "\n",
    "# local directory where I have saved the raw discharge data\n",
    "path_in = cfg.get('paths', {}).get('input', {}).get('discharge', {}).get('forecast', f'../data/discharge/reanalysis/')\n",
    "\n",
    "# path where the output exceedance datasets will be saved\n",
    "path_out = cfg.get('paths', {}).get('output', {}).get('exceedance', {}).get('forecast', f'../data/exceedance/forecast/')\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "# reporting points\n",
    "path_stations = cfg.get('paths', {}).get('input', {}).get('reporting_points', '..data/reporting_points/')\n",
    "file_stations = f'{path_stations}reporting_points_over_{area_threshold}km2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec6de5-35d0-4e78-a74d-0b9cc867050c",
   "metadata": {},
   "source": [
    "### 1 Discharge forecast\n",
    "\n",
    "#### List available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a7d7fe-2b33-41db-9d66-344913ea1a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COS:\t1460 files\n",
      "DWD:\t1460 files\n",
      "EUD:\t1460 files\n",
      "EUE:\t1460 files\n"
     ]
    }
   ],
   "source": [
    "# list files\n",
    "fore_files = {model: [] for model in list(models)}\n",
    "for year in [2021, 2022]:\n",
    "    for month in range(1, 13):    \n",
    "        # list files\n",
    "        for model in models:\n",
    "            fore_files[model] += glob.glob(f'{path_in}{model}/{year}/{month:02d}/*.nc')\n",
    "\n",
    "# count files and check if all are avaible\n",
    "n_files = pd.Series(data=[len(fore_files[model]) for model in models], index=models)\n",
    "\n",
    "# list of forecast from the beginning to the end of the data\n",
    "start, end = datetime(1900, 1, 1), datetime(2100, 1, 1)\n",
    "for model in models:\n",
    "    st, en = [datetime.strptime(fore_files[model][step][-13:-3], '%Y%m%d%H') for step in [0, -1]]\n",
    "    start = max(st, start)\n",
    "    end = min(en, end)\n",
    "dates = pd.date_range(start, end, freq='12h')\n",
    "\n",
    "# find missing files\n",
    "if any(n_files != len(dates)):\n",
    "    missing = {}\n",
    "    for model in models:\n",
    "        filedates = [datetime.strptime(file[-13:-3], '%Y%m%d%H') for file in fore_files[model]]    \n",
    "        missing[model] = [date for date in dates if date not in filedates]\n",
    "    print('mising files:', missing)\n",
    "\n",
    "# trim files to the period where all models are available\n",
    "for model in models:\n",
    "    fore_files[model] = [file for file in fore_files[model] if start <= datetime.strptime(file[-13:-3], '%Y%m%d%H') <= end]\n",
    "    print('{0}:\\t{1} files'.format(model, len(fore_files[model])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79709b93-c32f-4bac-81b0-69a5ffb6cc57",
   "metadata": {},
   "source": [
    "## 2 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c2fd7-ae1c-434c-b947-08c417f1bb8b",
   "metadata": {},
   "source": [
    "### 2.1 Stations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3d2f2-75b3-4ac1-871b-858efdd5a920",
   "metadata": {},
   "source": [
    "If the preprocessing of the discharge forecast was only to be done in the selected reporting points, the code to run is the following\n",
    "\n",
    "```Python\n",
    "# load selected points for all the catchments\n",
    "stations = pd.DataFrame()\n",
    "catchments = []\n",
    "results_path = '../results/reporting_points/'\n",
    "for folder in os.listdir(results_path):\n",
    "    try:\n",
    "        stn_cat = pd.read_csv(f'{results_path}{folder}/points_selected.csv', index_col='station_id')\n",
    "        stations = pd.concat((stations, stn_cat))\n",
    "        catchments.append(folder)\n",
    "    except:\n",
    "        continue\n",
    "print('no. stations:\\t\\t\\t{0}'.format(stations.shape[0]))\n",
    "```\n",
    "\n",
    "Instead, if the preprocessing is to be done in all the reporting points above a certain area threshold, the code to run is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9114c0d-e8b6-4e91-ba57-7ec1a11f8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporing points\n",
    "stations = pd.read_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc81fe-1c33-4bec-9075-67d2ea01dc4d",
   "metadata": {},
   "source": [
    "### 2.2 Reforecast data: exceedance probability\n",
    "\n",
    "This section will iteratively (station by station) load all the available forecast and compute the probability of exceeding the discharge threshold for each of the meteorological forcings. The result will be a NetCDF file for each station that contains the exceedance probability. These files will be later used in the skill assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff2b77d-9afc-4920-921c-adf2df95880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. new stations:\t\t\t2371\n"
     ]
    }
   ],
   "source": [
    "# select stations that haven't been processed before\n",
    "files = glob.glob(f'{path_out}*.nc')\n",
    "if len(files) > 0:\n",
    "    old_stations = [int(file.split('\\\\')[-1].split('.')[0]) for file in files]\n",
    "    new_stations = set(stations.index).difference(old_stations)\n",
    "    stations = stations.loc[new_stations]\n",
    "print('no. new stations:\\t\\t\\t{0}'.format(stations.shape[0]))\n",
    "\n",
    "# generate a DataArray with the discharge threshold of the stations in the catchment\n",
    "thresholds = xr.DataArray(stations[f'rl{rp}'], dims='id', coords={'id': stations.index.astype(str).tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7625c8a2-c2df-4f45-9518-ba8cf004cce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# exceedances over Q5\n",
    "exceedance_q5 = compute_exceedance(fore_files, thresholds) # * (1 + reducing_factor))\n",
    "# exceedances over 95% of the Q5\n",
    "exceedance_buffer = compute_exceedance(fore_files, thresholds * (1 - reducing_factor))\n",
    "# join both DataArrays in a single Dataset\n",
    "exceedance = xr.Dataset({'high': exceedance_q5, 'low': exceedance_buffer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c178304c-91ce-47a9-9f0f-b41329462faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eee76893e424bbc83be94c58ade8eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excecution time: 1256.8 s\n"
     ]
    }
   ],
   "source": [
    "# export files station by station\n",
    "for stn in tqdm_notebook(exceedance.id.data):\n",
    "    file = f'{stn:>04}.nc'\n",
    "    if file in os.listdir(path_out):\n",
    "        print(f'File {file} already exists')\n",
    "        continue\n",
    "    else:\n",
    "        exceedance.sel(id=stn).to_netcdf(f'{path_out}{file}')\n",
    "        \n",
    "end = time.perf_counter()\n",
    "\n",
    "print('excecution time: {0:.1f} s'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
