{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2493eb",
   "metadata": {},
   "source": [
    "# Skill assessment: NWP combination\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 09-02-2024<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "In this notebook I will analyse the EFAS skill in predicting flood events in general, i.e., looking whether events where predicted at some point in time, regardless of neither the offset nor the duration of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import cartopy.crs as ccrs\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import yaml\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir('../py/')\n",
    "from config import Config\n",
    "from compute import *\n",
    "from convert import dict2da\n",
    "from optimize import find_best_criterion, find_best_criteria, find_best_criteria_cv\n",
    "from plot.results import * \n",
    "from plot.maps import create_cmap, map_stations, map_hits, map_skill, map_events, gauge_legend\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f52219-ac24-403d-9f2b-9f7a42a65582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default text font size\n",
    "plt.rc('font', size=15)\n",
    "# Set the axes title font size\n",
    "plt.rc('axes', titlesize=16)\n",
    "# Set the axes labels font size\n",
    "plt.rc('axes', labelsize=15)\n",
    "# Set the font size for x tick labels\n",
    "plt.rc('xtick', labelsize=13)\n",
    "# Set the font size for y tick labels\n",
    "plt.rc('ytick', labelsize=13)\n",
    "# Set the legend font size\n",
    "plt.rc('legend', fontsize=13)\n",
    "# Set the font size of the figure title\n",
    "plt.rc('figure', titlesize=17)\n",
    "\n",
    "# projection that will be used in the maps\n",
    "proj = ccrs.LambertAzimuthalEqualArea(central_longitude=10,\n",
    "                                      central_latitude=52, \n",
    "                                      false_easting=4321000, \n",
    "                                      false_northing=3210000,\n",
    "                                      globe=ccrs.Globe(ellipse='GRS80'))\n",
    "\n",
    "# extent of the maps: [min_lon, max_lon, min_lat, max_lat]\n",
    "extent = [-10, 44, 28, 70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e0fda-fb63-48b5-9cc3-679d702d934a",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da98e640-2d7d-4a39-b073-eeebfeea387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('../conf')\n",
    "config = Config.load_from_yaml(config_path / 'config_COMB_leadtime_ranges.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f91d7-4689-4ee0-8839-548071e79a8f",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f00a4e-1c59-4a35-93da-3fc207f88ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = config.reporting_points['area']\n",
    "\n",
    "# reporting points\n",
    "path_stations = config.reporting_points['output']\n",
    "file_stations = path_stations / f'reporting_points_over_{area_threshold}km2.parquet'\n",
    "\n",
    "# catchments\n",
    "catchments = config.reporting_points['catchments']\n",
    "\n",
    "# rivers\n",
    "rivers_shp = config.reporting_points['rivers']\n",
    "if rivers_shp is not None:\n",
    "    rivers = gpd.read_file(rivers_shp)\n",
    "    \n",
    "# minimum performance required from the reporting points\n",
    "min_kge = config.reporting_points['KGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185a7cd-a77a-4137-bd8f-9103abd38c68",
   "metadata": {},
   "source": [
    "### 1.2 Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7394f9de-8e47-40de-bda0-6cdf4c16ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return period\n",
    "rp = config.discharge['return_period']['threshold']\n",
    "\n",
    "# lead time ranges\n",
    "leadtime = config.hits['leadtime']\n",
    "\n",
    "# parameters of the rolling window used to compute hits\n",
    "window = config.hits['window']\n",
    "\n",
    "# dissagregate the analysis by seasons?\n",
    "# seasonality = config.hits['seasonality']\n",
    "\n",
    "# path that contains the NetCDFs with hit, misses and false alarms pro\n",
    "path_in = config.hits['output']\n",
    "if leadtime is None:\n",
    "    time_agg = 'all_leadtimes'\n",
    "elif len(leadtime) == 10:\n",
    "    time_agg = 'daily'\n",
    "elif len(leadtime) == 20:\n",
    "    time_agg = '12h'\n",
    "else:\n",
    "    time_agg = '_'.join([str(lt + 12) for lt in leadtime])\n",
    "path_in = path_in / f'{rp}/COMB/{time_agg}/window_{window}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e08960-70df-464f-b0f5-5635b080ddf9",
   "metadata": {},
   "source": [
    "### 1.3 Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dba0a11-2dcb-44da-92b2-594547118224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current operationa criteria\n",
    "current_criteria = config.skill['current_criteria']\n",
    "\n",
    "# fixed notification criteria\n",
    "min_leadtime = config.skill['leadtime']\n",
    "min_area = config.skill['area']\n",
    "\n",
    "# coefficient of the fbeta-score\n",
    "beta = config.skill['beta']\n",
    "metric = f'f{beta}'\n",
    "\n",
    "# optimization parameters\n",
    "optimization = config.skill['optimization']\n",
    "kfold = optimization['kfold']\n",
    "train_size = optimization['train_size']\n",
    "stratify = optimization['stratify']\n",
    "tolerance = optimization['tolerance']\n",
    "min_spread = optimization['minimize_spread']\n",
    "del optimization\n",
    "\n",
    "# path where results will be saved\n",
    "path_out_root = config.skill['output']\n",
    "if min_kge is not None:\n",
    "    kge = f'kge_{min_kge}'\n",
    "else:\n",
    "    kge = 'no_kge'\n",
    "path_out_root = path_out_root / f'{rp}/COMB/{time_agg}/window_{window}/{kge}'\n",
    "path_out = path_out_root / metric\n",
    "path_out.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9737b56-1e73-4101-b2a9-e52ffb05c0b8",
   "metadata": {},
   "source": [
    "## 2 Data\n",
    "### 2.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f476112-311f-4e14-9619-a7e82ebbe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporting points\n",
    "stations = pd.read_parquet(file_stations)\n",
    "stations[['X', 'Y', 'area']] = stations[['X', 'Y', 'area']].astype(int)\n",
    "stations.sort_values('n_events_obs', ascending=True, inplace=True)\n",
    "\n",
    "# select stations that belong to the selected catchments\n",
    "if catchments is not None:\n",
    "    if isinstance(catchments, list) is False:\n",
    "        catchments = [catchments]\n",
    "    stations = stations.loc[stations.catchment.isin(catchments),:]\n",
    "\n",
    "# remove points with a performance (KGE) lower than the established threshold\n",
    "if min_kge is not None:\n",
    "    mask_kge = ~(stations.KGE <= min_kge)\n",
    "    stations = stations.loc[mask_kge]\n",
    "else:\n",
    "    # remove station with erroneous behaviour\n",
    "    stations = stations.loc[~(stations.n_events_obs >= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab57573-7be4-46c6-8520-9948cd1e33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask stations with events\n",
    "col_events = f'obs_events_{rp}'\n",
    "stations_w_events = (stations[col_events] > 0)\n",
    "\n",
    "print('All points')\n",
    "print('----------')\n",
    "print(f'no. reporting points:\\t\\t{stations.shape[0]}')\n",
    "print('no. stations with events:\\t{0}'.format(stations_w_events.sum()))\n",
    "print('no. observed events:\\t\\t{0}'.format(stations[col_events].sum()))\n",
    "\n",
    "# select stations according to catchment area\n",
    "if min_area > area_threshold:\n",
    "    stations_optimize = stations.loc[stations.area >= min_area].index\n",
    "else:\n",
    "    stations_optimize = stations.index\n",
    "\n",
    "print('\\nPoints selected for otimization')\n",
    "print('-------------------------------')\n",
    "print(f'no. reporting points:\\t\\t{len(stations_optimize)}')\n",
    "print('no. stations with events:\\t{0}'.format((stations.loc[stations_optimize, col_events] > 0).sum()))\n",
    "print('no. observed events:\\t\\t{0}'.format(stations.loc[stations_optimize, col_events].sum()))\n",
    "\n",
    "# suffix that will be used when saving plots\n",
    "suffix = f'{min_area}km2_{len(stations_optimize)}points'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5186b23-9639-49fa-9f75-f55cce348029",
   "metadata": {},
   "source": [
    "**Distribution of catchment area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f23b5-0454-4943-b7ef-c20fc7c85065",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = area_threshold\n",
    "xmax = 500001 #np.ceil(stations.area.max() / 500) * 500\n",
    "bins = np.arange(xmin, xmax, 500).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5.5))\n",
    "sns.histplot(stations.area, ax=ax, bins=bins, alpha=.5, label='all')\n",
    "sns.histplot(stations[stations_w_events].area, ax=ax, alpha=.5, color='orange', bins=bins, label='w/ events')\n",
    "ax.axvline(min_area, color='k', ls=':', lw=.75)\n",
    "ax.set(xlabel='area (km¬≤)', ylabel='no. reporting points', xlim=(xmin, 50000));\n",
    "ax.xaxis.set_major_locator(MultipleLocator(10000))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(2000))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(100))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(20))\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.legend(frameon=False);\n",
    "\n",
    "plt.savefig(path_out_root / f'area_distribution_{suffix}.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb112cb-8c44-4c38-8a13-c30bd57ddb07",
   "metadata": {},
   "source": [
    "> ***Figure 1**. Distribution of the reporting points according to catchment area. The complete set of reporting points is shown in blue, and the subset of reporting points with observed flood events during the study period is shown in red. The balck, dotted line represents the minimum catchment area that will be used for the optimization of the notification criteria.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdbed9-6609-44fa-8229-b68dbc2b6dd9",
   "metadata": {},
   "source": [
    "**Map of number of \"observed\" events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c968949d-01bf-479f-bc0a-bac888740d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'map_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmap_events\u001b[49m(x\u001b[38;5;241m=\u001b[39mstations\u001b[38;5;241m.\u001b[39mloc[stations_optimize]\u001b[38;5;241m.\u001b[39mX, \n\u001b[0;32m      2\u001b[0m            y\u001b[38;5;241m=\u001b[39mstations\u001b[38;5;241m.\u001b[39mloc[stations_optimize]\u001b[38;5;241m.\u001b[39mY, \n\u001b[0;32m      3\u001b[0m            events\u001b[38;5;241m=\u001b[39mstations\u001b[38;5;241m.\u001b[39mloc[stations_optimize][col_events],\n\u001b[0;32m      4\u001b[0m            rivers\u001b[38;5;241m=\u001b[39mrivers,\n\u001b[0;32m      5\u001b[0m            yscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m            size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m      7\u001b[0m            save\u001b[38;5;241m=\u001b[39mpath_out_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap_observed_events_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'map_events' is not defined"
     ]
    }
   ],
   "source": [
    "map_events(x=stations.loc[stations_optimize].X, \n",
    "           y=stations.loc[stations_optimize].Y, \n",
    "           events=stations.loc[stations_optimize][col_events],\n",
    "           rivers=rivers,\n",
    "           yscale='log',\n",
    "           size=4, \n",
    "           save=path_out_root / f'map_observed_events_{suffix}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe35be-0f72-4afc-9952-123342d65c86",
   "metadata": {},
   "source": [
    "> ***Figure 2**. Number of observed flood events during the study period.*\n",
    "\n",
    "The geographical distribution of events is not even. There is a higher proportion of stations with events in Central Europe, British Isles and the Mediterranean catchments than in Estearn and North-Eastern Europe. During the study period there were major events in the Rhine, Meuse and Ebro, which can be seen in the map.\n",
    "\n",
    "The reporting points with more than 5 flood events during the study period were removed, since it is suspicious that the 5-year return period was exceeded so many times in only 2 years of study period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbef3c-6231-44ad-8e33-c66e1b1a06e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a809c4a-d915-43da-905c-bad7176e244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hits for each station\n",
    "hits_stn = xr.open_mfdataset(f'{path_in}/*.nc', combine='nested', concat_dim='id')\n",
    "\n",
    "# extract selected stations\n",
    "stations = stations.loc[set(stations.index).intersection(hits_stn.id.data)]\n",
    "hits_stn = hits_stn.sel(id=stations.index.to_list()).compute()\n",
    "\n",
    "# convert to NaN lead times that can't be reached due to model limitations or persistence\n",
    "hits_stn = limit_leadtime(hits_stn)\n",
    "\n",
    "# subset of the 'hits' dataset with the stations selected for the optimization\n",
    "stations_optimize = list(set(stations_optimize).intersection(hits_stn.id.data))\n",
    "hits_opt = hits_stn.sel(id=stations_optimize).sum('id', skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d715613-88bb-45e2-a0f6-ec01c693ab7f",
   "metadata": {},
   "source": [
    "## 3 Analysis\n",
    "\n",
    "In this section I will compute the skill of the EFAS predictions in different ways. In all the following sections I will work with three metrics: $recall$, $precision$ and the $f_{beta}$ score. The three metrics are based in the contingency table of hits ($TP$ for true positives), false alarms ($FP$ for false positives) and misses ($FN$ for false negatives).\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$f_{beta} = \\frac{(1 + \\beta^2) \\cdot TP}{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}$$\n",
    "\n",
    "### 3.1 Skill computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313de1ab-db62-44ae-b02a-021b6c458628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skill by station\n",
    "skill_stn = hits2skill(hits_stn, beta=beta)\n",
    "\n",
    "# skill dataset for optimizing criteria\n",
    "skill_opt = hits2skill(hits_opt, beta=beta)\n",
    "\n",
    "# export\n",
    "skill_opt.to_netcdf(path_out / f'skill_{suffix}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bea2e5-55dd-4a1c-a7a7-a599ba06cbf1",
   "metadata": {},
   "source": [
    "**Benchmark: NWP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9091f6-8be0-48f2-8af8-b5e72d59ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load skill of NWP\n",
    "path_NWP = Path(str(path_out).replace('COMB', 'NWP'))\n",
    "skill_NWP = xr.open_dataset(path_NWP / f'skill_{suffix}.nc')\n",
    "\n",
    "# load optimized criteria for NWP\n",
    "with open(path_NWP / f'optimized_criteria_{suffix}.pkl', 'rb') as f:\n",
    "    criteria_NWP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720dfcf-35b2-4556-b608-fca8fecfb41a",
   "metadata": {},
   "source": [
    "### 3.2 Analyse overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4de8e-3cf4-4f84-9afb-e021538b7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simplify the plot, they will show only the following values of persistence and leadtime\n",
    "persistences = skill_opt.persistence.data # ['1/1', '2/2', '3/3']\n",
    "\n",
    "if min_leadtime is None:\n",
    "    min_leadtime = int(skill_opt.leadtime.min().data)\n",
    "if leadtime is None:\n",
    "    leadtimes = [12, 36, 60, 84]\n",
    "else:\n",
    "    leadtimes = np.array(leadtime) + 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690c6fa-b4d8-46ef-a0dd-32a0b36a028a",
   "metadata": {},
   "source": [
    "**Skill vs lead time for several probability thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56187b-292b-4ca7-bfb9-386eb6abf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colour map\n",
    "top = cm.get_cmap('Oranges', 128)\n",
    "bottom = cm.get_cmap('Blues_r', 128)\n",
    "newcolors = np.vstack((top(np.linspace(0.2, .95, 128)),\n",
    "                       bottom(np.linspace(.05, .8, 128))))\n",
    "OrBu = ListedColormap(newcolors, name='OrangeBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3474d-4fd6-4d0f-a259-47d82d554973",
   "metadata": {},
   "outputs": [],
   "source": [
    "At = np.unique(np.diff(skill_opt.leadtime))\n",
    "if (len(At) == 1) & (At[0] == 24):\n",
    "        \n",
    "    xmax = (skill_opt.leadtime.max().data - 12) / 24\n",
    "    \n",
    "    # versus the best NWP: EUE\n",
    "    benchmark_file = Path(str(path_out_root).replace('COMB', 'NWP')) / 'skill_EUE_benchmark_leadtime.csv'\n",
    "    benchmark = xr.Dataset.from_dataframe(pd.read_csv(benchmark_file, index_col='leadtime'))\n",
    "    benchmark = benchmark.set_coords(['model', 'probability', 'persistence'])\n",
    "    for pers in ['1/1']:#persistences: # skill_opt.persistence.data:\n",
    "        file = path_out / 'skill_probability_leadtime_{0}_COMB_vs_ENS.jpg'.format(path_out, metric, pers.replace('/', '-'))\n",
    "        plot_skill_by_probability(skill_opt,\n",
    "                                  probability=np.round(np.arange(.1, .96, .1), 3),\n",
    "                                  metric='f0.8',\n",
    "                                  persistence=pers,\n",
    "                                  coldim='approach',\n",
    "                                  benchmark=benchmark,\n",
    "                                  label='ENS',\n",
    "                                  lw=1,\n",
    "                                  alpha=.5,\n",
    "                                  ref_lt=60,\n",
    "                                  ref_p=None, xlabel='lead time (d)', cmap=OrBu, xlim=(0, xmax),\n",
    "                                  save=file)\n",
    "        \n",
    "    # versus the current criteria\n",
    "    for pers in ['1/1']:#persistences: # skill_opt.persistence.data:\n",
    "        file = path_out / 'skill_probability_leadtime_{0}_COMB.jpg'.format(path_out, metric, pers.replace('/', '-'))\n",
    "        plot_skill_by_probability(skill_opt, probability=np.round(np.arange(.1, .96, .1), 3), metric='f0.8', persistence=pers,\n",
    "                                  coldim='approach', benchmark=skill_opt.sel(current_criteria), l=1, alpha=.5, ref_lt=60,\n",
    "                                  ref_p=None, xlabel='lead time (d)', cmap=OrBu, xlim=(0, xmax),\n",
    "                                  save=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11882c-5d16-4145-9ee7-1ba95224812c",
   "metadata": {},
   "source": [
    "**Skill vs probability for several persistences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08707837-239a-4b60-891a-6360307f325c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define colour map\n",
    "top = cm.get_cmap('Oranges', 128)\n",
    "bottom = cm.get_cmap('Blues_r', 128)\n",
    "newcolors = np.vstack((top(np.linspace(.8, 0.2, 128)),\n",
    "                       bottom(np.linspace(.8, .2, 128))))\n",
    "OrBu = ListedColormap(newcolors, name='OrangeBlue')\n",
    "\n",
    "# versus the best NWP: EUE\n",
    "path_NWP = Path(str(path_out).replace('COMB', 'NWP'))\n",
    "benchmark_file = path_NWP / 'skill_EUE_benchmark_leadtime.csv'\n",
    "benchmark_NWP = xr.Dataset.from_dataframe(pd.read_csv(benchmark_file, index_col='leadtime'))\n",
    "benchmark_NWP = benchmark_NWP.set_coords(['model', 'leadtime', 'persistence', 'probability'])\n",
    "\n",
    "for lt in leadtimes:\n",
    "    file = path_out / 'skill_persistence_probability_{lt:03}h_COMB_vs_ENS.jpg'\n",
    "    plot_skill_by_persistence(skill_opt.sel(leadtime=lt),\n",
    "                              coldim='approach',\n",
    "                              metric=metric,\n",
    "                              benchmark=benchmark_NWP.sel(leadtime=lt),\n",
    "                              cmap=OrBu,\n",
    "                              label='ENS',\n",
    "                              save=None)#file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba25c9-b0bc-4755-81d4-f107fe07ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# versus the current criteria\n",
    "for lt in skill_opt.leadtime.data:\n",
    "    benchmark = skill_opt.sel(current_criteria).sel(leadtime=lt)\n",
    "    file = path_out / f'skill_persistence_probability_{lt:03}h_COMB.jpg'\n",
    "    plot_skill_by_persistence(skill_opt.sel(leadtime=lt),\n",
    "                              coldim='approach',\n",
    "                              metric=metric,\n",
    "                              benchmark=benchmark,\n",
    "                              cmap=OrBu,\n",
    "                              save=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf32c8-5990-4543-958a-9e6ad32967b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2.3 Optimize criteria\n",
    "\n",
    "In this section we will derive a optimal set of notification criteria based on the target skill metric. To avoid overfitting, the sample of reporting points is first divided in a training and a test subset. This division is done in a stratified way after randomly shuflling the points, to keep the proportion of observed events in the subsets and avoid geographic biases, respectively.\n",
    "\n",
    "To increase the robustness of the optimization, cross-validation is applied if the parameter `kfold` is set in the configuration file. In that case, `kfold` subsets of stations are generated, again in a stratified and random manner. The average skill over the subsets will be the data used to find the optimal criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daec635-7b82-42e0-bbda-2a7ca1c47240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "\n",
    "# divide stations in train and test samples\n",
    "X = stations_optimize\n",
    "y = stations.loc[X, 'n_events_obs']\n",
    "if stratify:\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size, random_state=seed, shuffle=True, stratify=y)\n",
    "else:\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size, random_state=seed, shuffle=True)\n",
    "\n",
    "best_criteria = {}\n",
    "best_model = {}\n",
    "for leadtime in hits_stn.leadtime.data:\n",
    "    \n",
    "    print(f'leadtime: {leadtime:>3} h')\n",
    "    print('---------------\\n')\n",
    "    \n",
    "    # subset of the 'hits' dataset for the training and test sets\n",
    "    hits_train = hits_stn.sel(id=Xtrain, leadtime=leadtime)\n",
    "    hits_test = hits_stn.sel(id=Xtest, leadtime=leadtime)\n",
    "\n",
    "    # optimize the notification criteria\n",
    "    if kfold is not None: # apply a cross-validation approach\n",
    "        skill_train, best_criteria_lt = find_best_criteria_cv(hits_train,\n",
    "                                                           stations.loc[Xtrain, 'n_events_obs'],\n",
    "                                                           dims=list(min_spread),\n",
    "                                                           kfold=kfold, train_size=train_size, random_state=seed, stratify=stratify,\n",
    "                                                           beta=beta, tolerance=tolerance, min_spread=list(min_spread.values()))\n",
    "    else:\n",
    "        # skill of the training sample\n",
    "        skill_train = hits2skill(hits_train.sum('id', skipna=True), beta=beta)\n",
    "        # best criteria for each approach\n",
    "        best_criteria_lt = find_best_criteria(skill_train, dims=list(min_spread),\n",
    "                                           metric=metric, tolerance=tolerance, min_spread=list(min_spread.values()))\n",
    "\n",
    "    # extract criteria as a dictionary\n",
    "    dims = [var for var in best_criteria_lt if var not in ['recall', 'precision', metric]]\n",
    "    best_criteria_lt = {app: {dim: best_criteria_lt.sel(approach=app)[dim].data for dim in dims} for app in best_criteria_lt.approach.data}\n",
    "    for key in best_criteria_lt:\n",
    "        best_criteria_lt[key]['approach'] = key\n",
    "    best_criteria[leadtime] = best_criteria_lt\n",
    "    \n",
    "    # add current criteria\n",
    "    best_criteria[leadtime]['current'] = current_criteria\n",
    "    \n",
    "    # skill of the train set for the optimized criteria\n",
    "    skill_train = dict2da({key: skill_train.sel(sel) for key, sel in best_criteria_lt.items()}, dim='approach')\n",
    "\n",
    "    # skill of the test set for the optimized criteria\n",
    "    skill_test = hits2skill(hits_test.sum('id', skipna=True), beta=beta)\n",
    "    skill_test = dict2da({key: skill_test.sel(sel) for key, sel in best_criteria_lt.items()}, dim='approach')\n",
    "\n",
    "    # performance of the complete set of stations\n",
    "    skill_all = dict2da({key: skill_opt.sel(leadtime=leadtime).sel(sel) for key, sel in best_criteria_lt.items()}, dim='approach')\n",
    "\n",
    "    # plot results of the optimization\n",
    "    plot_skill_training(skill_train,\n",
    "                        skill_test,\n",
    "                        skill_all,\n",
    "                        ylim=(-0.05, 1.05),\n",
    "                        xdim='approach',\n",
    "                        save=path_out / f'skill_training_{suffix}_{leadtime:03}h.jpg')\n",
    "\n",
    "    # print on screen results\n",
    "    for key, dct in best_criteria_lt.items():\n",
    "        print(key.replace('_', ' '))\n",
    "        print('.' * len(key))\n",
    "        for label, value in dct.items():\n",
    "            if label == 'approach':\n",
    "                continue\n",
    "            print(f'{label}:\\t{value}')\n",
    "        print('{0}(train|test):\\t{1:.3f}|{2:.3f}\\n'.format(metric,\n",
    "                                                         skill_train.sel(approach=key).median()[metric].data,\n",
    "                                                         skill_test.sel(approach=key).median()[metric].data))\n",
    "\n",
    "    # model with the highest skill\n",
    "    best_model[leadtime] = str(skill_test[metric].idxmax('approach').data)\n",
    "\n",
    "# export best criteria\n",
    "file = path_out / f'optimized_criteria_{suffix}.pkl'\n",
    "with open(file, 'wb') as f:\n",
    "    pickle.dump(best_criteria, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e203b81-e92e-4ac4-8eae-a0a55717a36b",
   "metadata": {},
   "source": [
    "> ***Figure 6**. Skill resulting of the otpimization process for each of the methods used to combine the meteorological forcings: 1D+1P, one deterministic and 1 probabilistic; MM, model mean; MW, member weighted; BW, Brier weihted; C, current operational criteria. If cross-validation was applied, the boxplots show the variance in the skill among the kfolds; if not, the black dots represent the skill of the training set. In either case, the orange dot represent the skill of the test set and the blue dots that of the complete set of reporting points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053bb3d-2408-4d00-b706-00180448de49",
   "metadata": {},
   "source": [
    "According to the performance on the test set (orange dots), **the optimal criteria is the one that uses the _member weighted_ approach with no persistence and a probability threshold of 40%**. However, for the training set the highest-performing approach was _1 deterministic and 1 probabilistic_, but this approach seems to suffer from overfitting, since it is the second lowest-performing in the test set.\n",
    "\n",
    "Since the target metric benefits precision over recall, the precision values are higher than those of recall. This difference is enhanced for the _member weighted_ (the best-performing) and the _current_ criteria.\n",
    "\n",
    "Persistence is not necessary in two out of four approaches: _1 deterministic and 1 probabilistic_, _member weighted_. The other two approaches have an optimized persistence of $4/4$.</font>\n",
    "\n",
    "In general, the optimized criteria, regarless of the approach, outperform the current operational criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b371fb-3ae7-44f3-8f21-4fca4383fad6",
   "metadata": {},
   "source": [
    "### 3.3 Analyse skill by reporting points\n",
    "\n",
    "Once we have optimized the notification criteria for each approach, we will have a look to the distribution of the hits/misses/false alarms and the skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67621c-ed43-49ca-88f3-727f6b546472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define colormap for the skill metrics\n",
    "top = cm.get_cmap('Oranges_r', 128)\n",
    "bottom = cm.get_cmap('Blues', 128)\n",
    "newcolors = np.vstack((top(np.linspace(0.2, .95, 128)),\n",
    "                       bottom(np.linspace(0.05, .8, 128))))\n",
    "OrBu = ListedColormap(newcolors, name='OrangeBlue')\n",
    "cmap_f1, norm_f1 = create_cmap(OrBu, np.arange(0, 1.01, 0.05), name='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b3550-b808-4ecb-91e0-dd37d2daa30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map hits/misses/false alarmas and performance for each of the total probability approaches\n",
    "s = 2\n",
    "alpha = 1\n",
    "for leadtime in [min_leadtime]:#leadtimes:\n",
    "    for key, criteria in best_criteria[leadtime].items():\n",
    "\n",
    "        title = '{0}\\nprobability = {1}\\npersistence = {2}'.format(key.replace('_', ' '), criteria['probability'], criteria['persistence'])\n",
    "\n",
    "        # extract TP, FN, FP for this approach\n",
    "        cols = ['TP', 'FN', 'FP']\n",
    "        stations[cols] = hits_stn.sel(leadtime=leadtime).sel(criteria).to_pandas()[cols]\n",
    "        \n",
    "        # plot maps of TP, FN, FP\n",
    "        map_hits(stations.loc[stations_optimize], cols=cols, mask=stations_w_events, s=s, alpha=alpha,\n",
    "                 title=title,\n",
    "                 save=path_out / f'hits_maps_reporting_points_{suffix}_{key}_{leadtime:03}h.jpg')\n",
    "\n",
    "        # compute metrics\n",
    "        cols = ['recall', 'precision', metric]\n",
    "        stations[cols] = pd.concat((compute_skill(stations.TP, stations.FN, stations.FP, beta=beta)), axis=1)\n",
    "        # plot maps of performance\n",
    "        map_skill(stations.loc[stations_optimize], cols=cols,\n",
    "                  bins=50, cmap=cmap_f1, norm=norm_f1,\n",
    "                  s=s, alpha=alpha,\n",
    "                  title=title,\n",
    "                  save=path_out / f'skill_maps_reporting_points_{suffix}_{key}_{leadtime:03}h.jpg')\n",
    "        \n",
    "stations.drop(['TP', 'FN', 'FP', 'recall', 'precision', metric], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfec845-0646-4e38-913e-a636101fa0cc",
   "metadata": {},
   "source": [
    "> ***Figure 7**. Maps of hits, misses and false alarms for the criteria otimized for each approach. The colour scale changes depending on the variable; orange (darker orange) means worse values, whereas blue (darker blue) better values. In the case of hits (TP) and misses (FN) a mask has been applied to remove reporting points with no observed events (gray points), since none of these variables can be computed if there are no observations to predict or miss. The histograms at the bottom show the distributions of hits, misses and false alarms over the whole domain.*\n",
    "\n",
    "> ***Figure 8**. Maps of skill for the criteria otimized for each approach. Orange values represent poor skill, whereas blue values high skill; gray dots represent points for which the metric can not be computed. The histograms at the bottom show the distribution of skill over the whole domain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c205dc-dc40-499c-9f71-2dd63921a75f",
   "metadata": {},
   "source": [
    "**Map of the skill metrics of the optimized vs the current criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683b33e-f3a7-4b02-8618-fa7d67e707d0",
   "metadata": {},
   "source": [
    "The following plot was designed for the article. It shows the results of the three skill metrics with an enfasis on the f-score, as it is the target metric. The difference between the skill metrics with the optimized and the current criteria are computed and shown in the plot through the direction of the arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c6200-db3d-4771-a464-d2b65d58b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = min_leadtime\n",
    "key = 'brier_weighted' # 'current'\n",
    "\n",
    "# skill of the benchmark, i.e., the current notification criteria\n",
    "benchmark = hits_stn.sel(best_criteria[lt]['current']).sel(leadtime=lt).to_pandas()\n",
    "R, P, F = compute_skill(benchmark.TP, benchmark.FN, benchmark.FP, beta=beta)\n",
    "stations[['recall_bench', 'precision_bench', 'fscore_bench']] = pd.concat((R, P, F), axis=1)\n",
    "\n",
    "# skill of the optimized notification criteria\n",
    "opt = hits_stn.sel(best_criteria[lt][key]).sel(leadtime=lt).to_pandas()\n",
    "R, P, F = compute_skill(opt.TP, opt.FN, opt.FP, beta=beta)\n",
    "stations[['recall_opt', 'precision_opt', 'fscore_opt']] = pd.concat((R, P, F), axis=1)\n",
    "\n",
    "# difference in skill between the optimized and the current criteria\n",
    "for met in ['fscore', 'recall', 'precision']:\n",
    "    stations[f'delta_{met}'] = stations[f'{met}_opt'] - stations[f'{met}_bench']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c4a80-b7e1-45b7-9dde-679abcb780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colour map for the following plots\n",
    "Bu = ListedColormap(plt.cm.get_cmap('Blues', 128)(np.linspace(.2, .9, 128)), 'blues')\n",
    "cmap_f1, norm_f1 = create_cmap(Bu, np.arange(0, 1.01, 0.1), name='skill', specify_color=(0, 'chocolate'))\n",
    "\n",
    "# select only stations used for the optimization\n",
    "stns = stations.loc[stations_optimize].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca4a62-8e09-4d1b-8d04-29a63787bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=12\n",
    "scale = 60\n",
    "alpha = .9\n",
    "factor = 1 / np.cos(np.deg2rad(20)) # 20¬∫ corresponds to skill 1\n",
    "\n",
    "# create figure\n",
    "h, b = extent[1] - extent[0], extent[3] - extent[2]\n",
    "width = 8\n",
    "fig = plt.figure(figsize=(1.5 * width, width * b / h))\n",
    "\n",
    "# plot f-score\n",
    "met = 'fscore'\n",
    "col = f'{met}_opt'\n",
    "ax_main = plt.axes([0., 0., 0.605, 1.], projection=proj)\n",
    "ax_main.set_extent(extent)\n",
    "stns.sort_values(col, ascending=True, inplace=True)\n",
    "map_stations(x=stns.X, \n",
    "             y=stns.Y,\n",
    "             z=stns[col],\n",
    "             theta=np.arccos(stns[f'delta_{met}'] / factor), # remove this line to plot dots instead of arrows\n",
    "             mask=stns[col].isnull(),\n",
    "             rivers=rivers,\n",
    "             size=s, scale=scale, cmap=cmap_f1, norm=norm_f1, \n",
    "             #headaxislength=0,\n",
    "             ax=ax_main)\n",
    "ax_main.set_title(r'$f_{0.8}$', fontsize=18)\n",
    "\n",
    "# plot precision and recall\n",
    "for i, met in enumerate(['precision', 'recall']):\n",
    "    col = f'{met}_opt'\n",
    "    ax = plt.axes([0.6, 0.525 * i, .34, .475], projection=proj)\n",
    "    ax.set_extent(extent)\n",
    "    stns.sort_values(col, ascending=True, inplace=True)\n",
    "    map_stations(x=stns.X, \n",
    "                 y=stns.Y, \n",
    "                 z=stns[col], \n",
    "                 theta=np.arccos(stns[f'delta_{met}'] / factor), # remove this line to plot dots instead of arrows\n",
    "                 size=s/4, scale=42, width=.002, \n",
    "                 #headaxislength=0,\n",
    "                 cmap=cmap_f1, norm=norm_f1, \n",
    "                 ax=ax)\n",
    "    ax.set_title(met, fontsize=18)\n",
    "\n",
    "# Colorbar\n",
    "cax = fig.add_axes([.2, -.1, .333, .01666])\n",
    "cbar = fig.colorbar(map_stations.colorbar, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('skill (-)')\n",
    "cbar.ax.tick_params(size=0)\n",
    "ticks = [0, .2, .4, .6, .8, 1.]\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels(ticks, fontsize=15)\n",
    "cbar.outline.set_edgecolor('none');\n",
    "\n",
    "# gauge chart\n",
    "cax2 = fig.add_axes([.6, -.2, .1, .2])\n",
    "gauge_legend(scale=2.3, width=.015, title=f'$\\Delta$ skill (-)', ax=cax2)\n",
    "\n",
    "\n",
    "# export\n",
    "filename = path_out / f'skill_maps_reporting_points_{suffix}_{key}_{leadtime:03}h.jpg'\n",
    "#plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f'The plot was saved in {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913fd5b-1f5d-44bc-afdf-cd3c6181fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export station including skill\n",
    "stations.drop(['n_events_obs', 'delta_fscore', 'delta_recall', 'delta_precision'], axis=1, inplace=True, errors='ignore')\n",
    "stations.to_parquet(path_out / str(file_stations).split('\\\\')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b9c97-c064-4773-ac40-ef5b886bc8ce",
   "metadata": {},
   "source": [
    "**Roebber diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469549b7-d58f-443a-aaa6-5713731662da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best performance of each NWP and approach for `min_leadtime`\n",
    "results = {'NWP': {},\n",
    "          'COMB': {}}\n",
    "for model, criteria in criteria_NWP[min_leadtime].items():\n",
    "    results['NWP'][model] = skill_NWP.sel(criteria).sel(leadtime=min_leadtime).to_pandas()[['precision', 'recall']].values\n",
    "\n",
    "for app, criteria in best_criteria[min_leadtime].items():\n",
    "    if len(app.split('_')) > 1:\n",
    "        app = ''.join([x[0] for x in app.split('_')]).upper()\n",
    "    results['COMB'][app] = skill_opt.sel(criteria).sel(leadtime=min_leadtime).to_pandas()[['precision', 'recall']].values\n",
    "\n",
    "# define colour map\n",
    "top = cm.get_cmap('Blues_r', 128)\n",
    "bottom = cm.get_cmap('Oranges', 128)\n",
    "newcolors = np.vstack((top(np.linspace(0.2, .95, 128)),\n",
    "                       bottom(np.linspace(.05, .8, 128))))\n",
    "BuOr = ListedColormap(newcolors, name='BlueOrange')\n",
    "colors_models = ListedColormap(BuOr(np.linspace(0, 1, 4))).colors\n",
    "\n",
    "fig, ax = roebber_diagram(metric='fscore', beta=beta, lw=.25)\n",
    "\n",
    "# benchmark\n",
    "ax.scatter(*results['COMB']['current'], marker='x', c='k', label='current', zorder=10)\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "\n",
    "# NWP\n",
    "labels = {'EUE': 'ENS', 'COS': 'COS', 'EUD': 'HRES', 'DWD': 'DWD'}\n",
    "for key, color in zip(['EUE', 'COS', 'EUD', 'DWD'], colors_models):\n",
    "    ax.scatter(*results['NWP'][key], marker='^', facecolors='none', edgecolors=color, label=labels[key], zorder=9)\n",
    "handles2, labels2 = ax.get_legend_handles_labels()\n",
    "# ax.scatter(-1, -1, color='w', label=' ')\n",
    "\n",
    "# COM\n",
    "for key, color in zip(['BW', 'MW', 'MM', '1D+1P'], colors_models):\n",
    "    ax.scatter(*results['COMB'][key], marker='o', facecolors='none', edgecolors=color, label=key, zorder=9)\n",
    "handles3, labels3 = ax.get_legend_handles_labels()\n",
    "\n",
    "len2 = len(handles2)\n",
    "handles3, labels3 = handles3[len2:], labels3[len2:]\n",
    "len1 = len(handles1)\n",
    "handles2, labels2 = handles2[len1:], labels2[len1:]\n",
    "\n",
    "fig.legend(handles1, labels1, frameon=False, bbox_to_anchor=[1., .0, .15, .84], loc=2)\n",
    "fig.text(1.05, .72, 'NWP', ha='left', va='top', fontsize=13)\n",
    "fig.legend(handles2, labels2, frameon=False, bbox_to_anchor=[1., .0, .15, .7], loc=2)\n",
    "fig.text(1.05, .42, 'COMB', ha='left', va='top', fontsize=13)\n",
    "fig.legend(handles3, labels3, frameon=False, bbox_to_anchor=[1, .0, .15, .4], loc=2);\n",
    "\n",
    "plt.savefig(path_out / f'roebber_{suffix}_{min_leadtime}h.jpg',\n",
    "            dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b3971-9fe8-4ffd-bbf2-d96454e27329",
   "metadata": {},
   "source": [
    "### 3.4 Analyse skill by catchment area\n",
    "\n",
    "So far we have analyzed only stations with a catchment area larger or equal than a fixed value (2000 km¬≤). Also, in the optimization of the notification criteria this minimum catchment area was fixed.\n",
    "\n",
    "In this section we will analyze how results change according to the catchment area. First, we will see the evolution of skill over catchment area for the notification criteria optimized for a minimum catchment area. Later, we will derive a new optimization criteria in which the probability threshold varies according to catchment area. This derivation is repeated for every approach, and the persistence criterion is fixed for each approach to the value optimized in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd4c9a-d394-400d-954f-9c6a1a346d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an array of catchment area thresholds\n",
    "area_max = np.ceil(stations.area.max() / 500) * 500\n",
    "areas = define_area_ranges(500, area_max, scale='semilog')\n",
    "\n",
    "# no. stations and events by catchment area threshold\n",
    "stations_area = summarize_by_area(stations.area, stations[col_events], areas)\n",
    "\n",
    "# hits and skill by catchment area\n",
    "areas = stations_area[stations_area.n_events_obs > 0].index.to_list()\n",
    "hits_area = hits_by_area(hits_stn.sel(leadtime=leadtimes), stations.area, areas)\n",
    "skill_area = hits2skill(hits_area, beta=beta)\n",
    "skill_area = skill_area.dropna('area', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36505b5c-8664-4266-9d6c-9869273dc9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criteria_area = {}\n",
    "for area in tqdm_notebook(areas):\n",
    "        \n",
    "    # divide stations in train and test samples\n",
    "    X = stations.loc[stations.area >= area].index\n",
    "    if len(X) == 0:\n",
    "        break\n",
    "    y = stations.loc[X, col_events]\n",
    "    if stratify:\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size, random_state=seed, shuffle=True, stratify=y)\n",
    "    else:\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size, random_state=seed, shuffle=True)\n",
    "\n",
    "    # subset of the 'hits' dataset for the training and test sets\n",
    "    hits_train = hits_stn.sel(id=Xtrain, leadtime=leadtimes)\n",
    "\n",
    "    # optimize the notification criteria\n",
    "    try:\n",
    "        if kfold is not None: # apply a cross-validation approach\n",
    "            skill_train, criteria = find_best_criteria_cv(hits_train,\n",
    "                                                          stations.loc[Xtrain, col_events],\n",
    "                                                          dims='probability',\n",
    "                                                          kfold=kfold, train_size=train_size, random_state=seed, stratify=stratify,\n",
    "                                                          beta=beta, tolerance=tolerance, min_spread=min_spread['probability'])\n",
    "        else:\n",
    "            # skill of the training sample\n",
    "            skill_train = hits2skill(hits_train.sum('id', skipna=True), beta=beta)\n",
    "            # best criteria for each approach\n",
    "            criteria = find_best_criteria(skill_train, dims='probability',\n",
    "                                          metric=metric, tolerance=tolerance, min_spread=min_spread['probability'])\n",
    "\n",
    "        criteria_area[area] = criteria['probability']\n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "criteria_area = dict2da(criteria_area, dim='area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d66d1-01ec-4ebb-a665-d83dd07bcf41",
   "metadata": {},
   "source": [
    "#### 3.4.1 Stations and events according to catchment area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c289e-a24d-44be-bd37-6c257d31f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 1\n",
    "alpha = 1.\n",
    "c1 = cm.get_cmap('Oranges', 128)(.66)\n",
    "c2 = cm.get_cmap('Blues', 128)(.66)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.plot(stations_area.index, stations_area.n_stations, alpha=alpha, c=c1, lw=lw, label='points', zorder=2)\n",
    "ax.plot(stations_area.index, stations_area.n_events_obs, alpha=alpha, c=c2, lw=lw, label='events', zorder=1)\n",
    "ax.axvline(x=min_area, ls=':', lw=.5, color='k', zorder=0)\n",
    "ax.text(2000, 0, 'current limit', va='bottom', ha='right', rotation=90)#, fontsize=12)\n",
    "ymin, ymax = 0, np.ceil(stations_area.max().max() / 1000) * 1000\n",
    "yticks = np.arange(ymin, ymax * 1.02, 500).astype(int)\n",
    "ax.set(xlabel='area ‚â• (km¬≤)',\n",
    "       xscale='log',\n",
    "       xlim=(500, area_max),\n",
    "       ylim=(ymin - ymax * .02, ymax * 1.02),\n",
    "       ylabel='count (-)',\n",
    "       yticks=yticks)\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.legend(frameon=False, loc=1);\n",
    "\n",
    "plt.savefig(path_out_root / f'points_observedEvents_vs_area_{suffix}.jpg', dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbd985-db94-47fb-b32b-61e8c8af1c00",
   "metadata": {},
   "source": [
    "> ***Figure 9**. Number of reporting points (orange) and observed events (blue) by catchment area.*\n",
    "\n",
    "The plot represents both the number of reporting points and the number of observed events over a increasing catchment area threshold. Note that the X axis is in logarithmic scale, and that the primary Y axis (reporting points) has a scale double than the secondary Y axis (events).\n",
    "\n",
    "There is a clear 2:1 relation between the number of reporting points and the number of observed events. Both variables dicrease exponentially with catchment area. However, for small catchments (lower than 2000 km¬≤ approx.) the 2:1 relation disappears; the number of events increases faster than that of reporting points with dicreasing area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec66fe-8b40-49d0-9246-e7f9455db3e6",
   "metadata": {},
   "source": [
    "#### 3.4.2 Current vs optimized criteria\n",
    "\n",
    "**Hits, misses and false alarms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d1a09-593e-46aa-9427-095cead404ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lt in [min_leadtime]:\n",
    "    plot_hits_by_variable(hits_area.sel(leadtime=lt),\n",
    "                          optimal_criteria=best_criteria[lt],\n",
    "                          variable='area',\n",
    "                          coldim='approach',\n",
    "                          reference=min_area,\n",
    "                          current_criteria=current_criteria,\n",
    "                          xscale='log',\n",
    "                          xlabel='area ‚â• (km¬≤)',\n",
    "                          xlim=(criteria_area.area.min(),\n",
    "                                criteria_area.area.max()),\n",
    "                          loc_legend=[.87, .8, .2, .1],\n",
    "                          save=None)#f'{path_out}{metric}/hits_vs_area_{suffix}_{lt:03}h.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5497b79-6c29-42d0-9606-f4a2f573a380",
   "metadata": {},
   "source": [
    "> ***Figure 10**. Evolution of hits, misses and false alarms with catchment area threshold. Each plot represents a different approach to combine the meteorological forcings. The primary Y axis is normalized by the number of observed events to allow for comparison; the secondary Y axis indicates the probability threshold. The continuous lines are the hits, whereas the shadows are the false alarms; the difference between the reference line ($\\frac{x}{obs}=1$) and the hits are the misses. The dotted lines are the probability thresholds. Black objects represent the current operational criteria and blue ones the criteria optimized for a fixed area threshold (represented by a vertical, solid, black line).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822e2b9-7b20-48d8-97c0-7c915edeec68",
   "metadata": {},
   "source": [
    "**Skill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25157721-720f-4b76-b7f4-ead62986bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lt in [min_leadtime]:#leadtimes:\n",
    "    plot_skill_by_variable(skill_area.sel(leadtime=lt), \n",
    "                           optimal_criteria=best_criteria[lt], \n",
    "                           variable='area', \n",
    "                           coldim='approach',\n",
    "                           reference=min_area, \n",
    "                           metric=metric, \n",
    "                           current_criteria=current_criteria, \n",
    "                           shades=False,\n",
    "                           xscale='log',\n",
    "                           xlabel='area ‚â• (km¬≤)',\n",
    "                           xlim=(skill_area.area.min(), 3e5), \n",
    "                           loc_text=3, \n",
    "                           loc_legend=[.87, .8, .2, .1],\n",
    "                           save=None)#f'{path_out}{metric}/skill_vs_area_{suffix}_{lt:03}h.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3b076-17c5-4ec4-8cfb-bcdd5ffbe919",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'brier_weighted': colors_models[0],\n",
    "          'member_weighted': colors_models[1],\n",
    "          'model_mean': colors_models[2],\n",
    "          '1_deterministic_+_1_probabilistic': colors_models[3]}\n",
    "\n",
    "for lt in [min_leadtime]:#leadtimes:\n",
    "    plot_skill_by_area(skill_area.sel(leadtime=lt),\n",
    "                       optimal_criteria=best_criteria[lt],\n",
    "                       current_criteria=current_criteria,\n",
    "                       reference = min_area,\n",
    "                       xscale='log',\n",
    "                       xlabel='area ‚â• (km¬≤)',\n",
    "                       xlim=(skill_area.area.data.min(), 3e5),\n",
    "                       metric=metric,\n",
    "                       colors=colors,\n",
    "                       # plot_prob=True,\n",
    "                       save=path_out / f'skill_vs_area_{suffix}_{lt:03}h.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44ed8c-552b-48c3-810c-1d42050d5b15",
   "metadata": {},
   "source": [
    "> ***Figure 11**. Evolution of skill with catchment area threshold. Each plot represents a different approach in which the meteorological forcings are combined. The primary Y axis indicates skill, and the secondary Y axis indicates the probability threshold. The continuous lines are the target skill score ($f_{0.8}$), and the shadows represent the difference between $precision$ and $recall$. The dotted lines are the probability thresholds. Black objects represent the current operational criteria and blue ones the criteria optimized for a fixed area threshold (represented by the vertical, solid, black line).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552798d2-952c-4d39-9652-a561caaa2f2c",
   "metadata": {},
   "source": [
    "#### 3.4.3 Fixed criteria vs area optimized criteria\n",
    "\n",
    "In the previous section we have analyzed how the skill of the system varies over catchment area for a fixed value of the probability threshold. But, what if we tune the probability threshold according to catchment area? Would it improved the skill of the system?\n",
    "\n",
    "**Hits, misses and false alarms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3942f9-93d4-4bd8-a672-ea0c173537c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lt in [min_leadtime]:#leadtimes:\n",
    "    plot_hits_by_variable(hits_area.sel(leadtime=lt),\n",
    "                          optimal_criteria=best_criteria[lt],\n",
    "                          variable='area',\n",
    "                          coldim='approach',\n",
    "                          reference=min_area,\n",
    "                          optimized_criteria=criteria_area.sel(leadtime=lt),\n",
    "                          xscale='log', xlabel='area ‚â• (km¬≤)', xlim=(criteria_area.area.min(), criteria_area.area.max()),\n",
    "                          loc_text=1, loc_legend=[.9, .8, .2, .1],\n",
    "                          save=None)#path_out / f'hits_vs_area_varying_probability_{suffix}_{lt:03}h.jpg', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc1999-a808-490d-8743-e83847422e50",
   "metadata": {},
   "source": [
    "> ***Figure 12**. Evolution of hits, misses and false alarms with catchment area threshold. Each plot represents a different approach to combine the meteorological forcings. The primary Y axis is normalized by the number of observed events to allow for comparison; the secondary Y axis indicates the probability threshold. The continuous lines are the hits, the shadows are the false alarms, and the difference between the reference line ($\\frac{x}{obs}=1$) and the hits are the misses. The dotted lines show the probability threshold. Blue objects are the results for the criteria optimized for a fixed area threshold (represented by a vertical, solid, black line), and orange objects those for the criteria optimized for every area threshold.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c5db3-1bd8-47ce-9c8e-ef8c550c4fa3",
   "metadata": {},
   "source": [
    "**Skill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945c714-52cf-4b88-b0c1-057f2e5ca2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lt in [min_leadtime]:#leadtimes:\n",
    "    plot_skill_by_variable(skill_area.sel(leadtime=lt),\n",
    "                           optimal_criteria=best_criteria[lt],\n",
    "                           variable='area', coldim='approach',\n",
    "                           reference=min_area, metric=metric,\n",
    "                           optimized_criteria=criteria_area.sel(leadtime=lt),\n",
    "                           shades=False,\n",
    "                           xscale='log', xlabel='area ‚â• (km¬≤)', xlim=(skill_area.area.data.min(), 3e5), loc_text=3, loc_legend=[.9, .8, .15, .1],\n",
    "                           save=path_out / f'skill_vs_area_varying_probability_{suffix}_{lt:03}h.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d530bbb-bbf4-4203-b8c3-bedde8aabf54",
   "metadata": {},
   "source": [
    "> ***Figure 13**. Evolution of skill with catchment area threshold. Each plot represents a different approach in which the meteorological forcings are combined. The primary Y axis indicates skill, and the secondary Y axis indicates the probability threshold. The continuous lines are the target skill score ($f_{0.8}$), and the shadows represent the difference between $precision$ and $recall$. The dotted lines show the probability threshold. Blue objects are the results for the criteria optimized for a fixed area threshold (represented by a vertical, solid, black line), and orange objects those for the criteria optimized for every area threshold.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741b8e6-e216-44a8-9ee0-ae72f75c5d97",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e0e67-27e4-429f-818b-e6cf0ef54a20",
   "metadata": {},
   "source": [
    "**Heatmap for the layout figure in the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8154f-2751-496b-877f-b29ff06370ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "cmap_f, norm_f = create_cmap('Blues', bounds=np.arange(.2, .71, .05), name='fscore')\n",
    "\n",
    "da = skill_opt[metric].sel(approach='member_weighted', leadtime=60, probability=skill_opt.probability[::2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.6, 2))\n",
    "\n",
    "plot_DataArray(da.sel(persistence=slice(None, None, -1)),\n",
    "               cmap=OrBu, alpha=.75,\n",
    "               xlabel='probability',\n",
    "               xtick_step=2,\n",
    "               ylabel='persistence',\n",
    "               cbar_kws={'shrink': .8, 'label': r'$f_{0.8}$ (-)'},\n",
    "               ax=ax)\n",
    "xticks = np.arange(1, len(da.probability), 2)\n",
    "xticklabels = da.isel(probability=xticks).probability.data\n",
    "ax.set(xticks=xticks + .5,\n",
    "       xticklabels=xticklabels)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "rect = patches.Rectangle((9, 5), 1, 1, facecolor='none', edgecolor='k', lw=1.5)\n",
    "ax.add_patch(rect);\n",
    "\n",
    "# plt.savefig('../docs/paper/Figures/paper_fscore_matrix.jpg', dpi=300, bbox_inches='tight');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
