{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Hits, misses and false alarms\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 08-08-2023<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "This notebooks computes the hits (true positives, $TP$), misses (false negatives, $FN$) and false alarms (false positives, $FP$) for all the selected reporting points and the complete study period.\n",
    "\n",
    "The input data are the data sets of exceedance over threshold both for the reanalsysis (\"observed\") and the forecast.\n",
    "\n",
    "The confusion matrix (hits, misses, false alarms) is computed for all the possible combinations of three criteria: model combination method (named _approach_), persistence, and probability threshold. The values of persisntece and probability thresholds for which the confusion matrix will be evaluated are defined by the user.\n",
    "\n",
    "The result is a new set of NetCDF files (one for station) that contains matrixes of hits, misses and false alarms for every combination of the criteria.\n",
    "\n",
    "**Interesting links**<br>\n",
    "[Evaluation metrics for imbalanced classification](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/)<br>\n",
    "[Cross entropy for machine learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)<br>\n",
    "[Probability metrics for imbalanced classification](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/)<br>\n",
    "[ROC curves and precision-recall curves for imbalanced classification](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/)<br>\n",
    "[Instructions for sending EFAS flood notifications](https://efascom.smhi.se/confluence/display/EDC/Instructions+for+sending%2C+upgrading+and+deactivating+EFAS+Flood+Notifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# from matplotlib.colors import ListedColormap\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm_notebook\n",
    "import yaml\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from compute import *\n",
    "from seasonal import disaggregate_by_season\n",
    "from convert import reshape_DataArray, dict2da\n",
    "from plot.results import plot_DataArray\n",
    "from plot.maps import create_cmap, map_events\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e0fda-fb63-48b5-9cc3-679d702d934a",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e195ef-46a3-4d0f-8171-dd93b07892c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config_NWP_daily.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730eecb-e002-43fb-a253-e580dc3498a9",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b51867-7a46-457d-8652-78fa28827c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area threshold\n",
    "area_threshold = cfg.get('reporting_points', {}).get('area', 500)\n",
    "\n",
    "# fixed notification criteria for the optimization process\n",
    "area_optimization = cfg.get('skill', {}).get('area', 2000) \n",
    "\n",
    "# reporting points\n",
    "path_stations = cfg.get('reporting_points', {}).get('output', '../results/reporting_points/')\n",
    "file_stations = f'{path_stations}reporting_points_over_{area_threshold}km2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0aa2c-d205-4715-9a35-b16e9d3bba58",
   "metadata": {},
   "source": [
    "### 1.2 Exceedance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce3024d-d294-426d-81ed-79bc59a7635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where results from the preprocessing steps were saved\n",
    "path_reanalysis = cfg.get('exceedance', {}).get('output', {}).get('reanalysis', f'../results/exceedance/reanalysis/')\n",
    "path_forecast = cfg.get('exceedance', {}).get('output', {}).get('forecast', f'../results/exceedance/forecast/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d534e-e8e3-410f-8bc1-6ab40dc0c903",
   "metadata": {},
   "source": [
    "### 1.1 Hits\n",
    "\n",
    "#### Notification criteria\n",
    "\n",
    "##### Probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4265e9c6-daf0-4538-b5f4-504bf79f0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability thresholds\n",
    "probability_range = cfg.get('hits', {}).get('criteria', {}).get('probability', [.05, .096, .05])\n",
    "probability = np.arange(*probability_range).round(3)\n",
    "probability = xr.DataArray(probability, dims=['probability'], coords={'probability': probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b5514-f62d-4efc-81d1-bf88a9849dfd",
   "metadata": {},
   "source": [
    "##### Persistence\n",
    "\n",
    "A list of tuples with two values: the first value is the width of the window rolling sum, and the second value the minimum number of positives in that window so that a notification is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ae087f-81b5-470d-9651-c41ed931caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = cfg.get('hits', {}).get('criteria', {}).get('persistence', [(1, 1), (2, 2), (2, 3)])\n",
    "persistence = {'/'.join([str(i) for i in pers]): pers for pers in persistence}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f63d5-261f-4422-90b0-81e0e5d264e0",
   "metadata": {},
   "source": [
    "##### Computation of hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8e44ad-4bbe-4aba-ae28-cdffd7bd8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead time ranges\n",
    "leadtime = cfg.get('hits', {}).get('leadtime', None)\n",
    "\n",
    "# parameters of the rolling window used to compute hits\n",
    "window = cfg.get('hits', {}).get('window', 1)\n",
    "center = cfg.get('hits', {}).get('center', True)\n",
    "\n",
    "# dissagregate the analysis by seasons?\n",
    "seasonality = cfg.get('hits', {}).get('seasonality', False)\n",
    "\n",
    "# path where the dataset of hits, misses and false alarms will be saved\n",
    "path_out_root = cfg.get('hits', {}).get('output', f'../results/hits/')\n",
    "if leadtime is None:\n",
    "    folder = 'all_leadtimes'\n",
    "elif len(leadtime) == 10:\n",
    "    folder = 'daily'\n",
    "elif len(leadtime) == 20:\n",
    "    folder = '12h'\n",
    "else:\n",
    "    folder = '_'.join([str(lt + 12) for lt in leadtime])\n",
    "path_out = f'{path_out_root}NWP/{folder}/'\n",
    "if seasonality:\n",
    "    path_out += 'seasonal/'\n",
    "for path in [path_out_root, path_out]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1667-e5d2-45fb-83a1-647f9c9c4e7a",
   "metadata": {},
   "source": [
    "## 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b284a86-2d64-4907-a259-e3c67b6de2d5",
   "metadata": {},
   "source": [
    "### 2.1 Reporting points\n",
    "\n",
    "I load all the stations that where selected in a previous [notebook](3_0_select_stations.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaff83-b602-41a5-ba88-9fe9c03e0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporing points\n",
    "stations = pd.read_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92410ba-c400-4ee7-bfda-148de5b2bcfb",
   "metadata": {},
   "source": [
    "### 2.2 Exceedance reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07621a2-bea9-4502-ad61-d09ecb06acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the REANALYSIS data\n",
    "rean_exc = xr.open_mfdataset(f'{path_reanalysis}*.nc', combine='nested', concat_dim='id')['exceedance']\n",
    "rean_exc = rean_exc.sel(id=stations.index).compute()\n",
    "\n",
    "if seasonality:\n",
    "    rean_exc = disaggregate_by_season(rean_exc)\n",
    "\n",
    "print(rean_exc.dims)\n",
    "print(rean_exc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693d76c-5439-4f44-952d-9ac44228a2bd",
   "metadata": {},
   "source": [
    "### 2.3 Exceedance forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af239a67-6f2d-4588-a830-d3fa6626b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the FORECAST data\n",
    "fore_exc = xr.open_mfdataset(f'{path_forecast}*.nc', combine='nested', concat_dim='id')\n",
    "fore_exc['id'] = fore_exc.id.astype(int)\n",
    "fore_exc = fore_exc.sel(id=stations.index)\n",
    "\n",
    "# reshape the DataArray of forecasted exceedance\n",
    "fore_exc = xr.Dataset({label: reshape_DataArray(da, trim=True) for label, da in fore_exc.items()})\n",
    "fore_exc = fore_exc.transpose('id', 'datetime', 'leadtime', 'model')\n",
    "\n",
    "# extract starting and ending dates\n",
    "if ('start' not in locals()) or ('end' not in locals()):\n",
    "    start = pd.to_datetime(max(rean_exc.datetime.min().data, fore_exc.datetime.min().data))\n",
    "    end = pd.to_datetime(min(rean_exc.datetime.max().data, fore_exc.datetime.max().data))\n",
    "else:\n",
    "    start = max(start, rean_exc.datetime.min(), fore_exc.datetime.min())\n",
    "    end = min(end, rean_exc.datetime.max(), fore_exc.datetime.max())\n",
    "\n",
    "# recalculate the exceedance datasets to convert the 3 classes (>Q5, >0.95¬∑Q5, <0.95¬∑Q5) to only 2 (exceedance, non-exceedance)\n",
    "rean_exc, fore_exc = recompute_exceedance(rean_exc.sel(datetime=slice(start, end)),\n",
    "                                          fore_exc['high'].sel(datetime=slice(start, end)),\n",
    "                                          fore_exc['low'].sel(datetime=slice(start, end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ba156-e97c-46b4-bde1-1a35e3e49e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cca45-cccb-49d2-8eb7-75be3cce7f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Observed events\n",
    "#### 3.1.1 Number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f45f60-6a2f-4aa3-8e81-d193605a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the onset of the events\n",
    "onsets =  xr.concat((rean_exc.isel(datetime=0).astype(bool), rean_exc.diff('datetime')), dim='datetime') == 1\n",
    "onsets = onsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# find the offset of the events\n",
    "offsets = xr.concat((rean_exc.isel(datetime=0).astype(bool), rean_exc.diff('datetime')), dim='datetime') == -1\n",
    "offsets = offsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# count number of events\n",
    "stations['n_events_obs'] = onsets.sum('datetime').to_pandas()\n",
    "\n",
    "print('No. stations with observed events:\\t{0}'.format((stations.n_events_obs > 0).sum()))\n",
    "print('No. observed events:\\t\\t\\t{0}'.format(stations.n_events_obs.sum()))\n",
    "\n",
    "if seasonality:\n",
    "    # compute number of events per season\n",
    "    rean_onsets4s = disaggregate_by_season(rean_onsets, dim='datetime')\n",
    "    cols = ['n_events_obs_winter', 'n_events_obs_spring', 'n_events_obs_summer', 'n_events_obs_autumn']\n",
    "    stations[cols] = rean_onsets4s.sum('datetime').to_pandas().transpose()\n",
    "\n",
    "# export the stations table\n",
    "stations.to_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2843be4-55d8-477d-8474-b43c8b3c7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of the number of events\n",
    "map_events(stations, 'n_events_obs', yscale='log', alpha=1, save=f'{path_out_root}/map_observed_events_{stations.shape[0]}points.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd2f44-b46b-410f-a9dc-4cdf61e42df8",
   "metadata": {},
   "source": [
    "> ***Figure 2**. Number of observed flood events during the study period.*\n",
    "\n",
    "The geographical distribution of events is not even. There is a higher proportion of stations with events in Central Europe, British Isles and the Mediterranean catchments than in Estearn and North-Eastern Europe. During the study period there were major events in the Rhine, Meuse and Ebro, which can be seen in the map.\n",
    "\n",
    "The reporting points with more than 5 flood events during the study period were removed, since it is suspicious that the 5-year return period was exceeded so many times in only 2 years of study period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497da33-c341-4cb9-aec6-768820a8fe5f",
   "metadata": {},
   "source": [
    "#### 3.1.2 Duration of the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5ee4f-74fe-4fd3-a481-72dedca2c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the duration (in hours) of each event\n",
    "event_durations = {}\n",
    "for stn in tqdm_notebook(onsets.id.data):\n",
    "    if stations.loc[stn, 'n_events_obs'] == 0:\n",
    "        continue\n",
    "    # dates of the onsets and offsets of the events\n",
    "    dates_onsets = onsets.sel(id=stn).where(onsets.sel(id=stn), drop=True).datetime.data\n",
    "    dates_offsets = offsets.sel(id=stn).where(offsets.sel(id=stn), drop=True).datetime.data\n",
    "    # corrections in case the amount of onsets and onsets don't match\n",
    "    if len(dates_onsets) > len(dates_offsets):\n",
    "        dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    elif len(dates_onsets) < len(dates_offsets):\n",
    "        dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "    # correction in case the first onset is later that the first offset\n",
    "    else:\n",
    "        if dates_onsets[0] > dates_offsets[0]:\n",
    "            print(stn)\n",
    "            dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "            dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    # convert into hours\n",
    "    durations = (dates_offsets - dates_onsets) / np.timedelta64(1, 'h')\n",
    "    event_durations[stn] = durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00388b-076c-47a5-84f3-c2521ddde9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary of durations into a flattened array\n",
    "durations_all, durations_opt = [], []\n",
    "for stn, arr in event_durations.items():\n",
    "    for d in arr:\n",
    "        durations_all.append(d)\n",
    "        if stations.loc[stn, 'area'] >= area_optimization:\n",
    "            durations_opt.append(d)\n",
    "durations_all = np.array(durations_all)\n",
    "durations_opt = np.array(durations_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a9d88-c48e-4bd4-a668-6aba02bb2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of event duration\n",
    "alpha = .7\n",
    "xmin = 0\n",
    "xmax = 16 * 24\n",
    "bins = np.arange(xmin, xmax + 1, 6).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.histplot(durations_all, ax=ax, alpha=alpha**2, bins=bins, color='steelblue', edgecolor=None, label=f'A ‚â• {area_threshold} km¬≤')\n",
    "sns.histplot(durations_opt, ax=ax, alpha=alpha, bins=bins, color='orange', edgecolor=None, label=f'A ‚â• {area_optimization} km¬≤')\n",
    "ax.set(xlabel='duration (h)', ylabel='no. events', xlim=(xmin, xmax));\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(96))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(24))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(100))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(25))\n",
    "ax.legend(frameon=False);\n",
    "\n",
    "plt.savefig(f'{path_out_root}duration_distribution_{stations.shape[0]}points.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ba493-3f90-4b28-9626-318c50892154",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4f9d5-0766-4a2e-969b-9ccf4bf19622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_netcdf = f'{path_out}window_{window}/'\n",
    "if os.path.exists(path_netcdf) is False:\n",
    "    os.makedirs(path_netcdf)\n",
    "\n",
    "for stn in tqdm_notebook(stations.index):\n",
    "    \n",
    "    # check if the output file already exists\n",
    "    file_out = f'{path_netcdf}{stn:>04}.nc'\n",
    "    if os.path.exists(file_out):\n",
    "        continue \n",
    "    \n",
    "    # HITS, MISSES, FALSE ALARMS\n",
    "    \n",
    "    hits = {}\n",
    "    for label, pers in persistence.items():\n",
    "        \n",
    "        # compute predicted events\n",
    "        # pred = exceedance2events(total_exc, persistence=pers, min_leadtime='all')\n",
    "        pred = exceedance2events(fore_exc.sel(id=stn), probability, persistence=pers, leadtime=leadtime)\n",
    "\n",
    "        # disaggregate seasonaly\n",
    "        if seasonality:\n",
    "            pred = disaggregate_by_season(pred)\n",
    "\n",
    "        # compute hits, misses and false alarms\n",
    "        hits[label] = events2hits(rean_exc.sel(id=stn), pred, center=center, w=window)\n",
    "    hits = dict2da(hits, dim='persistence')\n",
    "            \n",
    "    # convert to NaN lead times that can't be reached due to model limitations or persistence\n",
    "    hits = limit_leadtime(hits)\n",
    "    \n",
    "    print(f'Exporting file {file_out}', end='\\r')\n",
    "    hits.to_netcdf(file_out)\n",
    "\n",
    "    del pred, hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
