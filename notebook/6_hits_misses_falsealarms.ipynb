{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd5f453-13c1-47c1-ad27-8af80b47ee08",
   "metadata": {},
   "source": [
    "# Hits, misses and false alarms\n",
    "***\n",
    "\n",
    "**Author**: Chus Casado Rodr√≠guez<br>\n",
    "**Date**: 31-05-2023<br>\n",
    "\n",
    "\n",
    "**Introduction**:<br>\n",
    "This notebooks computes the hits (true positives, $TP$), misses (false negatives, $FN$) and false alarms (false positives, $FP$) for all the selected reporting points and the complete study period.\n",
    "\n",
    "The input data are the data sets of exceedance over threshold both for the reanalsysis (\"observed\") and the forecast.\n",
    "\n",
    "The confusion matrix (hits, misses, false alarms) is computed for all the possible combinations of three criteria: model combination method (named _approach_), persistence, and probability threshold. The values of persisntece and probability thresholds for which the confusion matrix will be evaluated are defined by the user.\n",
    "\n",
    "The result is a new set of NetCDF files (one for station) that contains matrixes of hits, misses and false alarms for every combination of the criteria.\n",
    "\n",
    "**Questions**:<br>\n",
    "\n",
    "* [ ] Take into account the model spread?\n",
    "* [ ] Aggregate results by river/administrative area? EFAS aims at alerting administrations about incoming events in there administrative area, shouldn't that aggregation be included in the results?\n",
    "* [ ] Remove extremely bad performing stations.\n",
    "\n",
    "**Pending tasks**:<br>\n",
    "\n",
    "* [x] Weighting the model average by the Brier score?\n",
    "* [x] Sort stations by catchment area (or other order)?\n",
    "* [x] Persistence\n",
    "* [ ] Analyse only the periods/stations close to an observed event and compute f1 for this extraction. Later on, on the complementary subset of data another metric must be computed to avoid false positives, p.e., false alarm ratio.\n",
    "* [ ] Rename approach 'current' as '1_deterministic_+_1_probabilistic'\n",
    "\n",
    "\n",
    "**Interesting links**<br>\n",
    "[Evaluation metrics for imbalanced classification](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/)<br>\n",
    "[Cross entropy for machine learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)<br>\n",
    "[Probability metrics for imbalanced classification](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/)<br>\n",
    "[ROC curves and precision-recall curves for imbalanced classification](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/)<br>\n",
    "[Instructions for sending EFAS flood notifications](https://efascom.smhi.se/confluence/display/EDC/Instructions+for+sending%2C+upgrading+and+deactivating+EFAS+Flood+Notifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee7a0f-f62b-4a8b-8147-c46a2e2471ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm_notebook\n",
    "import yaml\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('../py/')\n",
    "from compute import recompute_exceedance, exceedance2events, events2hits\n",
    "from seasonal import disaggregate_by_season\n",
    "from convert import reshape_DataArray\n",
    "from plot.results import plot_DataArray\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e0fda-fb63-48b5-9cc3-679d702d934a",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a666a518-7643-4490-b84a-458531a7cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# minimum catchment area\n",
    "area_threshold = cfg.get('reporting_points', {}).get('area', 500)\n",
    "\n",
    "# dissagregate the analysis by seasons?\n",
    "seasonality = cfg.get('seasonality', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d534e-e8e3-410f-8bc1-6ab40dc0c903",
   "metadata": {},
   "source": [
    "### 1.1 Notification criteria\n",
    "\n",
    "#### Probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4265e9c6-daf0-4538-b5f4-504bf79f0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability thresholds\n",
    "probability_range = cfg.get('criteria', {}).get('probability', [.05, .096, .05])\n",
    "probability = np.arange(*probability_range).round(3)\n",
    "probability = xr.DataArray(probability, dims=['probability'], coords={'probability': probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b5514-f62d-4efc-81d1-bf88a9849dfd",
   "metadata": {},
   "source": [
    "#### Persistence\n",
    "\n",
    "A list of tuples with two values: the first value is the width of the window rolling sum, and the second value the minimum number of positives in that window so that a notification is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ae087f-81b5-470d-9651-c41ed931caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = cfg.get('criteria', {}).get('persistence', [(1, 1), (2, 2), (2, 3)])\n",
    "persistence = {'/'.join([str(i) for i in pers]): pers for pers in persistence}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537e30d-2cc1-4560-895e-480c70d3a79d",
   "metadata": {},
   "source": [
    "#### Leadtime\n",
    "\n",
    "Notifications are only sent with a minimum leadtime (h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0059b69c-bdd6-4369-9e0f-d0a9415f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_leadtime = cfg.get('criteria', {}).get('leadtime', 'all')\n",
    "if min_leadtime is None:\n",
    "    min_leadtime = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f63d5-261f-4422-90b0-81e0e5d264e0",
   "metadata": {},
   "source": [
    "### 1.2 Computation of hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc34269-11f7-400a-a758-4f786e7bf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the rolling window used to compute hits\n",
    "window = cfg.get('skill_analysis', {}).get('window', 1)\n",
    "center = cfg.get('skill_analysis', {}).get('window', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c8ad7-1c86-4146-800b-0b6fc8be8862",
   "metadata": {},
   "source": [
    "### 1.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a0a6a6-bcbd-4056-8b6c-0eae6a8dacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where results from the preprocessing steps were saved\n",
    "path_reanalysis = cfg.get('paths', {}).get('output', {}).get('exceedance', {}).get('reanalysis', f'../results/exceedance/reanalysis/')\n",
    "path_forecast = cfg.get('paths', {}).get('output', {}).get('exceedance', {}).get('forecast', f'../results/exceedance/forecast/')\n",
    "\n",
    "# path where the dataset of hits, misses and false alarms will be saved\n",
    "path_out = cfg.get('paths', {}).get('output', {}).get('hits', f'../results/hits/')\n",
    "if seasonality:\n",
    "    path_out += 'seasonal/'\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "# files containing the weighting factors\n",
    "file_weights_member = cfg.get('paths', {}).get('input', {}).get('weights', {}).get('member', None)\n",
    "file_weights_brier = cfg.get('paths', {}).get('input', {}).get('weights', {}).get('brier', None)\n",
    "\n",
    "# reporting points\n",
    "path_stations = cfg.get('paths', {}).get('output', {}).get('reporting_points', '../results/reporting_points/')\n",
    "file_stations = f'{path_stations}reporting_points_over_{area_threshold}km2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1667-e5d2-45fb-83a1-647f9c9c4e7a",
   "metadata": {},
   "source": [
    "## 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b284a86-2d64-4907-a259-e3c67b6de2d5",
   "metadata": {},
   "source": [
    "### 2.1 Stations\n",
    "\n",
    "I load all the stations that where selected in a previous [notebook](3_0_select_stations.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eeaff83-b602-41a5-ba88-9fe9c03e0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporing points\n",
    "stations = pd.read_parquet(file_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92410ba-c400-4ee7-bfda-148de5b2bcfb",
   "metadata": {},
   "source": [
    "### 2.2 Exceedance reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07621a2-bea9-4502-ad61-d09ecb06acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 'datetime')\n",
      "(2371, 7829)\n"
     ]
    }
   ],
   "source": [
    "# load probability of exceeding the discharge threshold in the REANALYSIS data\n",
    "rean_exc = xr.open_mfdataset(f'{path_reanalysis}*.nc', combine='nested', concat_dim='id')['exceedance']\n",
    "rean_exc = rean_exc.sel(id=stations.index).compute()\n",
    "\n",
    "if seasonality:\n",
    "    rean_exc = disaggregate_by_season(rean_exc)\n",
    "\n",
    "print(rean_exc.dims)\n",
    "print(rean_exc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693d76c-5439-4f44-952d-9ac44228a2bd",
   "metadata": {},
   "source": [
    "### 2.3 Exceedance forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c55f61-ab7a-4cff-9a01-8275db972814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probability of exceeding the discharge threshold in the FORECAST data\n",
    "fore_exc = xr.open_mfdataset(f'{path_forecast}*.nc', combine='nested', concat_dim='id')\n",
    "fore_exc['id'] = fore_exc.id.astype(int)\n",
    "fore_exc = fore_exc.sel(id=stations.index)\n",
    "\n",
    "# reshape the DataArray of forecasted exceedance\n",
    "fore_exc = xr.Dataset({label: reshape_DataArray(da, trim=True) for label, da in fore_exc.items()})\n",
    "fore_exc = fore_exc.transpose('id', 'datetime', 'leadtime', 'model')\n",
    "\n",
    "# extract starting and ending dates\n",
    "if ('start' not in locals()) or ('end' not in locals()):\n",
    "    start = pd.to_datetime(fore_exc.datetime.min().data)\n",
    "    end = pd.to_datetime(fore_exc.datetime.max().data)\n",
    "\n",
    "# recalculate the exceedance datasets to convert the 3 classes (>Q5, >0.95¬∑Q5, <0.95¬∑Q5) to only 2 (exceedance, non-exceedance)\n",
    "rean_exc, fore_exc = recompute_exceedance(rean_exc.sel(datetime=slice(start, end)), fore_exc['high'], fore_exc['low'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691e19e-e785-4a14-abda-46a23c8236ea",
   "metadata": {},
   "source": [
    "### 2.3 Weighting factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18e56f-7612-41a6-8f71-b87aa054ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by the number of membes\n",
    "if file_weights_member is not None:\n",
    "    weights_member = xr.open_dataarray(file_weights_member)\n",
    "\n",
    "# by the Brier score\n",
    "if file_weights_brier is not None:\n",
    "    weights_brier = xr.open_dataarray(file_weights_brier, engine='netcdf4')\n",
    "\n",
    "# heatmap of weights\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(6, 3), constrained_layout=True, sharex=True, sharey=True)\n",
    "Weights = xr.Dataset({'no. member': weights_member, 'Brier score': weights_brier})\n",
    "for i, (ax, (var, da)) in enumerate(zip(axes, Weights.items())):\n",
    "    htm = plot_DataArray(da, vmin=0, vmax=1, ax=ax, ytick_step=1, xtick_step=1, title=f'weighted by {var}', cbar_kws={'shrink': .66})\n",
    "    if i == len(axes) - 1:\n",
    "        ax.set_xlabel('leadtime (h)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd165d-b753-49d6-bfbb-66dc6d131e8d",
   "metadata": {},
   "source": [
    "## 3. Computations\n",
    "### 3.1 Hits, misses and false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb6b00-ce82-4510-88c6-c4d7a3c225b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for stn in tqdm_notebook(stations.index):\n",
    "\n",
    "    # check if the output file already exists\n",
    "    file_out = f'{path_out}{stn:>04}.nc'\n",
    "    if os.path.exists(file_out):\n",
    "        continue\n",
    "        \n",
    "    # FORECAST EXCEEDANCE PROBABILITY\n",
    "    forecast = fore_exc.sel(id=stn)\n",
    "\n",
    "    # TOTAL PROBABILITY OF EXCEEDANCE\n",
    "\n",
    "    # exceedance according to 1 deterministic + 1 probabilistic\n",
    "    deterministic = (forecast.sel(model=['EUD', 'DWD']) >= probability).any('model')\n",
    "    probabilistic = (forecast.sel(model=['EUE', 'COS']) >= probability).any('model')\n",
    "    deterministic_probabilistic = deterministic & probabilistic\n",
    "\n",
    "    # exceedance according to mean over models\n",
    "    model_mean = forecast.mean('model', skipna=True) >= probability\n",
    "\n",
    "    # exceedance according to the mean over models weighted by the number of members\n",
    "    member_weighted = forecast.weighted(weights_member).mean('model', skipna=True) >= probability\n",
    "\n",
    "    # exceedance according to the mean over models weighted by the inverse Brier score\n",
    "    brier_weighted = forecast.weighted(weights_brier.fillna(0)).mean('model', skipna=True) >= probability\n",
    "\n",
    "    # merge all total probability approaches in a single DataArray\n",
    "    total_exc = xr.Dataset({\n",
    "                            '1_deterministic_+_1_probabilistic': deterministic_probabilistic,\n",
    "                            'model_mean': model_mean,\n",
    "                            'member_weighted': member_weighted,\n",
    "                            'brier_weighted': brier_weighted,\n",
    "                            }).to_array(dim='approach')\n",
    "\n",
    "    del forecast\n",
    "\n",
    "    # HITS, MISSES, FALSE ALARMS\n",
    "      \n",
    "    hits = {}\n",
    "    for label, pers in persistence.items():\n",
    "\n",
    "        # compute predicted events\n",
    "        pred = exceedance2events(total_exc, persistence=pers, min_leadtime=min_leadtime)\n",
    "               \n",
    "        # disaggregate seasonaly\n",
    "        if seasonality:\n",
    "            pred = disaggregate_by_season(pred)\n",
    "\n",
    "        # compute hits, misses and false alarms\n",
    "        if 'leadtime' in pred.dims:\n",
    "            aux = events2hits(rean_exc.sel(id=stn), pred, center=center, w=window)\n",
    "        else:\n",
    "            aux = events2hits(rean_exc, pred, center=center, w=window)\n",
    "        aux = aux.assign_coords(persistence=label)\n",
    "        hits[label] = aux.expand_dims(dim='persistence')\n",
    "        \n",
    "    hits = xr.concat(hits.values(), dim='persistence')\n",
    "    \n",
    "    print(f'Exporting file {file_out}', end='\\r')\n",
    "    hits.to_netcdf(file_out)\n",
    "\n",
    "    del pred, hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cca45-cccb-49d2-8eb7-75be3cce7f1b",
   "metadata": {},
   "source": [
    "### 3.2 Number of observed events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29c4b2-8e81-406b-ae6c-f214cdcb10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute onsets of the flood events\n",
    "rean_onsets = rean_exc.diff('datetime') == 1\n",
    "rean_onsets = xr.concat((rean_exc.isel(datetime=0).astype(bool), rean_onsets), 'datetime')\n",
    "\n",
    "# save number of observed events\n",
    "stations['n_events_obs'] = rean_onsets.sum('datetime')#.to_pandas()\n",
    "\n",
    "print('No. stations with observed events:\\t{0}'.format((stations.n_events_obs > 0).sum()))\n",
    "print('No. observed events:\\t\\t\\t{0}'.format(stations.n_events_obs.sum()))\n",
    "\n",
    "if seasonality:\n",
    "    # compute number of events per season\n",
    "    rean_onsets4s = disaggregate_by_season(rean_onsets, dim='datetime')\n",
    "    cols = ['n_events_obs_winter', 'n_events_obs_spring', 'n_events_obs_summer', 'n_events_obs_autumn']\n",
    "    stations[cols] = rean_onsets4s.sum('datetime').to_pandas().transpose()\n",
    "\n",
    "# export the stations table\n",
    "stations.to_parquet(file_stations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
