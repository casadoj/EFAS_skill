{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2765e5b3-1233-4484-a0c6-2ccf2ee56dbb",
   "metadata": {},
   "source": [
    "# Reanalysis preprocessing\n",
    "***\n",
    "\n",
    "__Author__: Chus Casado<br>\n",
    "__Date__:   28-07-2022<br>\n",
    "\n",
    "__Introduction__:<br>\n",
    "This code processes the raw EFAS reanalysis discharge data to extract the data necessary for the following steps in the skill analysis.\n",
    "\n",
    "The raw discharge data was downloaded from the Climate Data Store (CDS) and consists of NetCDF files for every year of the analysis. These NetCDF files contain values for the complete EFAS domain, but the succeeding analysis only require the time series for specific points: the selected reporting points. The codes extracts this timeseries and saves the result in a folder of the repository.\n",
    "\n",
    "In a second step, the discharge timeseries are compared against a discharge return period to create a new binary timeseries of exceedance/non-exceedance over the specified threshold. To account for events in which the peak discharge is close to the threshold, there's an option to create a 3-class exceedance timeseries: 0, non-exceendance; 1, exceedance over the reduced threshold ($0.95\\cdot Q_{rp}$); 2, exceedance over the actual threshold ($Q_{rp}$). By default, the reducing factor is $0.95$, but this value can be changed.\n",
    "\n",
    "**Questions**:<br>\n",
    "\n",
    "**To do**:<br>\n",
    "* [ ] How to define in the configuration file (`config.yml`) the two reporting point input files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ad6b4-05e2-45c1-b1fd-108774dca9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_root = os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_root = os.getcwd()\n",
    "\n",
    "os.chdir('../py/')\n",
    "from plot.timeseries import plot_events_timeseries\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a12b65-e1ca-40c6-a29b-a7091bd9522d",
   "metadata": {},
   "source": [
    "## 1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6238a-7c94-48ad-af29-ad877dcb582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/config_NWP_all_leadtimes.yml\", \"r\", encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5a26c-4675-44a6-974e-ff615aa93125",
   "metadata": {},
   "source": [
    "### 1.1 Reporting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8a2e9-87e4-4ee5-842d-6a7aa9344ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area thresholds\n",
    "min_area = cfg.get('reporting_points', {}).get('area', 500)\n",
    "area_optimization = cfg.get('skill', {}).get('area', 2000)\n",
    "\n",
    "# paths\n",
    "csv_stations = cfg.get('reporting_points', {}).get('input', {}).get('CSV', '../data/reporting_points/')\n",
    "GIS_stations = cfg.get('reporting_points', {}).get('input', {}).get('GIS', '../data/reporting_points/')\n",
    "path_out_stations = cfg.get('reporting_points', {}).get('output', '../results/reporting_points/')\n",
    "file_out_stations = f'{path_out_stations}reporting_points_over_{min_area}km2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe66ff2-3161-4270-8a7f-e87de994ba8f",
   "metadata": {},
   "source": [
    "### 1.2 Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7e5f4-5412-42fc-867f-61ee77f9324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where I have saved the raw discharge data\n",
    "path_in = cfg.get('discharge', {}).get('input', {}).get('reanalysis', f'../data/discharge/reanalysis/')\n",
    "\n",
    "# start and end of the study period\n",
    "start = cfg.get('discharge', {}).get('study_period', {}).get('start', None)\n",
    "if isinstance(start, str):\n",
    "    start = datetime.strptime(start, '%Y-%m-%d %H:%M')\n",
    "end = cfg.get('discharge', {}).get('study_period', {}).get('end', None)\n",
    "if isinstance(end, str):\n",
    "    end = datetime.strptime(end, '%Y-%m-%d %H:%M')\n",
    "\n",
    "# NetCDF file that contains the discharge thresholds for each reporting point\n",
    "file_thresholds = cfg.get('discharge', {}).get('return_period', {}).get('input', '../data/thresholds/return_levels.nc')\n",
    "# return period\n",
    "rp = cfg.get('discharge', {}).get('return_period', {}).get('threshold', 5)\n",
    "# percent buffer over the discharge threshold\n",
    "reducing_factor = cfg.get('discharge', {}).get('return_period', {}).get('reducing_factor', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35423c-bf40-4eb7-9fef-08fcddb0c0dd",
   "metadata": {},
   "source": [
    "## 2 Data\n",
    "\n",
    "### 2.1 Reporting points\n",
    "\n",
    "Load the table with all EFAS fixed reporting point and filter those points for which discharge data will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f537d1c-50e7-44e0-92ac-e6f20afaf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table of fixed reporting points\n",
    "stations = pd.read_csv(csv_stations, index_col='station_id')\n",
    "# stations.index = stations.index.astype(str)\n",
    "stations.index.name = 'id'\n",
    "\n",
    "# filter stations and fields\n",
    "mask = (stations['DrainingArea.km2.LDD'] >= min_area) & (stations.FixedRepPoint == True)\n",
    "stations = stations.loc[mask, ['StationName', 'LisfloodX', 'LisfloodY', 'DrainingArea.km2.LDD', 'Catchment', 'River', 'EC_Catchments', 'Country code', 'KGE', 'correlation', 'bias', 'variability']]\n",
    "stations.columns = stations.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country', 'KGE', 'correlation', 'bias', 'variability']\n",
    "stations[['strahler', 'pfafstetter']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a191e-db83-4037-b0d6-e3b6f3e7fb37",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156fbf9a-babb-4577-adbf-c457bf6ee4ba",
   "metadata": {},
   "source": [
    "```python\n",
    "rendimiento = pd.read_csv('../data/reporting_points/indexes_ltr_efas4.csv', usecols=['ObsID', 'KGE', 'KGE_r', 'KGE_B', 'KGE_y'], index_col='ObsID')\n",
    "\n",
    "stations.shape, rendimiento.shape\n",
    "\n",
    "stations[['KGE', 'correlation', 'bias', 'variability']] = rendimiento.round(3)\n",
    "stations.index.name = 'station_id'\n",
    "\n",
    "stations.head()\n",
    "\n",
    "stations.to_csv('../data/reporting_points/Station-2022-10-27v12_KGE.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81e98b-b77e-4c5c-8b2d-20be62cbdfa8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ea7ad-2e34-41cc-a1a8-f91b671e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shapefile with edited river and catchment names\n",
    "points_edited = gpd.read_file(GIS_stations)\n",
    "points_edited.set_index('station_id', inplace=True, drop=True)\n",
    "points_edited.index = points_edited.index.astype(int)\n",
    "points_edited = points_edited[['StationNam', 'LisfloodX', 'LisfloodY', 'DrainingAr', 'Subcatchme',\n",
    "                               'River', 'Catchment', 'Country co', 'strahler', 'pfafstette']]\n",
    "points_edited.columns = ['name', 'X', 'Y', 'area', 'subcatchment', 'river', 'catchment', 'country', 'strahler', 'pfafstetter']\n",
    "points_edited['KGE'] = np.nan\n",
    "# select points with a Pfafstetter code\n",
    "mask = points_edited.pfafstetter.isnull()\n",
    "points_edited = points_edited.loc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8086ef7-ce55-4184-8c14-8f168e7e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct names of catchments and rivers\n",
    "ids = list(set(stations.index).intersection(points_edited.index))\n",
    "stations = stations.loc[ids]\n",
    "for id in ids:\n",
    "    for col in ['subcatchment', 'river', 'catchment']:\n",
    "        if points_edited.loc[id, col] != np.nan:\n",
    "            stations.loc[id, col] = points_edited.loc[id, col]\n",
    "\n",
    "# add subcatchment and river order\n",
    "stations.loc[ids, ['strahler', 'pfafstetter']] = points_edited.loc[ids, ['strahler', 'pfafstetter']]\n",
    "\n",
    "print('no. stations:\\t{0}'.format(stations.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab79515-371d-43b9-bde0-552ab0a3a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarrys with station coordinates that will be used to extract data\n",
    "x = xr.DataArray(stations.X, dims='id')\n",
    "y = xr.DataArray(stations.Y, dims='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd462c-d693-4986-a926-ba0b56d8c02a",
   "metadata": {},
   "source": [
    "### 2.2 Discharge data\n",
    "\n",
    "It loads the EFAS discharge reanalyses for the complete EFAS domain, and out of if only it extracts the discharge time series for the previously selected reporting points and the study period. The discharge timeseries are saved in a _parquet_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cd530-28e7-40d5-8601-478e2de8f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'discharge'\n",
    "\n",
    "# output folder\n",
    "path_out = cfg.get(var, {}).get('output', {}).get('reanalysis', f'../data/{var}/reanalysis/')\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "# load dataset and extract variable discharge\n",
    "dis = xr.open_mfdataset(f'{path_in}EFAS_reanalysis_202*.nc')['dis06']\n",
    "dis.close()\n",
    "\n",
    "# trim data to the study period\n",
    "dis = dis.sel(time=slice(start, end))\n",
    "\n",
    "# extract discharge for the selected stations\n",
    "dis = dis.sel(x=x, y=y, method='nearest')\n",
    "dis = dis.drop(['x', 'y', 'step', 'surface', 'latitude', 'longitude', 'valid_time'])\n",
    "\n",
    "# compute the lazy DataArray\n",
    "dis = dis.compute()\n",
    "\n",
    "# add 6 h to the timesteps\n",
    "dis =dis.rename({'time': 'datetime'})\n",
    "dis['datetime'] = dis.datetime + np.timedelta64(6, 'h')\n",
    "\n",
    "# save extraction as NetCDF files\n",
    "dis.name = var\n",
    "for stn in tqdm(dis.id.data):\n",
    "    dis.sel(id=stn).to_netcdf(f'{path_out}{stn:>04}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7f0ae-da93-4861-b901-cb05626a0fb3",
   "metadata": {},
   "source": [
    "### 2.3 Discharge thresholds\n",
    "\n",
    "The discharge thresholds are the discharge values for return periods 1.5, 2, 5, 10, 20, 50, 100, 200 and 500 years. The data is supplied in a NetCDF file that contains all the river network in Europe. This NetCDF is loaded as an _xarray_ and the values corresponding to the selected reporting points are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572738f-55f2-4365-af91-f088dfbf0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load thresholds\n",
    "thresholds = xr.open_dataset(file_thresholds)\n",
    "\n",
    "# extract thresholds for the selected stations\n",
    "thresholds = thresholds.sel(x=x, y=y, method='nearest')\n",
    "thresholds = thresholds.drop(['x', 'y'])\n",
    "\n",
    "# add thresholds to the DataFrame of stations\n",
    "for var, da in thresholds.items():\n",
    "    stations.loc[thresholds.id, var] = da.values.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03efa13-8857-4695-8ff1-7e31b45ca0db",
   "metadata": {},
   "source": [
    "## 3 Analysis: exceedance over threshold\n",
    "\n",
    "This block of code computes the exceedances of the 5-year return period out of the discharge timeseries and thresholds that were previously extracted. The results are seved in a _parquet_ file that will be used in the succeeding analys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4990f47-19cf-4fae-8f29-57fbbf77df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'exceedance'\n",
    "\n",
    "# compute exceedance\n",
    "thr = f'rl{rp}'\n",
    "exceedance = dis >= thresholds[thr]\n",
    "if reducing_factor is not None:\n",
    "    # compute exceendance over the reduced threshold\n",
    "    exceedance_buffer = dis >= thresholds[thr] * (1 - reducing_factor)\n",
    "    \n",
    "    # create a 3-class exceedance DataArray:\n",
    "    # 0: non-exceedance\n",
    "    # 1: exceeedance over the reduced threshold\n",
    "    # 2: exceedance over the threshold\n",
    "    exceedance = np.maximum(exceedance.astype(int) * 2, exceedance_buffer.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb726615-db3b-4ca0-8d44-c34bb22c3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder\n",
    "path_out = cfg.get(var, {}).get('output', {}).get('reanalysis', f'../data/{var}/reanalysis/') + f'{rp}/'\n",
    "if os.path.exists(path_out) is False:\n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "# save as NetCDF files\n",
    "exceedance.name = var\n",
    "for stn in tqdm(exceedance.id.data):\n",
    "    exceedance.sel(id=stn).to_netcdf(f'{path_out}{stn:>04}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77381783-7009-46cc-8f77-93efda0038a1",
   "metadata": {},
   "source": [
    "### 3.1 Duration of events\n",
    "\n",
    "In this section we will analyse the distribution of the duration of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a5dc7-90ad-4f4f-aceb-bc68ded5a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the exceedance DataArray is binary\n",
    "exceedance_bi = exceedance.copy()\n",
    "exceedance_bi = exceedance_bi.where(exceedance_bi == 2, other=0)\n",
    "exceedance_bi = exceedance_bi.where(exceedance_bi == 0, other=1)\n",
    "\n",
    "# find the onset of the events\n",
    "onsets =  xr.concat((exceedance_bi.isel(datetime=0).astype(bool), exceedance_bi.diff('datetime')), dim='datetime') == 1\n",
    "onsets = onsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# find the offset of the events\n",
    "offsets = xr.concat((exceedance_bi.isel(datetime=0).astype(bool), exceedance_bi.diff('datetime')), dim='datetime') == -1\n",
    "offsets = offsets.sel(datetime=slice(start, end))\n",
    "\n",
    "# count number of events\n",
    "stations['n_events_obs'] = onsets.sum('datetime').to_pandas()\n",
    "\n",
    "# export DataFrame of stations\n",
    "stations.to_parquet(file_out_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65d338-e061-4a42-a6e5-00796e9d7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the duration (in hours) of each event\n",
    "event_durations = {}\n",
    "for stn in tqdm(onsets.id.data):\n",
    "    if stations.loc[stn, 'n_events_obs'] == 0:\n",
    "        continue\n",
    "    # dates of the onsets and offsets of the events\n",
    "    dates_onsets = onsets.sel(id=stn).where(onsets.sel(id=stn), drop=True).datetime.data\n",
    "    dates_offsets = offsets.sel(id=stn).where(offsets.sel(id=stn), drop=True).datetime.data\n",
    "    # corrections in case the amount of onsets and onsets don't match\n",
    "    if len(dates_onsets) > len(dates_offsets):\n",
    "        dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    elif len(dates_onsets) < len(dates_offsets):\n",
    "        dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "    # correction in case the first onset is later that the first offset\n",
    "    else:\n",
    "        if dates_onsets[0] > dates_offsets[0]:\n",
    "            print(stn)\n",
    "            dates_onsets = np.append(onsets.datetime[0].data, dates_onsets)\n",
    "            dates_offsets = np.append(dates_offsets, offsets.datetime[-1].data)\n",
    "    # convert into hours\n",
    "    durations = (dates_offsets - dates_onsets) / np.timedelta64(1, 'h')\n",
    "    event_durations[stn] = durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544b30c-2fc6-4b33-b858-798f7a6f253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary of durations into a flattened array\n",
    "durations_all = []\n",
    "durations_opt = []\n",
    "for stn, arr in event_durations.items():\n",
    "    for d in arr:\n",
    "        durations_all.append(d)\n",
    "        if stations.loc[stn, 'area'] >= area_optimization:\n",
    "            durations_opt.append(d)\n",
    "durations_all = np.array(durations_all)\n",
    "durations_opt = np.array(durations_opt)\n",
    "\n",
    "# plot distribution of event duration\n",
    "\n",
    "xmin = 0\n",
    "xmax = 16 * 24\n",
    "bins = np.arange(xmin, xmax + 1, 6).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.histplot(durations_all, ax=ax, bins=bins, color='steelblue', edgecolor=None, label=f'A ≥ {min_area} km²')\n",
    "sns.histplot(durations_opt, ax=ax, bins=bins, color='orange', edgecolor=None, label=f'A ≥ {area_optimization} km²')\n",
    "ax.set(xlabel='duration (h)', ylabel='no. events', xlim=(xmin, xmax));\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(96))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(24))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(100))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(25))\n",
    "ax.legend();\n",
    "\n",
    "plt.savefig(f'{path_out}duration_distribution.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3ecc7-5a5c-4aeb-9df9-fb131cea6c7b",
   "metadata": {},
   "source": [
    "***Figure 1**. Distribution of the duration of the \"observed\" flood events. Every bin corresponds to 6 h, i.e., the time step resolution of the simultion. Blue represents all the reporting points included in the analysis, and red the reporting points that will be used in the optimization of the notification criteria.*\n",
    "\n",
    "The figure above shows that the majority of events have a duration shorter than 2 days, both for the complete set of reporting points (82%) and the set that will be used in the optimization (79%). In the first set these events account for 82%. The mode in the duration is either 12 or 18 h for the complete and the optimization sets, respectively. Even though the number of events is reduced to a half between the 500 and the 2000 km² threshold, the distribution of the event duration is fairly similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
